"""
çŸ¥è¯†åº“ç®¡ç†å™¨

æœ¬æ¨¡å—å®ç°äº†çŸ¥è¯†åº“çš„ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š
1. KnowledgeBase: å•ä¸ªçŸ¥è¯†åº“çš„æ–‡æ¡£å¢åˆ æŸ¥æ“ä½œ
2. KnowledgeBaseManager: å¤šä¸ªçŸ¥è¯†åº“çš„åˆ›å»ºã€ç®¡ç†ã€åˆ é™¤

çŸ¥è¯†åº“çš„ä½œç”¨ï¼š
- å°†æ–‡æ¡£æŒ‰ä¸»é¢˜æˆ–ä¸šåŠ¡åˆ†ç±»ç®¡ç†
- æ¯ä¸ªçŸ¥è¯†åº“ç‹¬ç«‹ç»´æŠ¤è‡ªå·±çš„å‘é‡ç´¢å¼•
- æ”¯æŒå¤šçŸ¥è¯†åº“å¹¶è¡Œæ£€ç´¢

ä½¿ç”¨åœºæ™¯ï¼š
- "æŠ€æœ¯æ–‡æ¡£åº“"ï¼šå­˜æ”¾æŠ€æœ¯æ–‡æ¡£
- "äº§å“æ‰‹å†Œåº“"ï¼šå­˜æ”¾äº§å“è¯´æ˜
- "FAQåº“"ï¼šå­˜æ”¾å¸¸è§é—®é¢˜
"""

import shutil
import json
from typing import List, Optional, Dict
from pathlib import Path
from datetime import datetime
from langchain_core.documents import Document

from src.core.vectorstore.base import BaseVectorStore
from src.core.embedding.base import BaseEmbedding


class KnowledgeBase:
    """
    å•ä¸ªçŸ¥è¯†åº“
    
    å°è£…äº†å‘é‡å­˜å‚¨å’ŒåµŒå…¥æ¨¡å‹ï¼Œæä¾›æ–‡æ¡£çš„å¢åˆ æŸ¥æ“ä½œã€‚
    æ¯ä¸ªçŸ¥è¯†åº“éƒ½æœ‰ç‹¬ç«‹çš„å­˜å‚¨ç›®å½•å’Œå‘é‡ç´¢å¼•ã€‚
    
    å±æ€§ï¼š
        name: çŸ¥è¯†åº“åç§°ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
        vectorstore: å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆç”¨äºå­˜å‚¨å’Œæ£€ç´¢æ–‡æ¡£å‘é‡ï¼‰
        embedding: åµŒå…¥æ¨¡å‹å®ä¾‹ï¼ˆç”¨äºæ–‡æœ¬å‘é‡åŒ–ï¼‰
        kb_path: çŸ¥è¯†åº“åœ¨ç£ç›˜ä¸Šçš„å­˜å‚¨è·¯å¾„
        
    è®¾è®¡ç†å¿µï¼š
        - å•ä¸€èŒè´£ï¼šåªè´Ÿè´£å•ä¸ªçŸ¥è¯†åº“çš„æ“ä½œ
        - å°è£…æ€§ï¼šéšè—å‘é‡å­˜å‚¨çš„ç»†èŠ‚
        - æŒä¹…åŒ–ï¼šè‡ªåŠ¨ä¿å­˜åˆ°ç£ç›˜
    """
    
    def __init__(
        self,
        name: str,
        vectorstore: BaseVectorStore,
        embedding: BaseEmbedding,
        kb_path: Optional[Path] = None
    ):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“
        
        Args:
            name: çŸ¥è¯†åº“åç§°ï¼Œå»ºè®®ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼ˆå¦‚ "æŠ€æœ¯æ–‡æ¡£"ï¼‰
            vectorstore: å‘é‡å­˜å‚¨å®ä¾‹ï¼ˆå·²åˆå§‹åŒ–çš„ FAISSVectorStore ç­‰ï¼‰
            embedding: åµŒå…¥æ¨¡å‹å®ä¾‹ï¼ˆå·²åˆå§‹åŒ–çš„ OpenAIEmbedding ç­‰ï¼‰
            kb_path: çŸ¥è¯†åº“å­˜å‚¨è·¯å¾„ï¼Œé»˜è®¤ä¸º ./data/knowledge_base/{name}
            
        æ³¨æ„ï¼š
            - kb_path ä¼šè‡ªåŠ¨åˆ›å»ºï¼Œæ— éœ€æå‰åˆ›å»ºç›®å½•
            - vectorstore éœ€è¦é…ç½®æ­£ç¡®çš„ persist_directory
        """
        self.name = name
        self.vectorstore = vectorstore
        self.embedding = embedding
        
        # è®¾ç½®å­˜å‚¨è·¯å¾„ï¼ˆå¦‚æœæœªæŒ‡å®šï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„ï¼‰
        self.kb_path = kb_path or Path(f"./data/knowledge_base/{name}")
        
        # åˆ›å»ºç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        # parents=True: åˆ›å»ºæ‰€æœ‰å¿…éœ€çš„çˆ¶ç›®å½•
        # exist_ok=True: å¦‚æœç›®å½•å·²å­˜åœ¨ï¼Œä¸æŠ¥é”™
        self.kb_path.mkdir(parents=True, exist_ok=True)
        
        # æ–‡æ¡£ç´¢å¼•æ–‡ä»¶è·¯å¾„
        self.doc_index_file = self.kb_path / "documents.json"
        
        # åŠ è½½æˆ–åˆå§‹åŒ–æ–‡æ¡£ç´¢å¼•
        self.documents_index = self._load_doc_index()
    
    def _load_doc_index(self) -> List[Dict]:
        """
        åŠ è½½æ–‡æ¡£ç´¢å¼•
        
        ä» documents.json æ–‡ä»¶ä¸­åŠ è½½æ–‡æ¡£ç´¢å¼•ä¿¡æ¯ã€‚
        å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¿”å›ç©ºåˆ—è¡¨ã€‚
        
        Returns:
            List[Dict]: æ–‡æ¡£ç´¢å¼•åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
                - chunk_count: æ–‡æ¡£å—æ•°é‡
                - added_at: æ·»åŠ æ—¶é—´
                - updated_at: æœ€åæ›´æ–°æ—¶é—´
                
        ç´¢å¼•ç¤ºä¾‹ï¼š
            [
                {
                    "source": "docs/python.pdf",
                    "chunk_count": 15,
                    "added_at": "2024-01-01T10:00:00",
                    "updated_at": "2024-01-01T10:00:00"
                },
                ...
            ]
        """
        if self.doc_index_file.exists():
            try:
                with open(self.doc_index_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ–‡æ¡£ç´¢å¼•å¤±è´¥: {e}")
                return []
        return []
    
    def _save_doc_index(self):
        """
        ä¿å­˜æ–‡æ¡£ç´¢å¼•åˆ°ç£ç›˜
        
        å°†å½“å‰çš„æ–‡æ¡£ç´¢å¼•ä¿å­˜åˆ° documents.json æ–‡ä»¶ã€‚
        ä½¿ç”¨ JSON æ ¼å¼å­˜å‚¨ï¼Œä¾¿äºäººå·¥æŸ¥çœ‹å’Œç¼–è¾‘ã€‚
        
        æ³¨æ„ï¼š
            - ensure_ascii=False: æ”¯æŒä¸­æ–‡å­—ç¬¦
            - indent=2: æ ¼å¼åŒ–è¾“å‡ºï¼Œä¾¿äºé˜…è¯»
        """
        try:
            with open(self.doc_index_file, 'w', encoding='utf-8') as f:
                json.dump(self.documents_index, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜æ–‡æ¡£ç´¢å¼•å¤±è´¥: {e}")
    
    def add_documents(self, documents: List[Document]) -> int:
        """
        æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“
        
        å·¥ä½œæµç¨‹ï¼š
            1. æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨æ˜¯å¦ä¸ºç©º
            2. è°ƒç”¨ vectorstore.add_documents() æ·»åŠ æ–‡æ¡£
               - è¿™ä¼šè‡ªåŠ¨å¯¹æ–‡æ¡£è¿›è¡Œå‘é‡åŒ–ï¼ˆä½¿ç”¨ embeddingï¼‰
               - å°†å‘é‡å­˜å‚¨åˆ°å‘é‡ç´¢å¼•ä¸­
            3. è°ƒç”¨ vectorstore.persist() æŒä¹…åŒ–åˆ°ç£ç›˜
               - ç¡®ä¿æ•°æ®ä¸ä¼šå› ç¨‹åºé€€å‡ºè€Œä¸¢å¤±
               
        Args:
            documents: è¦æ·»åŠ çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆDocument å¯¹è±¡ï¼‰
            
        Returns:
            int: æˆåŠŸæ·»åŠ çš„æ–‡æ¡£æ•°é‡
            
        Raises:
            Exception: æ·»åŠ æ–‡æ¡£å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            
        ç¤ºä¾‹ï¼š
            >>> kb = KnowledgeBase("tech_docs", vectorstore, embedding)
            >>> docs = [Document(page_content="Python æ•™ç¨‹", metadata={"source": "a.txt"})]
            >>> count = kb.add_documents(docs)
            >>> print(f"æ·»åŠ äº† {count} ä¸ªæ–‡æ¡£")
        """
        try:
            # æ£€æŸ¥æ–‡æ¡£åˆ—è¡¨æ˜¯å¦ä¸ºç©º
            if not documents:
                return 0
            
            # æ·»åŠ æ–‡æ¡£åˆ°å‘é‡å­˜å‚¨
            # è¿™é‡Œä¼šè‡ªåŠ¨ï¼š
            # 1. å¯¹æ¯ä¸ªæ–‡æ¡£çš„ page_content è¿›è¡Œå‘é‡åŒ–
            # 2. å°†å‘é‡å’Œå…ƒæ•°æ®å­˜å‚¨åˆ° FAISS ç´¢å¼•
            self.vectorstore.add_documents(documents)
            
            # æŒä¹…åŒ–åˆ°ç£ç›˜
            # å°†å†…å­˜ä¸­çš„å‘é‡ç´¢å¼•ä¿å­˜åˆ° kb_path
            self.vectorstore.persist()
            
            # æ›´æ–°æ–‡æ¡£ç´¢å¼•
            # æŒ‰æ¥æºåˆ†ç»„ç»Ÿè®¡æ–‡æ¡£å—æ•°é‡
            source_chunks = {}
            for doc in documents:
                source = doc.metadata.get('source', 'unknown')
                if source not in source_chunks:
                    source_chunks[source] = 0
                source_chunks[source] += 1
            
            # æ›´æ–°æˆ–æ·»åŠ ç´¢å¼•è®°å½•
            current_time = datetime.now().isoformat()
            for source, chunk_count in source_chunks.items():
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥æ–‡æ¡£çš„ç´¢å¼•
                existing = next(
                    (d for d in self.documents_index if d['source'] == source),
                    None
                )
                
                if existing:
                    # å·²å­˜åœ¨ï¼šæ›´æ–°å—æ•°é‡å’Œæ—¶é—´
                    existing['chunk_count'] += chunk_count
                    existing['updated_at'] = current_time
                else:
                    # ä¸å­˜åœ¨ï¼šæ·»åŠ æ–°ç´¢å¼•
                    self.documents_index.append({
                        'source': source,
                        'chunk_count': chunk_count,
                        'added_at': current_time,
                        'updated_at': current_time
                    })
            
            # ä¿å­˜ç´¢å¼•åˆ°ç£ç›˜
            self._save_doc_index()
            
            # è¿”å›æ·»åŠ çš„æ–‡æ¡£æ•°é‡
            return len(documents)
            
        except Exception as e:
            # æ•è·æ‰€æœ‰å¼‚å¸¸ï¼Œæä¾›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            raise Exception(f"æ·»åŠ æ–‡æ¡£åˆ°çŸ¥è¯†åº“ '{self.name}' å¤±è´¥: {str(e)}") from e
    
    def search(self, query: str, k: int = 4) -> List[Document]:
        """
        åœ¨çŸ¥è¯†åº“ä¸­æœç´¢ç›¸å…³æ–‡æ¡£
        
        å·¥ä½œæµç¨‹ï¼š
            1. å°†æŸ¥è¯¢æ–‡æœ¬å‘é‡åŒ–ï¼ˆè‡ªåŠ¨å®Œæˆï¼‰
            2. åœ¨å‘é‡ç´¢å¼•ä¸­è¿›è¡Œç›¸ä¼¼åº¦æœç´¢
            3. è¿”å›æœ€ç›¸å…³çš„ k ä¸ªæ–‡æ¡£
            
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬ï¼ˆç”¨æˆ·çš„é—®é¢˜ï¼‰
            k: è¿”å›çš„æ–‡æ¡£æ•°é‡ï¼Œé»˜è®¤ 4
            
        Returns:
            List[Document]: æœ€ç›¸å…³çš„æ–‡æ¡£åˆ—è¡¨ï¼ŒæŒ‰ç›¸ä¼¼åº¦é™åºæ’åˆ—
            
        Raises:
            Exception: æœç´¢å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            
        æ³¨æ„ï¼š
            - å¦‚æœçŸ¥è¯†åº“ä¸ºç©ºï¼Œä¼šè¿”å›ç©ºåˆ—è¡¨
            - å¦‚æœæ–‡æ¡£æ€»æ•°å°‘äº kï¼Œè¿”å›æ‰€æœ‰æ–‡æ¡£
            
        ç¤ºä¾‹ï¼š
            >>> results = kb.search("ä»€ä¹ˆæ˜¯ Pythonï¼Ÿ", k=3)
            >>> for doc in results:
            ...     print(doc.page_content[:100])
        """
        try:
            # æ£€æŸ¥å‘é‡åº“æ˜¯å¦ä¸ºç©º
            if self.vectorstore.vectorstore is None:
                return []
            
            # è°ƒç”¨å‘é‡å­˜å‚¨çš„ç›¸ä¼¼åº¦æœç´¢
            # similarity_search ä¼šè‡ªåŠ¨ï¼š
            # 1. å¯¹ query è¿›è¡Œå‘é‡åŒ–
            # 2. è®¡ç®—ä¸æ‰€æœ‰æ–‡æ¡£å‘é‡çš„ç›¸ä¼¼åº¦ï¼ˆä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
            # 3. è¿”å›æœ€ç›¸ä¼¼çš„ k ä¸ªæ–‡æ¡£
            results = self.vectorstore.similarity_search(query, k=k)
            
            return results
            
        except Exception as e:
            raise Exception(f"åœ¨çŸ¥è¯†åº“ '{self.name}' ä¸­æœç´¢å¤±è´¥: {str(e)}") from e
    
    def delete(self):
        """
        åˆ é™¤çŸ¥è¯†åº“
        
        å·¥ä½œæµç¨‹ï¼š
            1. æ¸…ç©ºå†…å­˜ä¸­çš„å‘é‡ç´¢å¼•
            2. åˆ é™¤ç£ç›˜ä¸Šçš„å­˜å‚¨ç›®å½•ï¼ˆåŒ…æ‹¬æ‰€æœ‰æ–‡ä»¶ï¼‰
            
        æ³¨æ„ï¼š
            - è¿™æ˜¯ä¸å¯é€†æ“ä½œï¼Œè¯·è°¨æ…ä½¿ç”¨
            - åˆ é™¤åï¼ŒçŸ¥è¯†åº“å¯¹è±¡ä»ç„¶å­˜åœ¨ï¼Œä½†å·²æ— æ³•ä½¿ç”¨
            - å»ºè®®åœ¨åˆ é™¤å‰æç¤ºç”¨æˆ·ç¡®è®¤
            
        Raises:
            Exception: åˆ é™¤å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            
        ç¤ºä¾‹ï¼š
            >>> kb.delete()
            >>> # çŸ¥è¯†åº“å·²è¢«åˆ é™¤ï¼Œæ— æ³•å†ä½¿ç”¨
        """
        try:
            # æ­¥éª¤1: æ¸…ç©ºå‘é‡å­˜å‚¨ï¼ˆé‡Šæ”¾å†…å­˜ï¼‰
            # è¿™ä¼šå°† vectorstore.vectorstore è®¾ç½®ä¸º None
            self.vectorstore.delete()
            
            # æ­¥éª¤2: åˆ é™¤ç£ç›˜ä¸Šçš„å­˜å‚¨ç›®å½•
            if self.kb_path.exists():
                # shutil.rmtree() é€’å½’åˆ é™¤æ•´ä¸ªç›®å½•æ ‘
                # åŒ…æ‹¬ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶å’Œå­ç›®å½•
                shutil.rmtree(self.kb_path)
                
        except Exception as e:
            raise Exception(f"åˆ é™¤çŸ¥è¯†åº“ '{self.name}' å¤±è´¥: {str(e)}") from e
    
    def get_document_count(self) -> int:
        """
        è·å–çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£æ•°é‡ï¼ˆä¸æ˜¯chunkæ•°é‡ï¼‰
        
        Returns:
            int: æ–‡æ¡£æ•°é‡ï¼Œå¦‚æœçŸ¥è¯†åº“ä¸ºç©ºè¿”å› 0
            
        æ³¨æ„ï¼š
            è¿”å›çš„æ˜¯å®é™…æ–‡æ¡£æ–‡ä»¶æ•°é‡ï¼Œä¸æ˜¯chunkæ•°é‡
            ä¸€ä¸ªæ–‡æ¡£å¯èƒ½è¢«åˆ†æˆå¤šä¸ªchunk
        """
        return len(self.documents_index)
    
    def upload_file(
        self,
        file_path: str,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        splitter_type: str = "recursive"
    ) -> int:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°çŸ¥è¯†åº“
        
        è¿™æ˜¯ä¸€ä¸ªä¾¿æ·æ–¹æ³•ï¼Œå°è£…äº†å®Œæ•´çš„æ–‡æ¡£å¤„ç†æµç¨‹ï¼š
        åŠ è½½ â†’ åˆ‡åˆ† â†’ å‘é‡åŒ– â†’ å­˜å‚¨
        
        å·¥ä½œæµç¨‹ï¼š
            1. ä½¿ç”¨ DocumentLoaderFactory æ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨
            2. åŠ è½½æ–‡ä»¶å†…å®¹ä¸º Document å¯¹è±¡
            3. ä½¿ç”¨ TextSplitterFactory åˆ‡åˆ†æ–‡æ¡£ä¸ºå°å—
            4. è°ƒç”¨ add_documents() æ·»åŠ åˆ°å‘é‡åº“
            5. è‡ªåŠ¨æ›´æ–°æ–‡æ¡£ç´¢å¼•
            
        Args:
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆæ”¯æŒç›¸å¯¹è·¯å¾„å’Œç»å¯¹è·¯å¾„ï¼‰
                æ”¯æŒçš„æ ¼å¼: .txt, .pdf, .md, .markdown
            chunk_size: æ–‡æœ¬å—å¤§å°ï¼Œé»˜è®¤ 500 å­—ç¬¦
                å»ºè®®èŒƒå›´: 200-1000
            chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°ï¼Œé»˜è®¤ 50 å­—ç¬¦
                å»ºè®®ä¸º chunk_size çš„ 10%-20%
            splitter_type: åˆ‡åˆ†å™¨ç±»å‹ï¼Œé»˜è®¤ "recursive"
                - "recursive": é€’å½’åˆ‡åˆ†å™¨ï¼ˆé€šç”¨ï¼Œæ¨èï¼‰
                - "chinese": ä¸­æ–‡åˆ‡åˆ†å™¨ï¼ˆé’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–ï¼‰
                
        Returns:
            int: æˆåŠŸæ·»åŠ çš„æ–‡æ¡£å—æ•°é‡
            
        Raises:
            Exception: æ–‡ä»¶åŠ è½½ã€åˆ‡åˆ†æˆ–æ·»åŠ å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            
        ä½¿ç”¨åœºæ™¯ï¼š
            - Web ç•Œé¢æ–‡ä»¶ä¸Šä¼ 
            - æ‰¹é‡å¯¼å…¥æ–‡æ¡£ç›®å½•
            - å‘½ä»¤è¡Œå·¥å…·ä¸Šä¼ æ–‡ä»¶
            
        ç¤ºä¾‹ï¼š
            >>> kb = KnowledgeBase("æŠ€æœ¯æ–‡æ¡£", vectorstore, embedding)
            >>> 
            >>> # ä¸Šä¼ å•ä¸ªæ–‡ä»¶
            >>> count = kb.upload_file("docs/python_tutorial.pdf")
            >>> print(f"æ·»åŠ äº† {count} ä¸ªæ–‡æ¡£å—")
            >>> 
            >>> # ä¸Šä¼ ä¸­æ–‡æ–‡æ¡£ï¼ˆä½¿ç”¨ä¸­æ–‡åˆ‡åˆ†å™¨ï¼‰
            >>> count = kb.upload_file(
            ...     "docs/chinese_doc.txt",
            ...     chunk_size=300,
            ...     splitter_type="chinese"
            ... )
        """
        from src.core.document import DocumentLoaderFactory, TextSplitterFactory
        
        try:
            # ä¿å­˜åŸå§‹æ–‡ä»¶åï¼ˆç”¨äºæ˜¾ç¤ºï¼‰
            original_filename = Path(file_path).name
            
            # æ­¥éª¤1: åŠ è½½æ–‡ä»¶
            # DocumentLoaderFactory ä¼šæ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨
            print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡ä»¶: {original_filename}...")
            documents = DocumentLoaderFactory.load(file_path)
            
            if not documents:
                raise ValueError(f"æ–‡ä»¶ '{original_filename}' åŠ è½½åä¸ºç©º")
            
            # æ›´æ–°æ–‡æ¡£çš„ metadataï¼Œä½¿ç”¨åŸå§‹æ–‡ä»¶åä½œä¸º source
            # è¿™æ ·åœ¨æ–‡æ¡£åˆ—è¡¨ä¸­æ˜¾ç¤ºçš„å°±æ˜¯åŸå§‹æ–‡ä»¶åï¼Œè€Œä¸æ˜¯ä¸´æ—¶æ–‡ä»¶è·¯å¾„
            for doc in documents:
                # ä¿ç•™åŸå§‹æ–‡ä»¶åï¼Œè€Œä¸æ˜¯ä¸´æ—¶æ–‡ä»¶è·¯å¾„
                doc.metadata['source'] = original_filename
                doc.metadata['original_path'] = file_path  # ä¿ç•™åŸå§‹è·¯å¾„ç”¨äºå†…éƒ¨å¤„ç†
            
            # æ­¥éª¤2: åˆ‡åˆ†æ–‡æ¡£
            print(f"âœ‚ï¸ æ­£åœ¨åˆ‡åˆ†æ–‡æ¡£: {original_filename}...")
            # TextSplitterFactory æ ¹æ®ç±»å‹é€‰æ‹©åˆ‡åˆ†ç­–ç•¥
            chunks = TextSplitterFactory.split(
                documents=documents,
                splitter_type=splitter_type,
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap
            )
            
            if not chunks:
                raise ValueError(f"æ–‡ä»¶ '{original_filename}' åˆ‡åˆ†åä¸ºç©º")
            
            # æ­¥éª¤3: æ·»åŠ åˆ°å‘é‡åº“
            print(f"ğŸ’¾ æ­£åœ¨æ·»åŠ æ–‡æ¡£åˆ°å‘é‡åº“: {original_filename} ({len(chunks)} ä¸ªåˆ†å—)...")
            # add_documents() ä¼šè‡ªåŠ¨å¤„ç†å‘é‡åŒ–ã€å­˜å‚¨å’Œç´¢å¼•æ›´æ–°
            count = self.add_documents(chunks)
            
            print(f"âœ… æ–‡ä»¶ '{original_filename}' ä¸Šä¼ æˆåŠŸï¼Œå…± {count} ä¸ªåˆ†å—")
            return count
            
        except Exception as e:
            raise Exception(f"ä¸Šä¼ æ–‡ä»¶ '{file_path}' åˆ°çŸ¥è¯†åº“ '{self.name}' å¤±è´¥: {str(e)}") from e
    
    def upload_directory(
        self,
        directory_path: str,
        file_extensions: Optional[List[str]] = None,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        recursive: bool = True
    ) -> Dict[str, int]:
        """
        æ‰¹é‡ä¸Šä¼ ç›®å½•ä¸­çš„æ–‡ä»¶
        
        éå†ç›®å½•ï¼Œä¸Šä¼ æ‰€æœ‰æ”¯æŒçš„æ–‡ä»¶åˆ°çŸ¥è¯†åº“ã€‚
        
        Args:
            directory_path: ç›®å½•è·¯å¾„
            file_extensions: è¦å¤„ç†çš„æ–‡ä»¶æ‰©å±•ååˆ—è¡¨
                é»˜è®¤: ['.txt', '.pdf', '.md', '.markdown']
            chunk_size: æ–‡æœ¬å—å¤§å°
            chunk_overlap: æ–‡æœ¬å—é‡å å¤§å°
            recursive: æ˜¯å¦é€’å½’å¤„ç†å­ç›®å½•ï¼Œé»˜è®¤ True
            
        Returns:
            Dict[str, int]: å¤„ç†ç»“æœå­—å…¸
                - success: æˆåŠŸå¤„ç†çš„æ–‡ä»¶æ•°
                - failed: å¤±è´¥çš„æ–‡ä»¶æ•°
                - total_chunks: æ€»æ–‡æ¡£å—æ•°
                - files: æ¯ä¸ªæ–‡ä»¶çš„å¤„ç†ç»“æœåˆ—è¡¨
                
        ç¤ºä¾‹ï¼š
            >>> result = kb.upload_directory("./docs", recursive=True)
            >>> print(f"æˆåŠŸ: {result['success']}, å¤±è´¥: {result['failed']}")
            >>> print(f"æ€»å…±æ·»åŠ äº† {result['total_chunks']} ä¸ªæ–‡æ¡£å—")
        """
        from pathlib import Path
        
        if file_extensions is None:
            file_extensions = ['.txt', '.pdf', '.md', '.markdown']
        
        directory = Path(directory_path)
        if not directory.exists():
            raise FileNotFoundError(f"ç›®å½•ä¸å­˜åœ¨: {directory_path}")
        
        # æ”¶é›†æ‰€æœ‰ç¬¦åˆæ¡ä»¶çš„æ–‡ä»¶
        pattern = "**/*" if recursive else "*"
        files_to_process = []
        
        for file_path in directory.glob(pattern):
            if file_path.is_file() and file_path.suffix.lower() in file_extensions:
                files_to_process.append(file_path)
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶
        result = {
            'success': 0,
            'failed': 0,
            'total_chunks': 0,
            'files': []
        }
        
        for file_path in files_to_process:
            try:
                count = self.upload_file(
                    str(file_path),
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_overlap
                )
                result['success'] += 1
                result['total_chunks'] += count
                result['files'].append({
                    'file': str(file_path),
                    'status': 'success',
                    'chunks': count
                })
            except Exception as e:
                result['failed'] += 1
                result['files'].append({
                    'file': str(file_path),
                    'status': 'failed',
                    'error': str(e)
                })
        
        return result
    
    def list_documents(self) -> List[Dict]:
        """
        åˆ—å‡ºçŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£
        
        Returns:
            List[Dict]: æ–‡æ¡£ä¿¡æ¯åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ï¼š
                - source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
                - chunk_count: æ–‡æ¡£å—æ•°é‡
                - added_at: æ·»åŠ æ—¶é—´
                - updated_at: æœ€åæ›´æ–°æ—¶é—´
                
        ç¤ºä¾‹ï¼š
            >>> docs = kb.list_documents()
            >>> for doc in docs:
            ...     print(f"{doc['source']}: {doc['chunk_count']} å—")
        """
        return self.documents_index.copy()
    
    def get_document_info(self, source: str) -> Optional[Dict]:
        """
        è·å–æŒ‡å®šæ–‡æ¡£çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            source: æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
            
        Returns:
            Optional[Dict]: æ–‡æ¡£ä¿¡æ¯ï¼Œå¦‚æœä¸å­˜åœ¨è¿”å› None
        """
        return next(
            (d for d in self.documents_index if d['source'] == source),
            None
        )
    
    def delete_document(self, source: str):
        """
        åˆ é™¤æŒ‡å®šæ¥æºçš„æ–‡æ¡£
        
        æ³¨æ„ï¼š
            ç”±äº FAISS ä¸æ”¯æŒæŒ‰ ID åˆ é™¤å•ä¸ªæ–‡æ¡£ï¼Œ
            æ­¤æ–¹æ³•ä¼šé‡å»ºæ•´ä¸ªå‘é‡åº“ï¼ˆç§»é™¤æŒ‡å®šæ–‡æ¡£åé‡æ–°æ·»åŠ å…¶ä»–æ–‡æ¡£ï¼‰ã€‚
            
        å·¥ä½œæµç¨‹ï¼š
            1. ä»ç´¢å¼•ä¸­æŸ¥æ‰¾è¦åˆ é™¤çš„æ–‡æ¡£
            2. å¦‚æœæ˜¯å”¯ä¸€çš„æ–‡æ¡£ï¼Œæ¸…ç©ºå‘é‡åº“
            3. å¦‚æœæœ‰å…¶ä»–æ–‡æ¡£ï¼Œè°ƒç”¨ rebuild_vectorstore é‡å»º
            
        Args:
            source: è¦åˆ é™¤çš„æ–‡æ¡£æ¥æºï¼ˆæ–‡ä»¶è·¯å¾„ï¼‰
            
        Raises:
            ValueError: å¦‚æœæ–‡æ¡£ä¸å­˜åœ¨
            Exception: åˆ é™¤å¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            
        è­¦å‘Šï¼š
            - æ­¤æ“ä½œæ¯”è¾ƒè€—æ—¶ï¼Œå› ä¸ºéœ€è¦é‡å»ºå‘é‡åº“
            - å¦‚æœçŸ¥è¯†åº“å¾ˆå¤§ï¼Œå»ºè®®è°¨æ…ä½¿ç”¨
            
        ç¤ºä¾‹ï¼š
            >>> kb.delete_document("docs/old_file.pdf")
        """
        try:
            # æŸ¥æ‰¾æ–‡æ¡£
            doc_info = self.get_document_info(source)
            if not doc_info:
                raise ValueError(f"æ–‡æ¡£ '{source}' ä¸å­˜åœ¨äºçŸ¥è¯†åº“ä¸­")
            
            # ä»ç´¢å¼•ä¸­ç§»é™¤
            self.documents_index = [
                d for d in self.documents_index if d['source'] != source
            ]
            
            # ä¿å­˜æ›´æ–°åçš„ç´¢å¼•
            self._save_doc_index()
            
            # å¦‚æœç´¢å¼•ä¸ºç©ºï¼Œç›´æ¥æ¸…ç©ºå‘é‡åº“
            if not self.documents_index:
                self.vectorstore.delete()
                print(f"âœ… æ–‡æ¡£ '{source}' å·²åˆ é™¤ï¼ŒçŸ¥è¯†åº“å·²æ¸…ç©º")
                return
            
            # å¦åˆ™ï¼Œé‡å»ºå‘é‡åº“ï¼ˆä¸åŒ…å«è¢«åˆ é™¤çš„æ–‡æ¡£ï¼‰
            print(f"âš ï¸ æ­£åœ¨é‡å»ºå‘é‡åº“ï¼ˆç§»é™¤ '{source}'ï¼‰...")
            self.rebuild_vectorstore(exclude_sources=[source])
            print(f"âœ… æ–‡æ¡£ '{source}' å·²åˆ é™¤")
            
        except ValueError:
            # ä¿ç•™ ValueErrorï¼Œç›´æ¥å‘ä¸ŠæŠ›å‡º
            raise
        except Exception as e:
            raise Exception(f"åˆ é™¤æ–‡æ¡£ '{source}' å¤±è´¥: {str(e)}") from e
    
    def rebuild_vectorstore(
        self,
        new_embedding: Optional[BaseEmbedding] = None,
        exclude_sources: Optional[List[str]] = None
    ):
        """
        é‡å»ºå‘é‡åº“
        
        ä½¿ç”¨åœºæ™¯ï¼š
            1. æ›´æ¢ Embedding æ¨¡å‹åé‡æ–°å‘é‡åŒ–æ‰€æœ‰æ–‡æ¡£
            2. åˆ é™¤æ–‡æ¡£åé‡å»ºå‘é‡åº“
            3. å‘é‡åº“æŸåæ—¶æ¢å¤
            4. ä¼˜åŒ–å‘é‡åº“æ€§èƒ½
            
        å·¥ä½œæµç¨‹ï¼š
            1. ä¿å­˜å½“å‰æ–‡æ¡£ç´¢å¼•ä¿¡æ¯
            2. æ¸…ç©ºå‘é‡åº“
            3. å¦‚æœæä¾›æ–° Embeddingï¼Œæ›´æ–°æ¨¡å‹
            4. é‡æ–°åŠ è½½æ‰€æœ‰æ–‡æ¡£ï¼ˆæ’é™¤æŒ‡å®šçš„æ–‡æ¡£ï¼‰
            5. é‡æ–°åˆ‡åˆ†å’Œå‘é‡åŒ–
            6. æ·»åŠ åˆ°æ–°å‘é‡åº“
            
        Args:
            new_embedding: æ–°çš„ Embedding æ¨¡å‹ï¼ˆå¯é€‰ï¼‰
                å¦‚æœæä¾›ï¼Œå°†ä½¿ç”¨æ–°æ¨¡å‹é‡æ–°å‘é‡åŒ–æ‰€æœ‰æ–‡æ¡£
            exclude_sources: è¦æ’é™¤çš„æ–‡æ¡£æ¥æºåˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                é‡å»ºæ—¶ä¸åŒ…å«è¿™äº›æ–‡æ¡£
                
        Raises:
            Exception: é‡å»ºå¤±è´¥æ—¶æŠ›å‡ºå¼‚å¸¸
            
        æ³¨æ„ï¼š
            - æ­¤æ“ä½œæ¯”è¾ƒè€—æ—¶ï¼Œå°¤å…¶æ˜¯æ–‡æ¡£é‡å¤§æ—¶
            - é‡å»ºæœŸé—´çŸ¥è¯†åº“æš‚æ—¶ä¸å¯ç”¨
            - éœ€è¦ç¡®ä¿åŸå§‹æ–‡ä»¶ä»ç„¶å­˜åœ¨
            
        ç¤ºä¾‹ï¼š
            >>> # åœºæ™¯1: æ›´æ¢ Embedding æ¨¡å‹
            >>> new_embedding = OpenAIEmbedding(model_name="text-embedding-3-small")
            >>> kb.rebuild_vectorstore(new_embedding=new_embedding)
            >>> 
            >>> # åœºæ™¯2: åˆ é™¤æ–‡æ¡£åé‡å»º
            >>> kb.rebuild_vectorstore(exclude_sources=["old_file.pdf"])
        """
        from src.core.document import DocumentLoaderFactory, TextSplitterFactory
        
        try:
            print(f"ğŸ”„ å¼€å§‹é‡å»ºå‘é‡åº“ '{self.name}'...")
            
            # ä¿å­˜åŸå§‹æ–‡æ¡£åˆ—è¡¨
            original_docs = self.documents_index.copy()
            
            if not original_docs:
                print("âš ï¸ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— éœ€é‡å»º")
                return
            
            # è¿‡æ»¤è¦æ’é™¤çš„æ–‡æ¡£
            if exclude_sources:
                docs_to_rebuild = [
                    d for d in original_docs 
                    if d['source'] not in exclude_sources
                ]
            else:
                docs_to_rebuild = original_docs
            
            if not docs_to_rebuild:
                print("âš ï¸ æ‰€æœ‰æ–‡æ¡£éƒ½è¢«æ’é™¤ï¼Œæ¸…ç©ºå‘é‡åº“")
                self.vectorstore.delete()
                self.documents_index = []
                self._save_doc_index()
                return
            
            # æ¸…ç©ºå‘é‡åº“
            self.vectorstore.delete()
            
            # å¦‚æœæä¾›äº†æ–° Embeddingï¼Œæ›´æ–°
            if new_embedding:
                print("ğŸ“ ä½¿ç”¨æ–°çš„ Embedding æ¨¡å‹")
                self.embedding = new_embedding
                # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½éœ€è¦é‡æ–°åˆ›å»º vectorstore
                # å…·ä½“å–å†³äºä½ çš„ VectorStore å®ç°
            
            # æ¸…ç©ºæ–‡æ¡£ç´¢å¼•ï¼ˆå‡†å¤‡é‡æ–°æ·»åŠ ï¼‰
            self.documents_index = []
            
            # é‡æ–°åŠ è½½å¹¶æ·»åŠ æ¯ä¸ªæ–‡æ¡£
            total_chunks = 0
            failed_files = []
            
            for doc_info in docs_to_rebuild:
                source = doc_info['source']
                try:
                    print(f"  å¤„ç†: {source}")
                    
                    # åŠ è½½æ–‡æ¡£
                    documents = DocumentLoaderFactory.load(source)
                    
                    # åˆ‡åˆ†æ–‡æ¡£
                    chunks = TextSplitterFactory.split(
                        documents=documents,
                        splitter_type="recursive",
                        chunk_size=500,
                        chunk_overlap=50
                    )
                    
                    # æ·»åŠ åˆ°å‘é‡åº“
                    count = self.add_documents(chunks)
                    total_chunks += count
                    
                except Exception as e:
                    print(f"  âš ï¸ å¤±è´¥: {source} - {e}")
                    failed_files.append(source)
            
            print(f"âœ… é‡å»ºå®Œæˆ:")
            print(f"   æˆåŠŸ: {len(docs_to_rebuild) - len(failed_files)} ä¸ªæ–‡æ¡£")
            print(f"   å¤±è´¥: {len(failed_files)} ä¸ªæ–‡æ¡£")
            print(f"   æ€»å—æ•°: {total_chunks}")
            
            if failed_files:
                print(f"   å¤±è´¥æ–‡ä»¶: {', '.join(failed_files)}")
            
        except Exception as e:
            raise Exception(f"é‡å»ºå‘é‡åº“å¤±è´¥: {str(e)}") from e


class KnowledgeBaseManager:
    """
    çŸ¥è¯†åº“ç®¡ç†å™¨
    
    ç®¡ç†å¤šä¸ªçŸ¥è¯†åº“çš„åˆ›å»ºã€è·å–ã€åˆ—è¡¨ã€åˆ é™¤æ“ä½œã€‚
    ä½¿ç”¨å­—å…¸å­˜å‚¨æ‰€æœ‰çŸ¥è¯†åº“å®ä¾‹ï¼Œæä¾›ç»Ÿä¸€çš„ç®¡ç†æ¥å£ã€‚
    
    å±æ€§ï¼š
        root_path: æ‰€æœ‰çŸ¥è¯†åº“çš„æ ¹ç›®å½•
        knowledge_bases: å­˜å‚¨æ‰€æœ‰çŸ¥è¯†åº“çš„å­—å…¸ {name: KnowledgeBase}
        
    è®¾è®¡æ¨¡å¼ï¼š
        - å•ä¾‹æ¨¡å¼ï¼šé€šå¸¸æ•´ä¸ªåº”ç”¨åªéœ€è¦ä¸€ä¸ªç®¡ç†å™¨å®ä¾‹
        - å·¥å‚æ¨¡å¼ï¼šè´Ÿè´£åˆ›å»ºå’Œç®¡ç† KnowledgeBase å®ä¾‹
        
    ä½¿ç”¨åœºæ™¯ï¼š
        - Web åº”ç”¨ï¼šç®¡ç†æ‰€æœ‰ç”¨æˆ·çš„çŸ¥è¯†åº“
        - API æœåŠ¡ï¼šæä¾›çŸ¥è¯†åº“çš„ CRUD æ¥å£
        - å‘½ä»¤è¡Œå·¥å…·ï¼šæ‰¹é‡ç®¡ç†çŸ¥è¯†åº“
    """
    
    def __init__(self, root_path: Path = Path("./data/knowledge_base")):
        """
        åˆå§‹åŒ–çŸ¥è¯†åº“ç®¡ç†å™¨
        
        Args:
            root_path: çŸ¥è¯†åº“æ ¹ç›®å½•ï¼Œæ‰€æœ‰çŸ¥è¯†åº“éƒ½å­˜å‚¨åœ¨æ­¤ç›®å½•ä¸‹
                      é»˜è®¤ä¸º ./data/knowledge_base
                      
        ç›®å½•ç»“æ„ï¼š
            root_path/
            â”œâ”€â”€ kb1/        # çŸ¥è¯†åº“1çš„ç›®å½•
            â”œâ”€â”€ kb2/        # çŸ¥è¯†åº“2çš„ç›®å½•
            â””â”€â”€ ...
            
        æ³¨æ„ï¼š
            - root_path ä¼šè‡ªåŠ¨åˆ›å»º
            - æ¯ä¸ªçŸ¥è¯†åº“ä¼šåœ¨ root_path ä¸‹åˆ›å»ºç‹¬ç«‹çš„å­ç›®å½•
        """
        self.root_path = Path(root_path)
        # åˆ›å»ºæ ¹ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
        self.root_path.mkdir(parents=True, exist_ok=True)
        
        # åˆå§‹åŒ–çŸ¥è¯†åº“å­—å…¸
        # key: çŸ¥è¯†åº“åç§°ï¼ˆå­—ç¬¦ä¸²ï¼‰
        # value: KnowledgeBase å®ä¾‹
        self.knowledge_bases: dict[str, KnowledgeBase] = {}
    
    def create_kb(
        self,
        name: str,
        vectorstore: BaseVectorStore,
        embedding: BaseEmbedding,
    ) -> KnowledgeBase:
        """
        åˆ›å»ºæ–°çš„çŸ¥è¯†åº“
        
        å·¥ä½œæµç¨‹ï¼š
            1. æ£€æŸ¥çŸ¥è¯†åº“åç§°æ˜¯å¦å·²å­˜åœ¨ï¼ˆé¿å…é‡å¤ï¼‰
            2. ä¸ºçŸ¥è¯†åº“åˆ›å»ºç‹¬ç«‹çš„å­˜å‚¨ç›®å½•
            3. åˆ›å»º KnowledgeBase å®ä¾‹
            4. å°†å®ä¾‹æ·»åŠ åˆ°ç®¡ç†å­—å…¸
            5. è¿”å›åˆ›å»ºçš„çŸ¥è¯†åº“
            
        Args:
            name: çŸ¥è¯†åº“åç§°ï¼ˆå”¯ä¸€æ ‡è¯†ï¼‰
                 - å»ºè®®ä½¿ç”¨æœ‰æ„ä¹‰çš„åç§°ï¼Œå¦‚ "æŠ€æœ¯æ–‡æ¡£"ã€"äº§å“æ‰‹å†Œ"
                 - åç§°ä¸èƒ½åŒ…å«ç‰¹æ®Šå­—ç¬¦ï¼ˆ/, \, :, * ç­‰ï¼‰
            vectorstore: å‘é‡å­˜å‚¨å®ä¾‹
                 - é€šå¸¸æ˜¯ FAISSVectorStore çš„å®ä¾‹
                 - éœ€è¦é…ç½®æ­£ç¡®çš„ persist_directory
            embedding: åµŒå…¥æ¨¡å‹å®ä¾‹
                 - é€šå¸¸æ˜¯ OpenAIEmbedding æˆ– OllamaEmbedding
                 - åŒä¸€ä¸ªçŸ¥è¯†åº“åº”ä½¿ç”¨ç›¸åŒçš„ embedding æ¨¡å‹
                 
        Returns:
            KnowledgeBase: åˆ›å»ºçš„çŸ¥è¯†åº“å®ä¾‹
            
        Raises:
            ValueError: å¦‚æœçŸ¥è¯†åº“åç§°å·²å­˜åœ¨
            Exception: åˆ›å»ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯
            
        ç¤ºä¾‹ï¼š
            >>> manager = KnowledgeBaseManager()
            >>> vectorstore = FAISSVectorStore(persist_directory="./data/kb1")
            >>> embedding = OpenAIEmbedding(model_name="text-embedding-ada-002")
            >>> kb = manager.create_kb("æŠ€æœ¯æ–‡æ¡£", vectorstore, embedding)
            >>> print(f"åˆ›å»ºäº†çŸ¥è¯†åº“: {kb.name}")
        """
        try:
            # æ£€æŸ¥åç§°æ˜¯å¦å·²å­˜åœ¨
            # æ³¨æ„ï¼šè¿™é‡Œçš„é€»è¾‘è¦æ­£ç¡®ï¼
            if name in self.knowledge_bases:  # âœ… å¦‚æœå·²å­˜åœ¨ï¼ŒæŠ¥é”™
                raise ValueError(f"çŸ¥è¯†åº“ '{name}' å·²å­˜åœ¨ï¼Œè¯·ä½¿ç”¨ä¸åŒçš„åç§°")
            
            # ä¸ºçŸ¥è¯†åº“åˆ›å»ºç‹¬ç«‹çš„å­˜å‚¨ç›®å½•
            kb_path = self.root_path / name
            kb_path.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»º KnowledgeBase å®ä¾‹
            kb = KnowledgeBase(
                name=name,
                vectorstore=vectorstore,
                embedding=embedding,
                kb_path=kb_path
            )
            
            # æ·»åŠ åˆ°ç®¡ç†å­—å…¸
            self.knowledge_bases[name] = kb
            
            return kb
            
        except ValueError as e:
            # é‡æ–°æŠ›å‡º ValueErrorï¼ˆåç§°é‡å¤ï¼‰
            raise e
        except Exception as e:
            # æ•è·å…¶ä»–å¼‚å¸¸ï¼Œæä¾›å‹å¥½çš„é”™è¯¯ä¿¡æ¯
            raise Exception(f"åˆ›å»ºçŸ¥è¯†åº“ '{name}' å¤±è´¥: {str(e)}") from e
    
    def get_kb(self, name: str) -> Optional[KnowledgeBase]:
        """
        è·å–æŒ‡å®šåç§°çš„çŸ¥è¯†åº“
        
        Args:
            name: çŸ¥è¯†åº“åç§°
            
        Returns:
            Optional[KnowledgeBase]:
                - å¦‚æœæ‰¾åˆ°ï¼Œè¿”å› KnowledgeBase å®ä¾‹
                - å¦‚æœæœªæ‰¾åˆ°ï¼Œè¿”å› None
                
        æ³¨æ„ï¼š
            - ä¸ä¼šæŠ›å‡ºå¼‚å¸¸ï¼Œæœªæ‰¾åˆ°æ—¶è¿”å› None
            - è°ƒç”¨è€…éœ€è¦æ£€æŸ¥è¿”å›å€¼æ˜¯å¦ä¸º None
            
        ç¤ºä¾‹ï¼š
            >>> kb = manager.get_kb("æŠ€æœ¯æ–‡æ¡£")
            >>> if kb:
            ...     print(f"æ‰¾åˆ°çŸ¥è¯†åº“: {kb.name}")
            ... else:
            ...     print("çŸ¥è¯†åº“ä¸å­˜åœ¨")
        """
        # dict.get() æ–¹æ³•ï¼š
        # - å¦‚æœ key å­˜åœ¨ï¼Œè¿”å›å¯¹åº”çš„ value
        # - å¦‚æœ key ä¸å­˜åœ¨ï¼Œè¿”å› Noneï¼ˆé»˜è®¤å€¼ï¼‰
        return self.knowledge_bases.get(name)
    
    def list_kb(self) -> List[str]:
        """
        åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“çš„åç§°
        
        Returns:
            List[str]: çŸ¥è¯†åº“åç§°åˆ—è¡¨
            
        æ³¨æ„ï¼š
            - è¿”å›çš„æ˜¯åç§°åˆ—è¡¨ï¼Œä¸æ˜¯ KnowledgeBase å®ä¾‹
            - å¦‚æœæ²¡æœ‰çŸ¥è¯†åº“ï¼Œè¿”å›ç©ºåˆ—è¡¨ []
            
        ç¤ºä¾‹ï¼š
            >>> kb_names = manager.list_kb()
            >>> print(f"å…±æœ‰ {len(kb_names)} ä¸ªçŸ¥è¯†åº“")
            >>> for name in kb_names:
            ...     print(f"  - {name}")
        """
        # dict.keys() è¿”å›å­—å…¸çš„æ‰€æœ‰ key
        # list() å°†å…¶è½¬æ¢ä¸ºåˆ—è¡¨
        return list(self.knowledge_bases.keys())
    
    def delete_kb(self, name: str):
        """
        åˆ é™¤æŒ‡å®šçš„çŸ¥è¯†åº“
        
        å·¥ä½œæµç¨‹ï¼š
            1. æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            2. è°ƒç”¨çŸ¥è¯†åº“çš„ delete() æ–¹æ³•ï¼ˆæ¸…ç©ºå‘é‡ç´¢å¼•ã€åˆ é™¤æ–‡ä»¶ï¼‰
            3. ä»ç®¡ç†å­—å…¸ä¸­ç§»é™¤
            
        Args:
            name: è¦åˆ é™¤çš„çŸ¥è¯†åº“åç§°
            
        Raises:
            ValueError: å¦‚æœçŸ¥è¯†åº“ä¸å­˜åœ¨
            Exception: åˆ é™¤è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯
            
        æ³¨æ„ï¼š
            - è¿™æ˜¯ä¸å¯é€†æ“ä½œï¼Œåˆ é™¤åæ— æ³•æ¢å¤
            - ä¼šåˆ é™¤ç£ç›˜ä¸Šçš„æ‰€æœ‰æ–‡ä»¶
            - å»ºè®®åœ¨è°ƒç”¨å‰æç¤ºç”¨æˆ·ç¡®è®¤
            
        ç¤ºä¾‹ï¼š
            >>> if manager.get_kb("æ—§çŸ¥è¯†åº“"):
            ...     manager.delete_kb("æ—§çŸ¥è¯†åº“")
            ...     print("çŸ¥è¯†åº“å·²åˆ é™¤")
        """
        try:
            # ä»å­—å…¸ä¸­è·å–çŸ¥è¯†åº“
            kb = self.knowledge_bases.get(name)
            
            # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
            if not kb:
                raise ValueError(f"çŸ¥è¯†åº“ '{name}' ä¸å­˜åœ¨ï¼Œæ— æ³•åˆ é™¤")
            
            # è°ƒç”¨çŸ¥è¯†åº“çš„ delete() æ–¹æ³•
            # è¿™ä¼šæ¸…ç©ºå‘é‡ç´¢å¼•å¹¶åˆ é™¤ç£ç›˜æ–‡ä»¶
            kb.delete()
            
            # ä»ç®¡ç†å­—å…¸ä¸­ç§»é™¤
            del self.knowledge_bases[name]
            
        except ValueError as e:
            # é‡æ–°æŠ›å‡º ValueErrorï¼ˆçŸ¥è¯†åº“ä¸å­˜åœ¨ï¼‰
            raise e
        except Exception as e:
            # æ•è·å…¶ä»–å¼‚å¸¸
            raise Exception(f"åˆ é™¤çŸ¥è¯†åº“ '{name}' å¤±è´¥: {str(e)}") from e
    
    def get_kb_info(self, name: str) -> dict:
        """
        è·å–çŸ¥è¯†åº“çš„è¯¦ç»†ä¿¡æ¯
        
        Args:
            name: çŸ¥è¯†åº“åç§°
            
        Returns:
            dict: çŸ¥è¯†åº“ä¿¡æ¯ï¼ŒåŒ…å«ï¼š
                - name: åç§°
                - path: å­˜å‚¨è·¯å¾„
                - document_count: æ–‡æ¡£æ•°é‡
                - exists: æ˜¯å¦å­˜åœ¨
                
        è¿™æ˜¯ä¸€ä¸ªè¾…åŠ©æ–¹æ³•ï¼Œç”¨äºæ˜¾ç¤ºå’Œè°ƒè¯•ã€‚
        """
        kb = self.get_kb(name)
        
        if not kb:
            return {
                "name": name,
                "exists": False
            }
        
        return {
            "name": kb.name,
            "path": str(kb.kb_path),
            "document_count": kb.get_document_count(),
            "exists": True
        }

