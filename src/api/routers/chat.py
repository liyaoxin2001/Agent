"""
èŠå¤©ç›¸å…³ API è·¯ç”±

æä¾›å¯¹è¯é—®ç­”åŠŸèƒ½ã€‚
"""
from fastapi import APIRouter, Depends, HTTPException, status
from fastapi.responses import StreamingResponse
from typing import AsyncIterator
import uuid
import json

from src.api.models import ChatRequest, ChatResponse, ErrorResponse
from src.api.dependencies import get_kb_manager, get_llm
from src.knowledge_base.kb_manager import KnowledgeBaseManager
from src.core.llm.base import OpenAILLM
from src.agent.state import create_initial_state
from src.agent.graph import create_simple_rag_agent


router = APIRouter(
    prefix="/api/v1/chat",
    tags=["Chat"],
    responses={404: {"model": ErrorResponse}}
)


@router.get(
    "/default-model",
    summary="è·å–é»˜è®¤æ¨¡å‹",
    description="è·å–å½“å‰é…ç½®çš„é»˜è®¤æ¨¡å‹åç§°"
)
async def get_default_model(
    default_llm: OpenAILLM = Depends(get_llm)
) -> dict:
    """
    è·å–é»˜è®¤æ¨¡å‹ä¿¡æ¯
    
    è¿”å›:
        dict: åŒ…å«é»˜è®¤æ¨¡å‹åç§°å’Œæ˜¯å¦æ”¯æŒè§†è§‰
    """
    import os
    env_model = os.getenv("OPENAI_MODEL")
    return {
        "model_name": default_llm.model_name,
        "supports_vision": default_llm.supports_vision,
        "from_env": env_model is not None,
        "env_model": env_model,
    }


@router.post(
    "",
    response_model=ChatResponse,
    summary="å‘é€èŠå¤©æ¶ˆæ¯",
    description="å‘æŒ‡å®šçŸ¥è¯†åº“å‘é€é—®é¢˜ï¼Œè·å– AI ç”Ÿæˆçš„ç­”æ¡ˆ"
)
async def chat(
    request: ChatRequest,
    kb_manager: KnowledgeBaseManager = Depends(get_kb_manager),
    llm: OpenAILLM = Depends(get_llm)
) -> ChatResponse:
    """
    èŠå¤©ç«¯ç‚¹
    
    å‚æ•°:
        request: èŠå¤©è¯·æ±‚ï¼ˆåŒ…å«é—®é¢˜ã€çŸ¥è¯†åº“åç§°ç­‰ï¼‰
        kb_manager: çŸ¥è¯†åº“ç®¡ç†å™¨ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
        llm: LLM å®ä¾‹ï¼ˆä¾èµ–æ³¨å…¥ï¼‰
    
    è¿”å›:
        ChatResponse: åŒ…å«ç­”æ¡ˆã€æ£€ç´¢æ–‡æ¡£ã€å…ƒæ•°æ®ç­‰
    
    å¼‚å¸¸:
        404: çŸ¥è¯†åº“ä¸å­˜åœ¨
        500: æœåŠ¡å™¨å†…éƒ¨é”™è¯¯
    """
    try:
        # 1. æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨ï¼ˆRAG æ¨¡å¼å¿…éœ€ï¼‰
        if not request.kb_name:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail="RAG æ¨¡å¼éœ€è¦æŒ‡å®šçŸ¥è¯†åº“åç§°"
            )
        if request.kb_name not in kb_manager.knowledge_bases:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ '{request.kb_name}' ä¸å­˜åœ¨"
            )
        
        # 2. è·å–çŸ¥è¯†åº“
        kb = kb_manager.get_kb(request.kb_name)
        
        # 3. ç”Ÿæˆå¯¹è¯ IDï¼ˆå¦‚æœæ²¡æœ‰æä¾›ï¼‰
        conversation_id = request.conversation_id or f"conv-{uuid.uuid4().hex[:8]}"
        
        # 4. åˆ›å»º Agent
        agent = create_simple_rag_agent(
            llm=llm,
            vectorstore=kb.vectorstore,
            k=4
        )
        
        # 5. åˆ›å»ºåˆå§‹çŠ¶æ€
        state = create_initial_state(
            question=request.question,
            max_steps=request.max_steps,
            conversation_id=conversation_id
        )
        
        # 6. è¿è¡Œ Agent
        result = agent.invoke(state)
        
        # 7. æ„å»ºå“åº”
        retrieved_docs = []
        if result.get('retrieved_docs'):
            for doc in result['retrieved_docs']:
                retrieved_docs.append({
                    "content": doc.page_content[:200],  # é™åˆ¶é•¿åº¦
                    "source": doc.metadata.get("source", "unknown"),
                    "score": doc.metadata.get("score", 0.0)
                })
        
        metadata = {
            "retrieval_score": result.get("retrieval_score"),
            "confidence_score": result.get("confidence_score"),
            "step_count": result.get("step_count", 0),
            "kb_name": request.kb_name
        }
        
        return ChatResponse(
            answer=result.get("answer", "æŠ±æ­‰ï¼Œæˆ‘æ— æ³•å›ç­”è¿™ä¸ªé—®é¢˜ã€‚"),
            conversation_id=conversation_id,
            retrieved_docs=retrieved_docs,
            metadata=metadata
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"å¤„ç†èŠå¤©è¯·æ±‚æ—¶å‡ºé”™: {str(e)}"
        )


@router.post(
    "/general",
    response_model=ChatResponse,
    summary="é€šç”¨å¯¹è¯",
    description="é€šç”¨å¯¹è¯æ¨¡å¼ï¼Œç›´æ¥è°ƒç”¨ LLMï¼Œä¸ä½¿ç”¨çŸ¥è¯†åº“"
)
async def chat_general(
        request: ChatRequest,
        default_llm: OpenAILLM = Depends(get_llm)
    ) -> ChatResponse:
        """
        é€šç”¨å¯¹è¯ç«¯ç‚¹
        
        ç›´æ¥è°ƒç”¨ LLM ç”Ÿæˆå›ç­”ï¼Œä¸ä½¿ç”¨çŸ¥è¯†åº“æ£€ç´¢ã€‚
        æ”¯æŒåŠ¨æ€é€‰æ‹©æ¨¡å‹ã€‚
        """
        try:
            # å¦‚æœè¯·æ±‚ä¸­æŒ‡å®šäº†æ¨¡å‹ï¼Œä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹ï¼›å¦åˆ™ä½¿ç”¨é»˜è®¤æ¨¡å‹
            if request.model_name and request.model_name != default_llm.model_name:
                # åŠ¨æ€åˆ›å»ºæŒ‡å®šæ¨¡å‹çš„LLMå®ä¾‹
                llm = OpenAILLM(model_name=request.model_name)
            else:
                llm = default_llm
            
            # æ”¯æŒå›¾ç‰‡è¾“å…¥ï¼ˆä¼˜å…ˆä½¿ç”¨imagesï¼Œå…¼å®¹image_pathsï¼‰
            images = getattr(request, 'images', None) or getattr(request, 'image_paths', None) or []
            # è·å–å¯¹è¯å†å²
            conversation_history = getattr(request, 'conversation_history', None) or []
            answer = llm.generate(
                prompt=request.question,
                images=images if images else [],
                conversation_history=conversation_history
            )
            
            return ChatResponse(
                answer=answer,
                conversation_id=request.conversation_id or f"conv-{uuid.uuid4().hex[:8]}",
                retrieved_docs=[],
                metadata={
                    "mode": "general",
                    "has_images": bool(images),
                    "model_used": llm.model_name,
                    "supports_vision": llm.supports_vision
                }
            )
        except ValueError as e:
            # æ¨¡å‹ä¸æ”¯æŒè§†è§‰çš„é”™è¯¯ï¼Œè¿”å›400è€Œä¸æ˜¯500
            error_msg = str(e)
            if "ä¸æ”¯æŒè§†è§‰åŠŸèƒ½" in error_msg:
                raise HTTPException(
                    status_code=status.HTTP_400_BAD_REQUEST,
                    detail=error_msg
                )
            else:
                raise HTTPException(
                    status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                    detail=f"é€šç”¨å¯¹è¯å¤±è´¥: {error_msg}"
                )
        except Exception as e:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail=f"é€šç”¨å¯¹è¯å¤±è´¥: {str(e)}"
            )


@router.post(
    "/search",
    response_model=ChatResponse,
    summary="è”ç½‘æœç´¢",
    description="è”ç½‘æœç´¢ + AI æ€»ç»“"
)
async def chat_search(
    request: ChatRequest,
    llm: OpenAILLM = Depends(get_llm)
) -> ChatResponse:
    """
    è”ç½‘æœç´¢ç«¯ç‚¹
    
    ä½¿ç”¨ DuckDuckGo æœç´¢ï¼Œç„¶åç”¨ AI æ€»ç»“ç»“æœã€‚
    """
    try:
        from duckduckgo_search import DDGS
        
        # 1. æœç´¢
        search_results = []
        with DDGS() as ddgs:
            for r in ddgs.text(request.question, max_results=5):
                search_results.append({
                    "title": r.get("title", ""),
                    "snippet": r.get("body", ""),
                    "url": r.get("href", "")
                })
        
        if not search_results:
            return ChatResponse(
                answer="æŠ±æ­‰ï¼Œæœªæ‰¾åˆ°ç›¸å…³æœç´¢ç»“æœã€‚",
                conversation_id=request.conversation_id or f"conv-{uuid.uuid4().hex[:8]}",
                retrieved_docs=[],
                metadata={"mode": "search"}
            )
        
        # 2. æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([
            f"**{r['title']}**\n{r['snippet']}\næ¥æº: {r['url']}"
            for r in search_results[:3]
        ])
        
        # 3. AI æ€»ç»“
        prompt = f"""åŸºäºä»¥ä¸‹æœç´¢ç»“æœå›ç­”é—®é¢˜ã€‚

é—®é¢˜: {request.question}

æœç´¢ç»“æœ:
{context}

è¯·åŸºäºä¸Šè¿°ä¿¡æ¯ç»™å‡ºå‡†ç¡®çš„ç­”æ¡ˆã€‚"""
        
        answer = llm.generate(prompt)
        
        # 4. æ·»åŠ å¼•ç”¨æ¥æº
        answer += "\n\n**ğŸ” å‚è€ƒæ¥æº:**\n"
        for i, r in enumerate(search_results[:3], 1):
            answer += f"{i}. [{r['title']}]({r['url']})\n"
        
        return ChatResponse(
            answer=answer,
            conversation_id=request.conversation_id or f"conv-{uuid.uuid4().hex[:8]}",
            retrieved_docs=[{
                "content": r['snippet'],
                "source": r['url'],
                "title": r['title']
            } for r in search_results[:3]],
            metadata={"mode": "search", "result_count": len(search_results)}
        )
    except ImportError:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail="æœç´¢åŠŸèƒ½éœ€è¦å®‰è£… duckduckgo-search: pip install duckduckgo-search"
        )
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=f"æœç´¢å¤±è´¥: {str(e)}"
        )


@router.post(
    "/stream",
    summary="æµå¼èŠå¤©",
    description="ä½¿ç”¨æœåŠ¡å™¨å‘é€äº‹ä»¶ï¼ˆSSEï¼‰æµå¼è¿”å›ç­”æ¡ˆ"
)
async def chat_stream(
    request: ChatRequest,
    kb_manager: KnowledgeBaseManager = Depends(get_kb_manager),
    llm: OpenAILLM = Depends(get_llm)
):
    """
    æµå¼èŠå¤©ç«¯ç‚¹
    
    ä½¿ç”¨ SSE (Server-Sent Events) æµå¼è¿”å›ç”Ÿæˆçš„ç­”æ¡ˆã€‚
    
    TODO: éœ€è¦å®ç°æµå¼ç”ŸæˆèŠ‚ç‚¹
    """
    # æ£€æŸ¥çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
    if request.kb_name not in kb_manager.knowledge_bases:
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail=f"çŸ¥è¯†åº“ '{request.kb_name}' ä¸å­˜åœ¨"
        )
    
    async def event_generator() -> AsyncIterator[str]:
        """ç”Ÿæˆ SSE äº‹ä»¶æµ"""
        try:
            # è·å–çŸ¥è¯†åº“
            kb = kb_manager.get_kb(request.kb_name)
            
            # ç”Ÿæˆå¯¹è¯ ID
            conversation_id = request.conversation_id or f"conv-{uuid.uuid4().hex[:8]}"
            
            # å‘é€å¼€å§‹äº‹ä»¶
            yield f"data: {json.dumps({'type': 'start', 'conversation_id': conversation_id})}\n\n"
            
            # TODO: ä½¿ç”¨æµå¼ç”ŸæˆèŠ‚ç‚¹
            # è¿™é‡Œä½¿ç”¨ç®€å•çš„æ¨¡æ‹Ÿ
            answer = f"è¿™æ˜¯å¯¹é—®é¢˜ '{request.question}' çš„æµå¼å›ç­”..."
            for char in answer:
                yield f"data: {json.dumps({'type': 'token', 'content': char})}\n\n"
            
            # å‘é€ç»“æŸäº‹ä»¶
            yield f"data: {json.dumps({'type': 'end', 'conversation_id': conversation_id})}\n\n"
        
        except Exception as e:
            yield f"data: {json.dumps({'type': 'error', 'message': str(e)})}\n\n"
    
    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream"
    )
