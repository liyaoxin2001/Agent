# æ–‡æ¡£å¤„ç†æ¨¡å—çŸ¥è¯†ç‚¹æ€»ç»“

> **ä½œè€…**: AI Assistant  
> **æ—¥æœŸ**: 2026-01-08  
> **æ¨¡å—**: `src/core/document/`  
> **ç›¸å…³æ–‡ä»¶**: `loader.py`, `splitter.py`

---

## ğŸ“‹ ç›®å½•

1. [æ¨¡å—æ¦‚è¿°](#1-æ¨¡å—æ¦‚è¿°)
2. [æ ¸å¿ƒæ¦‚å¿µ](#2-æ ¸å¿ƒæ¦‚å¿µ)
3. [æ–‡æ¡£åŠ è½½å™¨è¯¦è§£](#3-æ–‡æ¡£åŠ è½½å™¨è¯¦è§£)
4. [æ–‡æœ¬åˆ‡åˆ†å™¨è¯¦è§£](#4-æ–‡æœ¬åˆ‡åˆ†å™¨è¯¦è§£)
5. [è®¾è®¡æ¨¡å¼åˆ†æ](#5-è®¾è®¡æ¨¡å¼åˆ†æ)
6. [å‚æ•°é…ç½®æŒ‡å—](#6-å‚æ•°é…ç½®æŒ‡å—)
7. [å®æˆ˜æ¡ˆä¾‹](#7-å®æˆ˜æ¡ˆä¾‹)
8. [å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–](#8-å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–)
9. [ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ](#9-ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ)
10. [å­¦ä¹ è·¯å¾„å»ºè®®](#10-å­¦ä¹ è·¯å¾„å»ºè®®)

---

## 1. æ¨¡å—æ¦‚è¿°

### 1.1 æ¨¡å—å®šä½

æ–‡æ¡£å¤„ç†æ¨¡å—æ˜¯ **RAG ç³»ç»Ÿçš„æ•°æ®å…¥å£**ï¼Œè´Ÿè´£å°†å„ç§æ ¼å¼çš„åŸå§‹æ–‡æ¡£è½¬æ¢ä¸ºå¯ä»¥è¢«å‘é‡åŒ–å’Œæ£€ç´¢çš„æ ‡å‡†æ ¼å¼ã€‚

```
ã€RAG ç³»ç»Ÿæ•°æ®æµã€‘
åŸå§‹æ–‡æ¡£ (PDF/TXT/MD)
    â†“
ğŸ”· æ–‡æ¡£åŠ è½½å™¨ (Loader)      â† æœ¬æ¨¡å—
    â†“
ç»“æ„åŒ–æ–‡æ¡£ (Document å¯¹è±¡)
    â†“
ğŸ”· æ–‡æœ¬åˆ‡åˆ†å™¨ (Splitter)     â† æœ¬æ¨¡å—
    â†“
æ–‡æ¡£å— (Chunks)
    â†“
Embedding æ¨¡å—              â† ä¸‹æ¸¸æ¨¡å—
    â†“
VectorStore æ¨¡å—            â† ä¸‹æ¸¸æ¨¡å—
    â†“
RAG Chain æ¨¡å—              â† ä¸‹æ¸¸æ¨¡å—
```

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦æ–‡æ¡£å¤„ç†ï¼Ÿ

#### â“ é—®é¢˜ 1ï¼šä¸ºä»€ä¹ˆä¸èƒ½ç›´æ¥æŠŠæ•´ä¸ªæ–‡æ¡£å‘é‡åŒ–ï¼Ÿ

**ç­”æ¡ˆ**ï¼šç›´æ¥å‘é‡åŒ–æ•´ä¸ªæ–‡æ¡£ä¼šå¯¼è‡´ï¼š

1. **æ£€ç´¢ä¸ç²¾ç¡®**
   ```
   ç”¨æˆ·é—®é¢˜: "Python æ˜¯ä»€ä¹ˆæ—¶å€™åˆ›å»ºçš„ï¼Ÿ"
   
   âŒ æ•´ä¸ªæ–‡æ¡£å‘é‡åŒ–ï¼š
   - æ–‡æ¡£åŒ…å« 100 é¡µå…³äº Python çš„å†…å®¹
   - åªæœ‰ç¬¬ 3 é¡µæåˆ° "1991 å¹´åˆ›å»º"
   - å‘é‡è¡¨ç¤ºæ˜¯æ•´ä¸ªæ–‡æ¡£çš„"å¹³å‡è¯­ä¹‰"
   - æ£€ç´¢æ—¶ï¼Œç›¸å…³ä¿¡æ¯è¢«æ·¹æ²¡åœ¨æ— å…³å†…å®¹ä¸­
   
   âœ… åˆ‡åˆ†åçš„å°å—å‘é‡åŒ–ï¼š
   - ç¬¬ 3 é¡µè¢«åˆ‡åˆ†æˆ 5 ä¸ªå—
   - å…¶ä¸­ä¸€ä¸ªå—æ˜ç¡®åŒ…å« "1991 å¹´åˆ›å»º"
   - è¿™ä¸ªå—çš„å‘é‡ç²¾ç¡®è¡¨ç¤ºäº†è¿™ä¸ªä¿¡æ¯
   - æ£€ç´¢æ—¶ç›´æ¥å‘½ä¸­ç›¸å…³å—
   ```

2. **è¶…å‡º LLM çš„ä¸Šä¸‹æ–‡çª—å£**
   ```python
   # GPT-3.5-turbo çš„é™åˆ¶
   max_tokens = 4096  # çº¦ 3000 å­—ï¼ˆä¸­æ–‡ï¼‰
   
   # å¦‚æœæ–‡æ¡£æœ‰ 10 ä¸‡å­—
   document_length = 100000
   
   # æ— æ³•ä¸€æ¬¡æ€§è¾“å…¥ç»™ LLM
   document_length > max_tokens  # True
   ```

3. **å‘é‡è¡¨ç¤ºè´¨é‡å·®**
   - Embedding æ¨¡å‹å¯¹ **çŸ­æ–‡æœ¬** çš„è¡¨ç¤ºæ›´å‡†ç¡®
   - é•¿æ–‡æœ¬çš„å‘é‡æ˜¯æ‰€æœ‰å†…å®¹çš„"å¹³å‡"ï¼Œä¸¢å¤±ç»†èŠ‚

#### â“ é—®é¢˜ 2ï¼šä¸ºä»€ä¹ˆéœ€è¦æ”¯æŒå¤šç§æ ¼å¼ï¼Ÿ

**ç­”æ¡ˆ**ï¼šå®é™…åº”ç”¨ä¸­çš„æ–‡æ¡£æ ¼å¼å¤šæ ·ï¼š

| æ–‡æ¡£ç±»å‹ | æ ¼å¼ | ä½¿ç”¨åœºæ™¯ | æŒ‘æˆ˜ |
|---------|------|---------|------|
| æŠ€æœ¯æ–‡æ¡£ | PDF | å­¦æœ¯è®ºæ–‡ã€äº§å“æ‰‹å†Œ | æå–æ–‡æœ¬ã€ä¿ç•™ç»“æ„ |
| ç¬”è®° | Markdown | å¼€å‘æ–‡æ¡£ã€Wiki | å¤„ç†ä»£ç å—ã€é“¾æ¥ |
| æ—¥å¿— | TXT | ç³»ç»Ÿæ—¥å¿—ã€èŠå¤©è®°å½• | ç¼–ç é—®é¢˜ã€æ ¼å¼ä¸ä¸€ |
| ç”µå­ä¹¦ | EPUB/PDF | çŸ¥è¯†åº“ | ç« èŠ‚è¯†åˆ«ã€å›¾ç‰‡å¤„ç† |

### 1.3 æ¨¡å—æ¶æ„

```
src/core/document/
â”œâ”€â”€ __init__.py           # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ loader.py             # æ–‡æ¡£åŠ è½½å™¨
â”‚   â”œâ”€â”€ BaseDocumentLoader       (æŠ½è±¡åŸºç±»)
â”‚   â”œâ”€â”€ TextLoader               (TXT åŠ è½½å™¨)
â”‚   â”œâ”€â”€ PDFLoader                (PDF åŠ è½½å™¨)
â”‚   â”œâ”€â”€ MarkdownLoader           (Markdown åŠ è½½å™¨)
â”‚   â””â”€â”€ DocumentLoaderFactory    (å·¥å‚ç±»)
â””â”€â”€ splitter.py           # æ–‡æœ¬åˆ‡åˆ†å™¨
    â”œâ”€â”€ BaseTextSplitter         (æŠ½è±¡åŸºç±»)
    â”œâ”€â”€ RecursiveTextSplitter    (é€’å½’åˆ‡åˆ†å™¨)
    â”œâ”€â”€ ChineseTextSplitter      (ä¸­æ–‡åˆ‡åˆ†å™¨)
    â””â”€â”€ TextSplitterFactory      (å·¥å‚ç±»)
```

---

## 2. æ ¸å¿ƒæ¦‚å¿µ

### 2.1 Document å¯¹è±¡

Document æ˜¯ LangChain ä¸­è¡¨ç¤ºæ–‡æ¡£çš„æ ‡å‡†æ•°æ®ç»“æ„ã€‚

#### ç»“æ„å®šä¹‰

```python
from langchain.schema import Document

# Document çš„å†…éƒ¨ç»“æ„
class Document:
    page_content: str      # æ–‡æ¡£å†…å®¹ï¼ˆå¿…éœ€ï¼‰
    metadata: dict         # å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
```

#### ç¤ºä¾‹

```python
# åˆ›å»ºä¸€ä¸ª Document
doc = Document(
    page_content="Python æ˜¯ç”± Guido van Rossum äº 1991 å¹´åˆ›å»ºçš„ã€‚",
    metadata={
        "source": "python_history.pdf",
        "page": 1,
        "author": "å¼ ä¸‰",
        "date": "2024-01-01"
    }
)

# è®¿é—®å†…å®¹
print(doc.page_content)  # æ–‡æœ¬å†…å®¹
print(doc.metadata)      # {'source': 'python_history.pdf', ...}
```

#### metadata çš„ä½œç”¨

1. **è¿½æº¯æ¥æº**ï¼šè®°å½•æ–‡æ¡£æ¥è‡ªå“ªä¸ªæ–‡ä»¶ã€å“ªä¸€é¡µ
   ```python
   # åœ¨ RAG ç­”æ¡ˆä¸­å¼•ç”¨æ¥æº
   answer = "Python åˆ›å»ºäº 1991 å¹´"
   source = doc.metadata["source"]  # "python_history.pdf"
   print(f"æ¥æºï¼š{source}")
   ```

2. **è¿‡æ»¤æ£€ç´¢**ï¼šæ ¹æ®å…ƒæ•°æ®ç­›é€‰æ–‡æ¡£
   ```python
   # åªæœç´¢ç‰¹å®šä½œè€…çš„æ–‡æ¡£
   results = vectorstore.similarity_search(
       query="Python",
       filter={"author": "å¼ ä¸‰"}
   )
   ```

3. **è°ƒè¯•è¿½è¸ª**ï¼šäº†è§£æ–‡æ¡£çš„å¤„ç†æµç¨‹
   ```python
   # æ·»åŠ å¤„ç†æ—¶é—´æˆ³
   doc.metadata["processed_at"] = datetime.now()
   ```

### 2.2 æ–‡æœ¬åˆ‡åˆ†çš„å…³é”®å‚æ•°

#### chunk_sizeï¼ˆå—å¤§å°ï¼‰

**å®šä¹‰**ï¼šæ¯ä¸ªæ–‡æœ¬å—çš„æœ€å¤§å­—ç¬¦æ•°ï¼ˆæˆ– token æ•°ï¼‰ã€‚

**ä½œç”¨**ï¼š
- æ§åˆ¶æ–‡æœ¬å—çš„ç²’åº¦
- å½±å“æ£€ç´¢ç²¾åº¦å’Œ LLM è¾“å…¥

**é€‰æ‹©å»ºè®®**ï¼š

| æ–‡æ¡£ç±»å‹ | æ¨è chunk_size | ç†ç”± |
|---------|----------------|------|
| æŠ€æœ¯æ–‡æ¡£ | 300-500 | æŠ€æœ¯æ¦‚å¿µé€šå¸¸åœ¨ 1-2 æ®µå†…å®Œæ•´ |
| æ–°é—»æ–‡ç«  | 500-800 | æ–°é—»æ®µè½è¾ƒé•¿ï¼Œéœ€è¦æ›´å¤§çš„å— |
| å­¦æœ¯è®ºæ–‡ | 800-1200 | è®ºæ–‡æ®µè½é•¿ï¼Œéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ |
| å¯¹è¯è®°å½• | 200-300 | å¯¹è¯å†…å®¹çŸ­ï¼Œéœ€è¦æ›´ç»†çš„ç²’åº¦ |
| ä»£ç æ–‡ä»¶ | 400-600 | å‡½æ•°å’Œç±»é€šå¸¸åœ¨è¿™ä¸ªèŒƒå›´ |

**ç¤ºä¾‹**ï¼š

```python
# chunk_size å¤ªå° âŒ
splitter = RecursiveTextSplitter(chunk_size=50)
# ç»“æœï¼šåˆ‡å¾—å¤ªç¢ï¼Œä¸¢å¤±ä¸Šä¸‹æ–‡
# "Python æ˜¯" / "ä¸€ç§ç¼–ç¨‹" / "è¯­è¨€ï¼Œå®ƒ" ...

# chunk_size å¤ªå¤§ âŒ
splitter = RecursiveTextSplitter(chunk_size=5000)
# ç»“æœï¼šåˆ‡å¾—å¤ªç²—ï¼Œæ£€ç´¢ä¸ç²¾ç¡®
# ä¸€ä¸ªå—åŒ…å«æ•´ç« å†…å®¹ï¼Œæ— å…³ä¿¡æ¯å¤ªå¤š

# chunk_size åˆé€‚ âœ…
splitter = RecursiveTextSplitter(chunk_size=500)
# ç»“æœï¼šåˆšå¥½åŒ…å«å®Œæ•´çš„æ¦‚å¿µæˆ–æ®µè½
# "Python æ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”± Guido van Rossum äº 1991 å¹´åˆ›å»º..."
```

#### chunk_overlapï¼ˆé‡å å¤§å°ï¼‰

**å®šä¹‰**ï¼šç›¸é‚»æ–‡æœ¬å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°ã€‚

**ä¸ºä»€ä¹ˆéœ€è¦é‡å ï¼Ÿ**

```
åŸæ–‡ï¼š
"äººå·¥æ™ºèƒ½åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ åŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€‚"

âŒ æ— é‡å  (overlap=0)ï¼š
å—1: "äººå·¥æ™ºèƒ½åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€‚"
å—2:                   "æœºå™¨å­¦ä¹ åŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€‚"
é—®é¢˜ï¼šå¦‚æœç”¨æˆ·é—® "æœºå™¨å­¦ä¹ ä¸äººå·¥æ™ºèƒ½çš„å…³ç³»"ï¼Œ
     å—1 å’Œå—2 éƒ½ä¸å®Œæ•´

âœ… æœ‰é‡å  (overlap=5)ï¼š
å—1: "äººå·¥æ™ºèƒ½åŒ…æ‹¬æœºå™¨å­¦ä¹ ã€‚"
å—2:         "æœºå™¨å­¦ä¹ ã€‚æœºå™¨å­¦ä¹ åŒ…æ‹¬æ·±åº¦å­¦ä¹ ã€‚"
ä¼˜åŠ¿ï¼šå—2 åŒ…å«äº† "æœºå™¨å­¦ä¹ " çš„å‰åæ–‡ï¼Œè¯­ä¹‰æ›´å®Œæ•´
```

**é€‰æ‹©å»ºè®®**ï¼š

```python
# ç»éªŒå…¬å¼
chunk_overlap = chunk_size * 0.1  # 10% é‡å ï¼ˆæœ€å°ï¼‰
chunk_overlap = chunk_size * 0.2  # 20% é‡å ï¼ˆæ¨èï¼‰

# å…·ä½“ç¤ºä¾‹
chunk_size = 500
chunk_overlap = 50   # 10%ï¼Œé€‚åˆç»“æ„æ¸…æ™°çš„æ–‡æ¡£
chunk_overlap = 100  # 20%ï¼Œé€‚åˆéœ€è¦æ›´å¤šä¸Šä¸‹æ–‡çš„æ–‡æ¡£
```

**å¯è§†åŒ–ç¤ºä¾‹**ï¼š

```python
# åŸæ–‡
text = "ABCDEFGHIJKLMNOPQRSTUVWXYZ"

# é…ç½® 1: chunk_size=10, overlap=0
# ç»“æœ: [ABCDEFGHIJ] [KLMNOPQRST] [UVWXYZ]

# é…ç½® 2: chunk_size=10, overlap=3
# ç»“æœ: [ABCDEFGHIJ] [HIJKLMNOPQ] [OPQRSTUVWX] [VWXYZ]
#                 ^^^         ^^^           ^^^
#                 é‡å éƒ¨åˆ†ï¼ˆä¿è¯è¯­ä¹‰è¿è´¯ï¼‰
```

### 2.3 åˆ‡åˆ†ç­–ç•¥

#### ç­–ç•¥å¯¹æ¯”

| ç­–ç•¥ | åˆ†éš”ç¬¦ | é€‚ç”¨åœºæ™¯ | ä¼˜ç¼ºç‚¹ |
|-----|--------|---------|--------|
| **å›ºå®šé•¿åº¦** | æ—  | ç®€å•æµ‹è¯• | âŒ å¯èƒ½åˆ‡æ–­å¥å­ |
| **æŒ‰å­—ç¬¦** | ç©ºæ ¼ã€æ ‡ç‚¹ | é€šç”¨æ–‡æœ¬ | âœ… ç®€å• âŒ ä¸æ™ºèƒ½ |
| **æŒ‰å¥å­** | ã€‚ï¼ï¼Ÿ | æ–°é—»ã€æ–‡ç«  | âœ… ä¿æŒå®Œæ•´ âŒ å¥å­é•¿åº¦ä¸å‡ |
| **é€’å½’åˆ‡åˆ†** | å¤šçº§åˆ†éš”ç¬¦ | é€šç”¨ï¼ˆæ¨èï¼‰| âœ… æ™ºèƒ½ âœ… è‡ªé€‚åº” |
| **è¯­ä¹‰åˆ‡åˆ†** | AI ç†è§£ | é«˜ç²¾åº¦åœºæ™¯ | âœ… æœ€æ™ºèƒ½ âŒ æ…¢ä¸”è´µ |

#### é€’å½’åˆ‡åˆ†çš„å·¥ä½œæµç¨‹

```
è¾“å…¥: "æ®µè½1\n\næ®µè½2\n\næ®µè½3"
chunk_size=100

æ­¥éª¤1: å°è¯•æŒ‰ "\n\n" (æ®µè½) åˆ†å‰²
  â†“
æ®µè½1 (80å­—ç¬¦) âœ… å°äº 100ï¼Œä¿ç•™
æ®µè½2 (150å­—ç¬¦) âŒ å¤§äº 100ï¼Œç»§ç»­åˆ‡åˆ†
æ®µè½3 (60å­—ç¬¦) âœ… å°äº 100ï¼Œä¿ç•™

æ­¥éª¤2: å¯¹æ®µè½2ï¼Œå°è¯•æŒ‰ "\n" (è¡Œ) åˆ†å‰²
  â†“
è¡Œ1 (80å­—ç¬¦) âœ… å°äº 100ï¼Œä¿ç•™
è¡Œ2 (70å­—ç¬¦) âœ… å°äº 100ï¼Œä¿ç•™

æ­¥éª¤3: å¯¹ä»è¶…é•¿çš„å—ï¼ŒæŒ‰ "ã€‚" (å¥å­) åˆ†å‰²
  â†“
...ä¾æ­¤ç±»æ¨

æœ€ç»ˆç»“æœ: 5ä¸ªå—ï¼Œæ¯ä¸ªéƒ½å°äº 100 å­—ç¬¦ï¼Œä¸”å°½é‡ä¿æŒè¯­ä¹‰å®Œæ•´
```

---

## 3. æ–‡æ¡£åŠ è½½å™¨è¯¦è§£

### 3.1 BaseDocumentLoaderï¼ˆæŠ½è±¡åŸºç±»ï¼‰

#### è®¾è®¡ç›®çš„

```python
# ä¸ºä»€ä¹ˆéœ€è¦æŠ½è±¡åŸºç±»ï¼Ÿ

# âŒ æ²¡æœ‰æŠ½è±¡åŸºç±»ï¼ˆä»£ç é‡å¤ã€éš¾ä»¥æ‰©å±•ï¼‰
def load_txt(file_path):
    # å®ç°ç»†èŠ‚...
    pass

def load_pdf(file_path):
    # å®ç°ç»†èŠ‚...
    pass

def load_docx(file_path):
    # å®ç°ç»†èŠ‚...
    pass

# ä½¿ç”¨æ—¶éœ€è¦æ‰‹åŠ¨åˆ¤æ–­
if file_path.endswith('.txt'):
    docs = load_txt(file_path)
elif file_path.endswith('.pdf'):
    docs = load_pdf(file_path)
# ...å¤ªå¤š if-else


# âœ… æœ‰æŠ½è±¡åŸºç±»ï¼ˆç»Ÿä¸€æ¥å£ã€æ˜“äºæ‰©å±•ï¼‰
class BaseDocumentLoader(ABC):
    @abstractmethod
    def load(self, file_path: str) -> List[Document]:
        pass

# æ‰€æœ‰åŠ è½½å™¨éƒ½å®ç°ç›¸åŒæ¥å£
loader = get_loader(file_path)  # è‡ªåŠ¨é€‰æ‹©
docs = loader.load(file_path)   # ç»Ÿä¸€è°ƒç”¨
```

#### æ¥å£å®šä¹‰

```python
from abc import ABC, abstractmethod
from typing import List
from langchain.schema import Document

class BaseDocumentLoader(ABC):
    """
    æ–‡æ¡£åŠ è½½å™¨æŠ½è±¡åŸºç±»
    
    è®¾è®¡åŸåˆ™ï¼š
    1. å•ä¸€èŒè´£ï¼šæ¯ä¸ªåŠ è½½å™¨åªè´Ÿè´£ä¸€ç§æ ¼å¼
    2. å¼€é—­åŸåˆ™ï¼šå¯¹æ‰©å±•å¼€æ”¾ï¼Œå¯¹ä¿®æ”¹å…³é—­
    3. é‡Œæ°æ›¿æ¢ï¼šå­ç±»å¯ä»¥æ›¿æ¢çˆ¶ç±»
    """
    
    @abstractmethod
    def load(self, file_path: str) -> List[Document]:
        """
        åŠ è½½æ–‡æ¡£çš„ç»Ÿä¸€æ¥å£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            List[Document]: æ–‡æ¡£åˆ—è¡¨
            - æœ‰äº›æ ¼å¼è¿”å›å•ä¸ª Documentï¼ˆå¦‚ TXTï¼‰
            - æœ‰äº›æ ¼å¼è¿”å›å¤šä¸ª Documentï¼ˆå¦‚ PDF çš„æ¯ä¸€é¡µï¼‰
        """
        pass
```

### 3.2 TextLoaderï¼ˆæ–‡æœ¬åŠ è½½å™¨ï¼‰

#### æ ¸å¿ƒå®ç°

```python
from langchain_community.document_loaders import TextLoader as LCTextLoader

class TextLoader(BaseDocumentLoader):
    def __init__(self, encoding: str = "utf-8", autodetect_encoding: bool = True):
        """
        åˆå§‹åŒ–æ–‡æœ¬åŠ è½½å™¨
        
        å‚æ•°è¯´æ˜ï¼š
        - encoding: æ–‡ä»¶ç¼–ç ï¼ˆé»˜è®¤ UTF-8ï¼‰
        - autodetect_encoding: è‡ªåŠ¨æ£€æµ‹ç¼–ç 
          â”œâ”€ True: å¦‚æœ UTF-8 å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç ï¼ˆæ¨èï¼‰
          â””â”€ False: åªä½¿ç”¨æŒ‡å®šç¼–ç ï¼ˆæ›´å¿«ä½†å¯èƒ½å¤±è´¥ï¼‰
        """
        self.encoding = encoding
        self.autodetect_encoding = autodetect_encoding
    
    def load(self, file_path: str) -> List[Document]:
        try:
            # ä½¿ç”¨ LangChain çš„ TextLoader
            loader = LCTextLoader(
                file_path=file_path,
                encoding=self.encoding,
                autodetect_encoding=self.autodetect_encoding
            )
            documents = loader.load()
            return documents
        except Exception as e:
            raise Exception(f"æ–‡æœ¬æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}") from e
```

#### ç¼–ç é—®é¢˜è¯¦è§£

**å¸¸è§ç¼–ç æ ¼å¼**ï¼š

| ç¼–ç  | è¯´æ˜ | ä½¿ç”¨åœºæ™¯ |
|-----|------|---------|
| UTF-8 | é€šç”¨ç¼–ç ï¼Œæ”¯æŒæ‰€æœ‰å­—ç¬¦ | æ¨èï¼Œç°ä»£ç³»ç»Ÿé»˜è®¤ |
| GBK | ä¸­æ–‡ç¼–ç ï¼ˆç®€ä½“ï¼‰ | ä¸­å›½å¤§é™†æ—§ç³»ç»Ÿ |
| GB18030 | ä¸­æ–‡ç¼–ç ï¼ˆæ‰©å±•ï¼‰ | ä¸­å›½å›½å®¶æ ‡å‡† |
| Big5 | ä¸­æ–‡ç¼–ç ï¼ˆç¹ä½“ï¼‰ | ä¸­å›½å°æ¹¾ã€é¦™æ¸¯ |
| Latin-1 | è¥¿æ¬§ç¼–ç  | è‹±æ–‡ç³»ç»Ÿ |

**é—®é¢˜ç¤ºä¾‹**ï¼š

```python
# æ–‡ä»¶å®é™…ç¼–ç æ˜¯ GBKï¼Œä½†ç”¨ UTF-8 è¯»å–
loader = TextLoader(encoding="utf-8", autodetect_encoding=False)
docs = loader.load("gbk_file.txt")
# âŒ é”™è¯¯ï¼šUnicodeDecodeError

# è§£å†³æ–¹æ¡ˆ1ï¼šæŒ‡å®šæ­£ç¡®ç¼–ç 
loader = TextLoader(encoding="gbk")
docs = loader.load("gbk_file.txt")  # âœ… æˆåŠŸ

# è§£å†³æ–¹æ¡ˆ2ï¼šè‡ªåŠ¨æ£€æµ‹ï¼ˆæ¨èï¼‰
loader = TextLoader(autodetect_encoding=True)
docs = loader.load("gbk_file.txt")  # âœ… è‡ªåŠ¨è¯†åˆ«ä¸º GBK
```

### 3.3 PDFLoaderï¼ˆPDF åŠ è½½å™¨ï¼‰

#### æ ¸å¿ƒå®ç°

```python
from langchain_community.document_loaders import PyPDFLoader

class PDFLoader(BaseDocumentLoader):
    def load(self, file_path: str) -> List[Document]:
        """
        åŠ è½½ PDF æ–‡ä»¶
        
        ç‰¹ç‚¹ï¼š
        1. æŒ‰é¡µåŠ è½½ï¼šæ¯ä¸€é¡µæ˜¯ä¸€ä¸ª Document
        2. è‡ªåŠ¨æå–æ–‡æœ¬
        3. ä¿ç•™é¡µç ä¿¡æ¯
        """
        try:
            loader = PyPDFLoader(file_path)
            documents = loader.load()
            
            # documents ç»“æ„:
            # [
            #     Document(page_content="ç¬¬1é¡µå†…å®¹", metadata={"page": 0, "source": "..."}),
            #     Document(page_content="ç¬¬2é¡µå†…å®¹", metadata={"page": 1, "source": "..."}),
            #     ...
            # ]
            
            return documents
        except Exception as e:
            raise Exception(f"PDF æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}") from e
```

#### PDF å¤„ç†æŒ‘æˆ˜

**1. æ–‡å­—æå–ä¸å®Œæ•´**

```python
# é—®é¢˜ï¼šæ‰«æç‰ˆ PDFï¼ˆå›¾ç‰‡ï¼‰
# âŒ PyPDFLoader æ— æ³•æå–æ–‡å­—

# è§£å†³æ–¹æ¡ˆï¼šä½¿ç”¨ OCR
from langchain_community.document_loaders import UnstructuredPDFLoader

loader = UnstructuredPDFLoader(
    file_path="scanned.pdf",
    mode="elements",
    strategy="ocr_only"  # ä½¿ç”¨ OCR è¯†åˆ«
)
```

**2. æ ¼å¼æ··ä¹±**

```python
# é—®é¢˜ï¼šPDF çš„æ–‡æœ¬æå–å¯èƒ½åŒ…å«ï¼š
# - é¡µçœ‰é¡µè„š
# - é¡µç 
# - è¡¨æ ¼æ ¼å¼æ··ä¹±
# - å¤šåˆ—æ’ç‰ˆé¡ºåºé”™è¯¯

# ç¤ºä¾‹ï¼šæå–ç»“æœ
extracted_text = """
ç¬¬ 1 é¡µ                                    æ–‡æ¡£æ ‡é¢˜
å†…å®¹è¡Œ1        å·¦åˆ—æ–‡å­—       å³åˆ—æ–‡å­—
å†…å®¹è¡Œ2        ...           ...
"""

# è§£å†³æ–¹æ¡ˆï¼šé¢„å¤„ç†
def clean_pdf_text(text: str) -> str:
    # ç§»é™¤é¡µçœ‰é¡µè„šï¼ˆæ ¹æ®å…·ä½“æ¨¡å¼ï¼‰
    text = re.sub(r'ç¬¬ \d+ é¡µ.*?\n', '', text)
    # ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    return text
```

**3. å…ƒæ•°æ®åˆ©ç”¨**

```python
# åŠ è½½ PDF
loader = PDFLoader()
documents = loader.load("book.pdf")

# åˆ©ç”¨é¡µç è¿›è¡Œå®šä½
for doc in documents:
    page_num = doc.metadata["page"]
    if page_num >= 10 and page_num <= 20:
        # åªå¤„ç†ç¬¬ 10-20 é¡µ
        process(doc)

# åœ¨ç­”æ¡ˆä¸­å¼•ç”¨é¡µç 
answer = "Python åˆ›å»ºäº 1991 å¹´"
source = f"{doc.metadata['source']}ï¼Œç¬¬ {doc.metadata['page']+1} é¡µ"
print(f"æ¥æºï¼š{source}")
```

### 3.4 MarkdownLoaderï¼ˆMarkdown åŠ è½½å™¨ï¼‰

#### æ ¸å¿ƒå®ç°

```python
from langchain_community.document_loaders import UnstructuredMarkdownLoader

class MarkdownLoader(BaseDocumentLoader):
    def __init__(self, mode: str = "single"):
        """
        mode å‚æ•°ï¼š
        - "single": æ•´ä¸ªæ–‡ä»¶ä½œä¸ºä¸€ä¸ª Document
        - "elements": æŒ‰å…ƒç´ åˆ†å‰²ï¼ˆæ ‡é¢˜ã€æ®µè½ã€åˆ—è¡¨ç­‰ï¼‰
        """
        self.mode = mode
    
    def load(self, file_path: str) -> List[Document]:
        loader = UnstructuredMarkdownLoader(
            file_path=file_path,
            mode=self.mode
        )
        return loader.load()
```

#### Markdown ç‰¹æ®Šå¤„ç†

**1. ä»£ç å—å¤„ç†**

```markdown
# åŸå§‹ Markdown
è¿™æ˜¯ä¸€æ®µè¯´æ˜ã€‚

```python
def hello():
    print("Hello")
```

æ›´å¤šè¯´æ˜ã€‚
```

```python
# åŠ è½½åçš„å¤„ç†
doc = loader.load("example.md")[0]
content = doc.page_content

# é—®é¢˜ï¼šä»£ç å—å¯èƒ½å½±å“è¯­ä¹‰ç†è§£
# è§£å†³æ–¹æ¡ˆ1ï¼šä¿ç•™ä»£ç ï¼ˆé€‚åˆæŠ€æœ¯æ–‡æ¡£ï¼‰
# è§£å†³æ–¹æ¡ˆ2ï¼šç§»é™¤ä»£ç ï¼ˆé€‚åˆçº¯æ–‡æœ¬ç†è§£ï¼‰
def remove_code_blocks(text: str) -> str:
    # ç§»é™¤ä»£ç å—
    text = re.sub(r'```.*?```', '', text, flags=re.DOTALL)
    return text
```

**2. æ ‡é¢˜ç»“æ„åˆ©ç”¨**

```python
# æŒ‰æ ‡é¢˜å±‚çº§åˆ‡åˆ† Markdown
def split_by_headers(markdown_text: str):
    from langchain.text_splitter import MarkdownHeaderTextSplitter
    
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    
    splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )
    
    splits = splitter.split_text(markdown_text)
    # æ¯ä¸ª split åŒ…å«æ ‡é¢˜ä¿¡æ¯çš„ metadata
    return splits
```

### 3.5 DocumentLoaderFactoryï¼ˆå·¥å‚ç±»ï¼‰

#### å®ç°åŸç†

```python
class DocumentLoaderFactory:
    """
    å·¥å‚æ¨¡å¼ï¼šæ ¹æ®æ–‡ä»¶æ‰©å±•åè‡ªåŠ¨é€‰æ‹©åŠ è½½å™¨
    """
    
    # æ–‡ä»¶æ‰©å±•å â†’ åŠ è½½å™¨ç±» çš„æ˜ å°„
    _loaders = {
        '.txt': TextLoader,
        '.pdf': PDFLoader,
        '.md': MarkdownLoader,
        '.markdown': MarkdownLoader,
    }
    
    @classmethod
    def get_loader(cls, file_path: str) -> BaseDocumentLoader:
        """è·å–åŠ è½½å™¨å®ä¾‹"""
        ext = Path(file_path).suffix.lower()  # è·å–æ‰©å±•å
        
        loader_class = cls._loaders.get(ext)
        if loader_class is None:
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
        
        return loader_class()  # è¿”å›å®ä¾‹
    
    @classmethod
    def load(cls, file_path: str) -> List[Document]:
        """ä¾¿æ·æ–¹æ³•ï¼šç›´æ¥åŠ è½½"""
        loader = cls.get_loader(file_path)
        return loader.load(file_path)
```

#### ä½¿ç”¨ç¤ºä¾‹

```python
# æ–¹å¼1ï¼šæ‰‹åŠ¨é€‰æ‹©åŠ è½½å™¨ âŒ ç¹ç
if file_path.endswith('.txt'):
    loader = TextLoader()
elif file_path.endswith('.pdf'):
    loader = PDFLoader()
# ...

# æ–¹å¼2ï¼šä½¿ç”¨å·¥å‚ âœ… ç®€æ´
loader = DocumentLoaderFactory.get_loader(file_path)
documents = loader.load(file_path)

# æ–¹å¼3ï¼šæ›´ç®€æ´ âœ…âœ… æ¨è
documents = DocumentLoaderFactory.load(file_path)
```

#### æ‰©å±•æ–°æ ¼å¼

```python
# éœ€æ±‚ï¼šæ”¯æŒ Word æ–‡æ¡£ (.docx)
from langchain_community.document_loaders import Docx2txtLoader

class DocxLoader(BaseDocumentLoader):
    def load(self, file_path: str) -> List[Document]:
        loader = Docx2txtLoader(file_path)
        return loader.load()

# æ³¨å†Œåˆ°å·¥å‚
DocumentLoaderFactory._loaders['.docx'] = DocxLoader

# ç«‹å³å¯ç”¨
docs = DocumentLoaderFactory.load("document.docx")  # âœ… è‡ªåŠ¨è¯†åˆ«
```

---

## 4. æ–‡æœ¬åˆ‡åˆ†å™¨è¯¦è§£

### 4.1 BaseTextSplitterï¼ˆæŠ½è±¡åŸºç±»ï¼‰

#### æ ¸å¿ƒè®¾è®¡

```python
class BaseTextSplitter(ABC):
    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        length_function: Optional[callable] = None
    ):
        """
        å‚æ•°è¯´æ˜ï¼š
        
        chunk_size: å—å¤§å°
            - é»˜è®¤æŒ‰å­—ç¬¦æ•°è®¡ç®—
            - å¯ä»¥æ”¹ä¸º token æ•°ï¼ˆä½¿ç”¨ length_functionï¼‰
        
        chunk_overlap: é‡å å¤§å°
            - å»ºè®®ä¸º chunk_size çš„ 10%-20%
        
        length_function: é•¿åº¦è®¡ç®—å‡½æ•°
            - é»˜è®¤: len() (å­—ç¬¦æ•°)
            - Token è®¡æ•°: tiktoken.encode()
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.length_function = length_function or len
        
        # å‚æ•°éªŒè¯
        assert chunk_size > 0
        assert chunk_overlap >= 0
        assert chunk_overlap < chunk_size
    
    @abstractmethod
    def split_documents(self, documents: List[Document]) -> List[Document]:
        """åˆ‡åˆ†æ–‡æ¡£åˆ—è¡¨"""
        pass
```

#### å­—ç¬¦æ•° vs Token æ•°

**ä¸ºä»€ä¹ˆè¦å…³å¿ƒ Tokenï¼Ÿ**

```python
# é—®é¢˜ï¼šLLM çš„é™åˆ¶æ˜¯ Token æ•°ï¼Œä¸æ˜¯å­—ç¬¦æ•°
text = "ä½ å¥½ä¸–ç•Œ"

# å­—ç¬¦æ•°
char_count = len(text)  # 4

# Token æ•°ï¼ˆä½¿ç”¨ OpenAI çš„ tokenizerï¼‰
import tiktoken
encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
token_count = len(encoding.encode(text))  # 6

# ä¸­æ–‡ï¼šé€šå¸¸ 1 ä¸ªå­— â‰ˆ 1.5-2 ä¸ª tokens
# è‹±æ–‡ï¼šé€šå¸¸ 1 ä¸ªå•è¯ â‰ˆ 1-2 ä¸ª tokens
```

**å¦‚ä½•æŒ‰ Token åˆ‡åˆ†ï¼Ÿ**

```python
import tiktoken

def token_counter(text: str) -> int:
    """è®¡ç®— token æ•°"""
    encoding = tiktoken.encoding_for_model("gpt-3.5-turbo")
    return len(encoding.encode(text))

# ä½¿ç”¨ token è®¡æ•°å™¨
splitter = RecursiveTextSplitter(
    chunk_size=500,      # 500 tokensï¼ˆä¸æ˜¯å­—ç¬¦ï¼‰
    chunk_overlap=50,
    length_function=token_counter  # ä½¿ç”¨ token è®¡æ•°
)

chunks = splitter.split_documents(documents)
# æ¯ä¸ª chunk ç°åœ¨æ˜¯ ~500 tokens
```

### 4.2 RecursiveTextSplitterï¼ˆé€’å½’åˆ‡åˆ†å™¨ï¼‰

#### å·¥ä½œåŸç†è¯¦è§£

**åˆ†éš”ç¬¦ä¼˜å…ˆçº§**ï¼š

```python
separators = [
    "\n\n",   # ä¼˜å…ˆçº§ 1: æ®µè½åˆ†éš”
    "\n",     # ä¼˜å…ˆçº§ 2: è¡Œåˆ†éš”
    "ã€‚",     # ä¼˜å…ˆçº§ 3: ä¸­æ–‡å¥å·
    "ï¼",     # ä¼˜å…ˆçº§ 4: ä¸­æ–‡æ„Ÿå¹å·
    "ï¼Ÿ",     # ä¼˜å…ˆçº§ 5: ä¸­æ–‡é—®å·
    ".",      # ä¼˜å…ˆçº§ 6: è‹±æ–‡å¥å·
    " ",      # ä¼˜å…ˆçº§ 7: ç©ºæ ¼
    "",       # ä¼˜å…ˆçº§ 8: å­—ç¬¦çº§ï¼ˆå…œåº•ï¼‰
]
```

**é€’å½’æµç¨‹å›¾**ï¼š

```
è¾“å…¥æ–‡æœ¬
    â†“
å°è¯•åˆ†éš”ç¬¦1 ("\n\n")
    â†“
åˆ†å‰²åçš„å— â”€â”€â”€â†’ æ£€æŸ¥æ¯ä¸ªå—çš„å¤§å°
    â†“                    â†“
    â”œâ”€ å— â‰¤ chunk_size   ä¿ç•™ âœ…
    â””â”€ å— > chunk_size   ç»§ç»­é€’å½’
              â†“
        å°è¯•åˆ†éš”ç¬¦2 ("\n")
              â†“
        åˆ†å‰²åçš„å— â”€â”€â”€â†’ ...
```

**å®Œæ•´ç¤ºä¾‹**ï¼š

```python
text = """äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒä¸“æ³¨äºåˆ›å»ºæ™ºèƒ½ç³»ç»Ÿã€‚

æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚

è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚"""

splitter = RecursiveTextSplitter(
    chunk_size=50,  # æ•…æ„è®¾å°ï¼Œå±•ç¤ºé€’å½’è¿‡ç¨‹
    chunk_overlap=10,
    separators=["\n\n", "\n", "ã€‚", ""]
)

chunks = splitter.split_text(text)

# åˆ‡åˆ†è¿‡ç¨‹ï¼š
# 1. æŒ‰ "\n\n" åˆ†å‰²æˆ 3 æ®µ
# 2. ç¬¬ 1 æ®µ 53 å­—ç¬¦ > 50ï¼ŒæŒ‰ "ã€‚" ç»§ç»­åˆ†å‰²
#    â†’ "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚"
#    â†’ "å®ƒä¸“æ³¨äºåˆ›å»ºæ™ºèƒ½ç³»ç»Ÿã€‚"
# 3. ç¬¬ 2 æ®µä¹Ÿè¶…é•¿ï¼Œç»§ç»­åˆ†å‰²...

# æœ€ç»ˆç»“æœï¼š
# ['äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚',
#  'å®ƒä¸“æ³¨äºåˆ›å»ºæ™ºèƒ½ç³»ç»Ÿã€‚',
#  'æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„æ ¸å¿ƒæŠ€æœ¯ã€‚',
#  'æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚',
#  'è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºç†è§£äººç±»è¯­è¨€ã€‚']
```

#### keep_separator å‚æ•°

```python
text = "ä½ å¥½ã€‚ä¸–ç•Œã€‚"

# keep_separator=Trueï¼ˆä¿ç•™åˆ†éš”ç¬¦ï¼‰
splitter = RecursiveTextSplitter(separators=["ã€‚"], keep_separator=True)
# ç»“æœ: ["ä½ å¥½ã€‚", "ä¸–ç•Œã€‚"]

# keep_separator=Falseï¼ˆä¸ä¿ç•™åˆ†éš”ç¬¦ï¼‰
splitter = RecursiveTextSplitter(separators=["ã€‚"], keep_separator=False)
# ç»“æœ: ["ä½ å¥½", "ä¸–ç•Œ"]

# å»ºè®®ï¼škeep_separator=True
# ç†ç”±ï¼šä¿ç•™æ ‡ç‚¹ç¬¦å·ï¼Œè¯­ä¹‰æ›´å®Œæ•´
```

### 4.3 ChineseTextSplitterï¼ˆä¸­æ–‡åˆ‡åˆ†å™¨ï¼‰

#### ä¸­æ–‡ç‰¹æ®Šæ€§

**1. æ²¡æœ‰ç©ºæ ¼åˆ†è¯**

```python
# è‹±æ–‡ï¼šå¤©ç„¶çš„ç©ºæ ¼åˆ†éš”
english = "This is a sentence."
words = english.split()  # ['This', 'is', 'a', 'sentence.']

# ä¸­æ–‡ï¼šæ²¡æœ‰ç©ºæ ¼
chinese = "è¿™æ˜¯ä¸€ä¸ªå¥å­ã€‚"
words = chinese.split()  # ['è¿™æ˜¯ä¸€ä¸ªå¥å­ã€‚'] â† æ— æ³•åˆ†è¯
```

**2. æ ‡ç‚¹ç¬¦å·ä¸åŒ**

```python
# è‹±æ–‡æ ‡ç‚¹: . , ! ? ; :
# ä¸­æ–‡æ ‡ç‚¹: ã€‚ ï¼Œ ï¼ ï¼Ÿ ï¼› ï¼š

# éœ€è¦åŒæ—¶è€ƒè™‘ä¸­è‹±æ–‡æ ‡ç‚¹
separators = [
    "ã€‚", "ï¼", "ï¼Ÿ",  # ä¸­æ–‡
    ".", "!", "?",      # è‹±æ–‡
]
```

**3. PDF æå–é—®é¢˜**

```python
# PDF æå–çš„ä¸­æ–‡å¸¸è§é—®é¢˜ï¼š
original = "äººå·¥æ™ºèƒ½å¾ˆé‡è¦"

# é—®é¢˜1ï¼šå­—é—´æ’å…¥ç©ºæ ¼
extracted1 = "äºº å·¥ æ™º èƒ½ å¾ˆ é‡ è¦"

# é—®é¢˜2ï¼šç¼–ç é—®é¢˜
extracted2 = "äººå·¥æ™ºèƒ½å¯°å ¥å™¸ç‘•ï¿½"  # ä¹±ç 

# é—®é¢˜3ï¼šæ’ç‰ˆæ··ä¹±
extracted3 = "äººå·¥\næ™ºèƒ½å¾ˆ\né‡è¦"  # ä¸å¿…è¦çš„æ¢è¡Œ
```

#### ChineseTextSplitter å®ç°

```python
class ChineseTextSplitter(BaseTextSplitter):
    def __init__(
        self,
        chunk_size: int = 300,  # ä¸­æ–‡å»ºè®®æ›´å°
        chunk_overlap: int = 30,
        pdf_mode: bool = False  # PDF æ¨¡å¼
    ):
        super().__init__(chunk_size, chunk_overlap)
        self.pdf_mode = pdf_mode
        
        # ä¸­æ–‡ä¼˜å…ˆçš„åˆ†éš”ç¬¦
        self.chinese_separators = [
            "\n\n", "\n",
            "ã€‚", "ï¼", "ï¼Ÿ",  # ä¸­æ–‡å¥å­
            "ï¼›", "ï¼Œ",        # ä¸­æ–‡çŸ­å¥
        ]
    
    def _preprocess_text(self, text: str) -> str:
        """é¢„å¤„ç†ä¸­æ–‡æ–‡æœ¬"""
        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        if self.pdf_mode:
            # PDF ç‰¹æ®Šå¤„ç†
            # 1. ç§»é™¤è¡Œå†…å¤šä½™ç©ºæ ¼
            text = re.sub(r' +', ' ', text)
            
            # 2. ç§»é™¤ä¸­æ–‡å­—ç¬¦é—´çš„ç©ºæ ¼
            # "äºº å·¥ æ™º èƒ½" â†’ "äººå·¥æ™ºèƒ½"
            text = re.sub(r'([\u4e00-\u9fa5]) +([\u4e00-\u9fa5])', r'\1\2', text)
        
        # 3. ç§»é™¤å¤šä½™ç©ºè¡Œ
        text = re.sub(r'\n{3,}', '\n\n', text)
        
        return text.strip()
```

#### ä¸­æ–‡åˆ†è¯åˆ‡åˆ†ï¼ˆè¿›é˜¶ï¼‰

```python
# ä½¿ç”¨ jieba è¿›è¡Œä¸­æ–‡åˆ†è¯
import jieba

def chinese_word_splitter(text: str, chunk_size: int = 200):
    """åŸºäºåˆ†è¯çš„ä¸­æ–‡åˆ‡åˆ†"""
    # åˆ†è¯
    words = jieba.lcut(text)  # ['äººå·¥æ™ºèƒ½', 'æ˜¯', 'è®¡ç®—æœº', 'ç§‘å­¦', ...]
    
    # æŒ‰è¯ç»„è£…æˆå—
    chunks = []
    current_chunk = []
    current_length = 0
    
    for word in words:
        current_chunk.append(word)
        current_length += len(word)
        
        if current_length >= chunk_size:
            chunks.append(''.join(current_chunk))
            current_chunk = []
            current_length = 0
    
    if current_chunk:
        chunks.append(''.join(current_chunk))
    
    return chunks

# ä¼˜åŠ¿ï¼šæŒ‰è¯åˆ‡åˆ†ï¼Œä¸ä¼šåˆ‡æ–­è¯è¯­
# åŠ£åŠ¿ï¼šéœ€è¦é¢å¤–ä¾èµ–ï¼ˆjiebaï¼‰ï¼Œé€Ÿåº¦è¾ƒæ…¢
```

### 4.4 TextSplitterFactoryï¼ˆå·¥å‚ç±»ï¼‰

#### å®ç°

```python
class TextSplitterFactory:
    @classmethod
    def get_splitter(
        cls,
        splitter_type: str = "recursive",
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        **kwargs
    ) -> BaseTextSplitter:
        """
        è·å–åˆ‡åˆ†å™¨
        
        Args:
            splitter_type: ç±»å‹
                - "recursive": é€šç”¨é€’å½’åˆ‡åˆ†å™¨
                - "chinese": ä¸­æ–‡åˆ‡åˆ†å™¨
        """
        if splitter_type == "recursive":
            return RecursiveTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                **kwargs
            )
        elif splitter_type == "chinese":
            return ChineseTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=chunk_overlap,
                **kwargs
            )
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ‡åˆ†å™¨ç±»å‹: {splitter_type}")
```

#### ä½¿ç”¨åœºæ™¯

```python
# åœºæ™¯1ï¼šé€šç”¨æ–‡æ¡£ï¼ˆè‹±æ–‡æˆ–ä¸­è‹±æ··åˆï¼‰
splitter = TextSplitterFactory.get_splitter("recursive")

# åœºæ™¯2ï¼šä¸­æ–‡æ–‡æ¡£
splitter = TextSplitterFactory.get_splitter("chinese")

# åœºæ™¯3ï¼šPDF æå–çš„ä¸­æ–‡
splitter = TextSplitterFactory.get_splitter(
    "chinese",
    pdf_mode=True  # é¢å¤–çš„é¢„å¤„ç†
)

# åœºæ™¯4ï¼šä¸€æ­¥åˆ°ä½
chunks = TextSplitterFactory.split(
    documents=documents,
    splitter_type="chinese",
    chunk_size=300
)
```

---

## 5. è®¾è®¡æ¨¡å¼åˆ†æ

### 5.1 ç­–ç•¥æ¨¡å¼ï¼ˆStrategy Patternï¼‰

**å®šä¹‰**ï¼šå®šä¹‰ä¸€ç³»åˆ—ç®—æ³•ï¼ŒæŠŠå®ƒä»¬å°è£…èµ·æ¥ï¼Œå¹¶ä½¿å®ƒä»¬å¯ä»¥äº’ç›¸æ›¿æ¢ã€‚

**åº”ç”¨åœºæ™¯**ï¼šä¸åŒçš„åŠ è½½å™¨ã€ä¸åŒçš„åˆ‡åˆ†å™¨

```python
# é—®é¢˜ï¼šå¦‚ä½•å¤„ç†å¤šç§æ–‡ä»¶æ ¼å¼ï¼Ÿ
# ä¼ ç»Ÿåšæ³•ï¼šå¤§é‡ if-else
def load_document(file_path):
    if file_path.endswith('.txt'):
        # å¤„ç† TXT
        pass
    elif file_path.endswith('.pdf'):
        # å¤„ç† PDF
        pass
    # ...
    # âŒ é—®é¢˜ï¼šæ‰©å±•æ–°æ ¼å¼éœ€è¦ä¿®æ”¹è¿™ä¸ªå‡½æ•°

# ç­–ç•¥æ¨¡å¼ï¼šæ¯ç§æ ¼å¼æ˜¯ä¸€ä¸ªç­–ç•¥
class LoadStrategy(ABC):
    @abstractmethod
    def load(self, file_path): pass

class TxtStrategy(LoadStrategy):
    def load(self, file_path): ...

class PdfStrategy(LoadStrategy):
    def load(self, file_path): ...

# ä½¿ç”¨ï¼šç­–ç•¥å¯ä»¥åŠ¨æ€åˆ‡æ¢
strategy = get_strategy(file_path)
docs = strategy.load(file_path)
# âœ… ä¼˜åŠ¿ï¼šæ–°å¢æ ¼å¼åªéœ€æ·»åŠ æ–°ç­–ç•¥ï¼Œæ— éœ€ä¿®æ”¹åŸæœ‰ä»£ç 
```

**æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š

```python
# åŠ è½½å™¨ç­–ç•¥
loader: BaseDocumentLoader = select_loader(file_path)
docs = loader.load(file_path)  # ç»Ÿä¸€æ¥å£

# åˆ‡åˆ†å™¨ç­–ç•¥
splitter: BaseTextSplitter = select_splitter(doc_type)
chunks = splitter.split_documents(docs)  # ç»Ÿä¸€æ¥å£
```

### 5.2 å·¥å‚æ¨¡å¼ï¼ˆFactory Patternï¼‰

**å®šä¹‰**ï¼šæä¾›ä¸€ä¸ªåˆ›å»ºå¯¹è±¡çš„æ¥å£ï¼Œè®©å­ç±»å†³å®šå®ä¾‹åŒ–å“ªä¸ªç±»ã€‚

**åº”ç”¨åœºæ™¯**ï¼šDocumentLoaderFactory, TextSplitterFactory

```python
# ç®€å•å·¥å‚
class LoaderFactory:
    @staticmethod
    def create_loader(file_type: str):
        if file_type == 'txt':
            return TextLoader()
        elif file_type == 'pdf':
            return PDFLoader()
        # ...

# ä¼˜åŠ¿ï¼š
# 1. å®¢æˆ·ç«¯æ— éœ€çŸ¥é“å…·ä½“ç±»å
# 2. é›†ä¸­ç®¡ç†å¯¹è±¡åˆ›å»ºé€»è¾‘
# 3. æ˜“äºæ‰©å±•æ–°ç±»å‹
```

**æœ¬é¡¹ç›®ä¸­çš„åº”ç”¨**ï¼š

```python
# 1. æ³¨å†Œæœºåˆ¶
DocumentLoaderFactory._loaders = {
    '.txt': TextLoader,
    '.pdf': PDFLoader,
    # ...
}

# 2. æ ¹æ®æ¡ä»¶åˆ›å»º
ext = Path(file_path).suffix
loader_class = _loaders[ext]
loader = loader_class()

# 3. å®¢æˆ·ç«¯ä½¿ç”¨
docs = DocumentLoaderFactory.load("any_file.pdf")
# æ— éœ€çŸ¥é“å†…éƒ¨ä½¿ç”¨äº† PDFLoader
```

### 5.3 æ¨¡æ¿æ–¹æ³•æ¨¡å¼ï¼ˆTemplate Method Patternï¼‰

**å®šä¹‰**ï¼šå®šä¹‰ç®—æ³•çš„éª¨æ¶ï¼Œå°†æŸäº›æ­¥éª¤å»¶è¿Ÿåˆ°å­ç±»å®ç°ã€‚

**åº”ç”¨åœºæ™¯**ï¼šBaseTextSplitter

```python
class BaseTextSplitter(ABC):
    def split_documents(self, documents: List[Document]):
        """æ¨¡æ¿æ–¹æ³•ï¼šå®šä¹‰ç®—æ³•éª¨æ¶"""
        results = []
        for doc in documents:
            # æ­¥éª¤1ï¼šé¢„å¤„ç†ï¼ˆå¯ç”±å­ç±»è¦†ç›–ï¼‰
            processed = self._preprocess(doc)
            
            # æ­¥éª¤2ï¼šåˆ‡åˆ†ï¼ˆç”±å­ç±»å®ç°ï¼‰
            chunks = self._split(processed)
            
            # æ­¥éª¤3ï¼šåå¤„ç†ï¼ˆå¯ç”±å­ç±»è¦†ç›–ï¼‰
            processed_chunks = self._postprocess(chunks)
            
            results.extend(processed_chunks)
        return results
    
    @abstractmethod
    def _split(self, doc): 
        """ç”±å­ç±»å®ç°å…·ä½“åˆ‡åˆ†é€»è¾‘"""
        pass
    
    def _preprocess(self, doc):
        """é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥è¦†ç›–"""
        return doc
    
    def _postprocess(self, chunks):
        """é»˜è®¤å®ç°ï¼Œå­ç±»å¯ä»¥è¦†ç›–"""
        return chunks
```

### 5.4 é€‚é…å™¨æ¨¡å¼ï¼ˆAdapter Patternï¼‰

**å®šä¹‰**ï¼šå°†ä¸€ä¸ªç±»çš„æ¥å£è½¬æ¢æˆå®¢æˆ·ç«¯æœŸæœ›çš„å¦ä¸€ä¸ªæ¥å£ã€‚

**åº”ç”¨åœºæ™¯**ï¼šå°è£… LangChain çš„åŠ è½½å™¨

```python
# LangChain çš„æ¥å£
from langchain_community.document_loaders import PyPDFLoader as LC_PDFLoader

# æˆ‘ä»¬çš„æ¥å£
class PDFLoader(BaseDocumentLoader):
    def load(self, file_path: str):
        # é€‚é… LangChain çš„æ¥å£åˆ°æˆ‘ä»¬çš„æ¥å£
        lc_loader = LC_PDFLoader(file_path)
        return lc_loader.load()

# ä¼˜åŠ¿ï¼š
# 1. ç»Ÿä¸€é¡¹ç›®å†…çš„æ¥å£é£æ ¼
# 2. å¦‚æœæ›´æ¢åº•å±‚åº“ï¼Œåªéœ€ä¿®æ”¹é€‚é…å™¨
# 3. å¯ä»¥åœ¨é€‚é…å™¨ä¸­æ·»åŠ é¢å¤–é€»è¾‘ï¼ˆå¦‚é”™è¯¯å¤„ç†ï¼‰
```

---

## 6. å‚æ•°é…ç½®æŒ‡å—

### 6.1 chunk_size é€‰æ‹©æŒ‡å—

#### æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©

| æ–‡æ¡£ç±»å‹ | chunk_size | ç†ç”± |
|---------|-----------|------|
| **æŠ€æœ¯æ–‡æ¡£** | 400-600 | ä»£ç ç¤ºä¾‹å’Œè§£é‡Šéœ€è¦ä¸€èµ· |
| **API æ–‡æ¡£** | 300-500 | æ¯ä¸ª API æ˜¯ç‹¬ç«‹å•å…ƒ |
| **æ–°é—»æ–‡ç« ** | 500-800 | æ®µè½è¾ƒé•¿ï¼Œéœ€è¦å®Œæ•´è¯­å¢ƒ |
| **å­¦æœ¯è®ºæ–‡** | 800-1200 | è®ºè¯è¿‡ç¨‹éœ€è¦æ›´å¤šä¸Šä¸‹æ–‡ |
| **å¯¹è¯è®°å½•** | 200-300 | å¯¹è¯è½®æ¬¡çŸ­ï¼Œç»†ç²’åº¦æ›´å¥½ |
| **ä»£ç æ–‡ä»¶** | 500-800 | å‡½æ•°/ç±»éœ€è¦å®Œæ•´ |
| **FAQ** | 200-400 | æ¯ä¸ª QA å¯¹ç‹¬ç«‹ |

#### æ ¹æ® LLM çš„ä¸Šä¸‹æ–‡çª—å£é€‰æ‹©

```python
# 1. è®¡ç®—å¯ç”¨çš„ä¸Šä¸‹æ–‡ç©ºé—´
max_tokens = 4096  # GPT-3.5-turbo
prompt_template_tokens = 200  # Prompt æ¨¡æ¿å ç”¨
answer_tokens = 500  # é¢„ç•™ç»™ç­”æ¡ˆ

available_for_context = max_tokens - prompt_template_tokens - answer_tokens
# = 3396 tokens

# 2. è®¡ç®—èƒ½æ”¾å…¥å¤šå°‘ä¸ªå—
k = 4  # æ£€ç´¢ 4 ä¸ªç›¸å…³æ–‡æ¡£å—
tokens_per_chunk = available_for_context / k
# = 849 tokens per chunk

# 3. è½¬æ¢ä¸ºå­—ç¬¦æ•°ï¼ˆä¸­æ–‡: 1 å­— â‰ˆ 2 tokensï¼‰
chunk_size = tokens_per_chunk / 2
# â‰ˆ 425 å­—ç¬¦

# ç»“è®ºï¼šè®¾ç½® chunk_size = 400-450
```

#### æ ¹æ®æ£€ç´¢ç²¾åº¦éœ€æ±‚é€‰æ‹©

```python
# é«˜ç²¾åº¦åœºæ™¯ï¼ˆç²¾ç¡®å®šä½ä¿¡æ¯ï¼‰
chunk_size = 200-300
# ä¼˜åŠ¿ï¼šæ¯ä¸ªå—å†…å®¹å°‘ï¼Œæ£€ç´¢æ›´ç²¾ç¡®
# åŠ£åŠ¿ï¼šå¯èƒ½åˆ‡æ–­å®Œæ•´è¯­å¢ƒ

# å¹³è¡¡åœºæ™¯ï¼ˆæ¨èï¼‰
chunk_size = 400-600
# ä¼˜åŠ¿ï¼šç²¾åº¦å’Œè¯­å¢ƒå…¼é¡¾
# åŠ£åŠ¿ï¼šæ— 

# é‡è§†è¯­å¢ƒåœºæ™¯
chunk_size = 800-1200
# ä¼˜åŠ¿ï¼šåŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ï¼Œç­”æ¡ˆæ›´å®Œæ•´
# åŠ£åŠ¿ï¼šæ£€ç´¢å¯èƒ½ä¸å¤Ÿç²¾ç¡®ï¼Œæ— å…³ä¿¡æ¯å¢å¤š
```

### 6.2 chunk_overlap é€‰æ‹©æŒ‡å—

#### ç»éªŒå…¬å¼

```python
# å…¬å¼1ï¼šå›ºå®šæ¯”ä¾‹
chunk_overlap = chunk_size * 0.1  # 10% é‡å ï¼ˆæœ€å°ï¼‰
chunk_overlap = chunk_size * 0.15 # 15% é‡å ï¼ˆæ¨èï¼‰
chunk_overlap = chunk_size * 0.2  # 20% é‡å ï¼ˆæœ€å¤§ï¼‰

# å…¬å¼2ï¼šå›ºå®šå€¼
chunk_overlap = 50   # é€‚åˆ chunk_size = 300-500
chunk_overlap = 100  # é€‚åˆ chunk_size = 500-800
chunk_overlap = 150  # é€‚åˆ chunk_size = 800-1200
```

#### å®éªŒå¯¹æ¯”

```python
# æµ‹è¯•æ•°æ®
text = """äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒä¸“æ³¨äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œéœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚
æœºå™¨å­¦ä¹ æ˜¯ AI çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚
æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚"""

# é…ç½®1: æ— é‡å 
chunks_no_overlap = split(text, chunk_size=100, overlap=0)
# ç»“æœ: 3 ä¸ªå—
# å—1: "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒä¸“æ³¨äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œéœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
# å—2: "æœºå™¨å­¦ä¹ æ˜¯ AI çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
# å—3: "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚"
# é—®é¢˜ï¼šå¦‚æœæŸ¥è¯¢ "æœºå™¨å­¦ä¹ ä¸äººå·¥æ™ºèƒ½çš„å…³ç³»"ï¼Œå—1 å’Œå—2 éƒ½ä¸å®Œæ•´

# é…ç½®2: 10 å­—ç¬¦é‡å 
chunks_with_overlap = split(text, chunk_size=100, overlap=10)
# å—1: "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ã€‚å®ƒä¸“æ³¨äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œéœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚"
# å—2:                               "çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚æœºå™¨å­¦ä¹ æ˜¯ AI çš„æ ¸å¿ƒæŠ€æœ¯ä¹‹ä¸€ã€‚å®ƒè®©è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ ã€‚"
# å—3:                                                       "ä»æ•°æ®ä¸­å­¦ä¹ ã€‚æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„å­é¢†åŸŸã€‚å®ƒä½¿ç”¨ç¥ç»ç½‘ç»œè¿›è¡Œå­¦ä¹ ã€‚"
# ä¼˜åŠ¿ï¼šå—2 åŒ…å«äº† "äººå·¥æ™ºèƒ½" åˆ° "æœºå™¨å­¦ä¹ " çš„è¿‡æ¸¡ï¼Œè¯­ä¹‰æ›´å®Œæ•´
```

### 6.3 å®Œæ•´é…ç½®ç¤ºä¾‹

```python
# é…ç½®1: æŠ€æœ¯åšå®¢
config_tech_blog = {
    "chunk_size": 500,
    "chunk_overlap": 50,
    "splitter_type": "recursive"
}

# é…ç½®2: å­¦æœ¯è®ºæ–‡ï¼ˆè‹±æ–‡ï¼‰
config_academic_en = {
    "chunk_size": 1000,
    "chunk_overlap": 100,
    "splitter_type": "recursive",
    "separators": ["\n\n", "\n", ". ", "? ", "! ", ";", ",", " ", ""]
}

# é…ç½®3: ä¸­æ–‡æŠ€æœ¯æ–‡æ¡£
config_chinese_doc = {
    "chunk_size": 400,
    "chunk_overlap": 40,
    "splitter_type": "chinese"
}

# é…ç½®4: PDF æå–çš„ä¸­æ–‡
config_chinese_pdf = {
    "chunk_size": 300,
    "chunk_overlap": 30,
    "splitter_type": "chinese",
    "pdf_mode": True
}

# é…ç½®5: ä»£ç æ–‡ä»¶
config_code = {
    "chunk_size": 600,
    "chunk_overlap": 60,
    "splitter_type": "recursive",
    "separators": ["\n\nclass ", "\n\ndef ", "\n\n", "\n", " ", ""]
}

# ä½¿ç”¨é…ç½®
splitter = TextSplitterFactory.get_splitter(**config_chinese_doc)
chunks = splitter.split_documents(documents)
```

---

## 7. å®æˆ˜æ¡ˆä¾‹

### 7.1 æ¡ˆä¾‹1ï¼šåŠ è½½å¹¶åˆ‡åˆ†æŠ€æœ¯æ–‡æ¡£

```python
from src.core.document import DocumentLoaderFactory, RecursiveTextSplitter

# æ­¥éª¤1: åŠ è½½æ–‡æ¡£
documents = DocumentLoaderFactory.load("python_tutorial.pdf")
print(f"åŠ è½½äº† {len(documents)} é¡µ")

# æ­¥éª¤2: åˆ‡åˆ†æ–‡æ¡£
splitter = RecursiveTextSplitter(
    chunk_size=500,
    chunk_overlap=50
)
chunks = splitter.split_documents(documents)
print(f"åˆ‡åˆ†æˆ {len(chunks)} ä¸ªå—")

# æ­¥éª¤3: æ£€æŸ¥åˆ‡åˆ†è´¨é‡
for i, chunk in enumerate(chunks[:3]):
    print(f"\nå— {i+1}:")
    print(f"é•¿åº¦: {len(chunk.page_content)}")
    print(f"å†…å®¹: {chunk.page_content[:100]}...")
    print(f"æ¥æº: {chunk.metadata}")
```

### 7.2 æ¡ˆä¾‹2ï¼šæ‰¹é‡å¤„ç†æ–‡æ¡£ç›®å½•

```python
from pathlib import Path

def process_document_directory(dir_path: str, chunk_size: int = 500):
    """æ‰¹é‡å¤„ç†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡æ¡£"""
    dir_path = Path(dir_path)
    all_chunks = []
    
    # éå†æ‰€æœ‰æ–‡ä»¶
    for file_path in dir_path.glob("**/*"):
        if file_path.is_file():
            try:
                # åŠ è½½
                docs = DocumentLoaderFactory.load(str(file_path))
                print(f"âœ… åŠ è½½: {file_path.name}")
                
                # åˆ‡åˆ†
                splitter = RecursiveTextSplitter(
                    chunk_size=chunk_size,
                    chunk_overlap=chunk_size // 10
                )
                chunks = splitter.split_documents(docs)
                all_chunks.extend(chunks)
                
            except Exception as e:
                print(f"âŒ å¤±è´¥: {file_path.name} - {e}")
    
    print(f"\næ€»è®¡: {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    return all_chunks

# ä½¿ç”¨
chunks = process_document_directory("./knowledge_base/")
```

### 7.3 æ¡ˆä¾‹3ï¼šé’ˆå¯¹ PDF çš„ç‰¹æ®Šå¤„ç†

```python
def load_and_process_pdf(pdf_path: str):
    """åŠ è½½ PDF å¹¶è¿›è¡Œç‰¹æ®Šå¤„ç†"""
    # 1. åŠ è½½ PDF
    loader = PDFLoader()
    documents = loader.load(pdf_path)
    
    # 2. è¿‡æ»¤ç©ºé¡µ
    documents = [doc for doc in documents if len(doc.page_content.strip()) > 50]
    print(f"è¿‡æ»¤åå‰©ä½™ {len(documents)} é¡µ")
    
    # 3. æ¸…ç†æ–‡æœ¬
    for doc in documents:
        # ç§»é™¤é¡µçœ‰é¡µè„šï¼ˆå‡è®¾é¡µçœ‰æ˜¯ "ç¬¬ X é¡µ"ï¼‰
        doc.page_content = re.sub(r'ç¬¬ \d+ é¡µ', '', doc.page_content)
        # ç§»é™¤å¤šä½™ç©ºæ ¼
        doc.page_content = re.sub(r' +', ' ', doc.page_content)
        # ç§»é™¤å¤šä½™æ¢è¡Œ
        doc.page_content = re.sub(r'\n{3,}', '\n\n', doc.page_content)
    
    # 4. åˆ‡åˆ†ï¼ˆä½¿ç”¨ä¸­æ–‡åˆ‡åˆ†å™¨ + PDF æ¨¡å¼ï¼‰
    splitter = ChineseTextSplitter(
        chunk_size=400,
        chunk_overlap=40,
        pdf_mode=True
    )
    chunks = splitter.split_documents(documents)
    
    # 5. æ·»åŠ é¢å¤–å…ƒæ•°æ®
    for i, chunk in enumerate(chunks):
        chunk.metadata['chunk_id'] = i
        chunk.metadata['total_chunks'] = len(chunks)
    
    return chunks

# ä½¿ç”¨
chunks = load_and_process_pdf("research_paper.pdf")
```

### 7.4 æ¡ˆä¾‹4ï¼šæ™ºèƒ½é€‰æ‹©åˆ‡åˆ†ç­–ç•¥

```python
def smart_split(documents: List[Document]) -> List[Document]:
    """æ ¹æ®æ–‡æ¡£å†…å®¹æ™ºèƒ½é€‰æ‹©åˆ‡åˆ†ç­–ç•¥"""
    # æ£€æµ‹æ–‡æ¡£è¯­è¨€
    sample_text = documents[0].page_content[:500]
    
    # ç®€å•çš„ä¸­æ–‡æ£€æµ‹ï¼ˆæ£€æŸ¥ä¸­æ–‡å­—ç¬¦æ¯”ä¾‹ï¼‰
    chinese_chars = len(re.findall(r'[\u4e00-\u9fa5]', sample_text))
    total_chars = len(sample_text)
    chinese_ratio = chinese_chars / total_chars if total_chars > 0 else 0
    
    # æ ¹æ®è¯­è¨€é€‰æ‹©åˆ‡åˆ†å™¨
    if chinese_ratio > 0.3:  # ä¸­æ–‡ä¸ºä¸»
        print("æ£€æµ‹åˆ°ä¸­æ–‡æ–‡æ¡£ï¼Œä½¿ç”¨ä¸­æ–‡åˆ‡åˆ†å™¨")
        splitter = ChineseTextSplitter(
            chunk_size=300,
            chunk_overlap=30
        )
    else:  # è‹±æ–‡ä¸ºä¸»
        print("æ£€æµ‹åˆ°è‹±æ–‡æ–‡æ¡£ï¼Œä½¿ç”¨é€’å½’åˆ‡åˆ†å™¨")
        splitter = RecursiveTextSplitter(
            chunk_size=600,
            chunk_overlap=60
        )
    
    return splitter.split_documents(documents)

# ä½¿ç”¨
chunks = smart_split(documents)
```

### 7.5 æ¡ˆä¾‹5ï¼šä¿ç•™æ–‡æ¡£ç»“æ„ä¿¡æ¯

```python
def split_with_structure(markdown_file: str):
    """åˆ‡åˆ† Markdown å¹¶ä¿ç•™ç»“æ„ä¿¡æ¯"""
    from langchain.text_splitter import MarkdownHeaderTextSplitter
    
    # è¯»å–æ–‡ä»¶
    with open(markdown_file, 'r', encoding='utf-8') as f:
        markdown_text = f.read()
    
    # æŒ‰æ ‡é¢˜å±‚çº§åˆ‡åˆ†
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
    ]
    
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on
    )
    
    # ç¬¬ä¸€æ­¥ï¼šæŒ‰æ ‡é¢˜åˆ‡åˆ†
    header_splits = markdown_splitter.split_text(markdown_text)
    
    # ç¬¬äºŒæ­¥ï¼šæ¯ä¸ªç« èŠ‚å†…éƒ¨å†ç»†åˆ†
    final_chunks = []
    for split in header_splits:
        # å¦‚æœç« èŠ‚å†…å®¹å¤ªé•¿ï¼Œè¿›ä¸€æ­¥åˆ‡åˆ†
        if len(split.page_content) > 500:
            text_splitter = RecursiveTextSplitter(
                chunk_size=500,
                chunk_overlap=50
            )
            sub_chunks = text_splitter.split_documents([split])
            final_chunks.extend(sub_chunks)
        else:
            final_chunks.append(split)
    
    # æŸ¥çœ‹ç»“æœï¼ˆåŒ…å«ç»“æ„ä¿¡æ¯ï¼‰
    for chunk in final_chunks[:3]:
        print(f"æ ‡é¢˜: {chunk.metadata}")
        print(f"å†…å®¹: {chunk.page_content[:100]}...\n")
    
    return final_chunks

# ä½¿ç”¨
chunks = split_with_structure("documentation.md")
# æ¯ä¸ª chunk çš„ metadata åŒ…å«æ ‡é¢˜å±‚çº§ä¿¡æ¯
# ä¾‹å¦‚: {'Header 1': 'Introduction', 'Header 2': 'What is RAG?'}
```

---

## 8. å¸¸è§é—®é¢˜ä¸ä¼˜åŒ–

### 8.1 é—®é¢˜1ï¼šåˆ‡åˆ†åä¿¡æ¯ä¸¢å¤±

**ç°è±¡**ï¼š

```python
# åŸæ–‡
text = "æ ¹æ®ç ”ç©¶è¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½åœ¨ 2023 å¹´å–å¾—äº†é‡å¤§çªç ´ã€‚"

# åˆ‡åˆ†å
chunk1 = "æ ¹æ®ç ”ç©¶è¡¨æ˜ï¼Œäººå·¥æ™ºèƒ½"
chunk2 = "åœ¨ 2023 å¹´å–å¾—äº†é‡å¤§çªç ´ã€‚"

# é—®é¢˜ï¼šç”¨æˆ·æŸ¥è¯¢ "äººå·¥æ™ºèƒ½åœ¨ 2023 å¹´çš„çªç ´"
# chunk1 ç¼ºå°‘æ—¶é—´ï¼Œchunk2 ç¼ºå°‘ä¸»è¯­ï¼Œéƒ½ä¸å®Œæ•´
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ1ï¼šå¢å¤§ chunk_overlap
splitter = RecursiveTextSplitter(
    chunk_size=50,
    chunk_overlap=10  # å¢åŠ é‡å 
)

# æ–¹æ¡ˆ2ï¼šå¢å¤§ chunk_sizeï¼ˆä¿æŒæ›´å¤šä¸Šä¸‹æ–‡ï¼‰
splitter = RecursiveTextSplitter(
    chunk_size=100,  # è®©ä¸€ä¸ª chunk åŒ…å«å®Œæ•´ä¿¡æ¯
    chunk_overlap=10
)

# æ–¹æ¡ˆ3ï¼šåœ¨ metadata ä¸­ä¿ç•™å‰åæ–‡
def add_context_to_metadata(chunks):
    for i, chunk in enumerate(chunks):
        # æ·»åŠ ä¸Šä¸€å—çš„ç»“å°¾
        if i > 0:
            chunk.metadata['prev_text'] = chunks[i-1].page_content[-50:]
        # æ·»åŠ ä¸‹ä¸€å—çš„å¼€å¤´
        if i < len(chunks) - 1:
            chunk.metadata['next_text'] = chunks[i+1].page_content[:50]
    return chunks
```

### 8.2 é—®é¢˜2ï¼šåˆ‡åˆ†ä¸å‡åŒ€

**ç°è±¡**ï¼š

```python
# åˆ‡åˆ†ç»“æœ
chunks_sizes = [100, 450, 80, 500, 120]
# æœ‰çš„å¤ªå°ï¼Œæœ‰çš„å¤ªå¤§
```

**åŸå› **ï¼š

```python
# åŸå› 1ï¼šæ–‡æ¡£ç»“æ„ä¸è§„åˆ™
# ä¾‹å¦‚ï¼šæŸäº›æ®µè½ç‰¹åˆ«é•¿ï¼ŒæŸäº›ç‰¹åˆ«çŸ­

# åŸå› 2ï¼šåˆ†éš”ç¬¦é€‰æ‹©ä¸å½“
# ä¾‹å¦‚ï¼šåªç”¨ "\n\n"ï¼Œä½†æ–‡æ¡£æ²¡æœ‰ç©ºè¡Œåˆ†éš”
```

**è§£å†³æ–¹æ¡ˆ**ï¼š

```python
# æ–¹æ¡ˆ1ï¼šåå¤„ç†åˆå¹¶å°å—
def merge_small_chunks(chunks, min_size=100):
    merged = []
    buffer = []
    
    for chunk in chunks:
        if len(chunk.page_content) < min_size:
            # å¤ªå°ï¼Œæ”¾å…¥ç¼“å†²åŒº
            buffer.append(chunk.page_content)
        else:
            # è¶³å¤Ÿå¤§
            if buffer:
                # å…ˆè¾“å‡ºç¼“å†²åŒºçš„å†…å®¹ï¼ˆåˆå¹¶ï¼‰
                merged.append(Document(page_content=' '.join(buffer)))
                buffer = []
            merged.append(chunk)
    
    if buffer:
        merged.append(Document(page_content=' '.join(buffer)))
    
    return merged

# æ–¹æ¡ˆ2ï¼šä½¿ç”¨æ›´çµæ´»çš„åˆ†éš”ç¬¦
splitter = RecursiveTextSplitter(
    separators=[
        "\n\n",  # æ®µè½
        "\n",    # è¡Œ
        "ã€‚",    # å¥å­
        " ",     # å•è¯
        ""       # å­—ç¬¦ï¼ˆå…œåº•ï¼‰
    ]
)
```

### 8.3 é—®é¢˜3ï¼šç‰¹æ®Šæ ¼å¼å¤„ç†

#### é—®é¢˜3.1ï¼šä»£ç å—

```python
# é—®é¢˜ï¼šä»£ç å—å¯èƒ½è¢«åˆ‡æ–­
code_text = """
è¿™æ˜¯ä¸€æ®µè¯´æ˜ã€‚

```python
def hello_world():
    print("Hello, World!")
    print("This is a long function")
    # ... æ›´å¤šä»£ç 
```

æ›´å¤šè¯´æ˜ã€‚
"""

# åˆ‡åˆ†åå¯èƒ½ï¼š
# å—1: "è¿™æ˜¯ä¸€æ®µè¯´æ˜ã€‚\n\n```python\ndef hello_world():"
# å—2: "    print(\"Hello, World!\")\n..."
# é—®é¢˜ï¼šä»£ç å—è¢«åˆ‡æ–­ï¼Œå½±å“ç†è§£

# è§£å†³æ–¹æ¡ˆï¼šè¯†åˆ«ä»£ç å—å¹¶æ•´ä½“ä¿ç•™
def split_with_code_blocks(text, chunk_size=500):
    # æå–ä»£ç å—
    code_blocks = re.findall(r'```.*?```', text, flags=re.DOTALL)
    
    # æ›¿æ¢ä»£ç å—ä¸ºå ä½ç¬¦
    placeholder_text = text
    for i, code in enumerate(code_blocks):
        placeholder_text = placeholder_text.replace(code, f"[CODE_BLOCK_{i}]")
    
    # åˆ‡åˆ†å ä½ç¬¦æ–‡æœ¬
    splitter = RecursiveTextSplitter(chunk_size=chunk_size)
    chunks = splitter.split_text(placeholder_text)
    
    # æ¢å¤ä»£ç å—
    for i, chunk in enumerate(chunks):
        for j, code in enumerate(code_blocks):
            chunk = chunk.replace(f"[CODE_BLOCK_{j}]", code)
        chunks[i] = chunk
    
    return chunks
```

#### é—®é¢˜3.2ï¼šè¡¨æ ¼

```python
# é—®é¢˜ï¼šè¡¨æ ¼æ ¼å¼è¢«ç ´å
table_text = """
| æ¨¡å‹ | å‚æ•°é‡ | æ€§èƒ½ |
|-----|--------|------|
| GPT-3 | 175B | é«˜ |
| BERT | 340M | ä¸­ |
"""

# åˆ‡åˆ†å¯èƒ½ç ´åè¡¨æ ¼ç»“æ„

# è§£å†³æ–¹æ¡ˆ1ï¼šæ•´ä½“ä¿ç•™è¡¨æ ¼
def is_table(text):
    lines = text.split('\n')
    return any('|' in line for line in lines)

def split_preserve_tables(text, chunk_size=500):
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    chunks = []
    current_chunk = []
    current_size = 0
    
    for para in paragraphs:
        para_size = len(para)
        
        # å¦‚æœæ˜¯è¡¨æ ¼ï¼Œå°è¯•æ•´ä½“ä¿ç•™
        if is_table(para):
            if current_chunk:
                chunks.append('\n\n'.join(current_chunk))
                current_chunk = []
                current_size = 0
            
            if para_size <= chunk_size:
                chunks.append(para)
            else:
                # è¡¨æ ¼å¤ªå¤§ï¼Œåªèƒ½åˆ‡åˆ†
                chunks.append(para)  # æˆ–è€…æŒ‰è¡Œåˆ‡åˆ†
        else:
            # æ™®é€šæ®µè½
            if current_size + para_size > chunk_size:
                if current_chunk:
                    chunks.append('\n\n'.join(current_chunk))
                current_chunk = [para]
                current_size = para_size
            else:
                current_chunk.append(para)
                current_size += para_size
    
    if current_chunk:
        chunks.append('\n\n'.join(current_chunk))
    
    return chunks
```

### 8.4 é—®é¢˜4ï¼šæ€§èƒ½ä¼˜åŒ–

#### é—®é¢˜4.1ï¼šå¤§æ–‡ä»¶å¤„ç†æ…¢

```python
# é—®é¢˜ï¼š100MB çš„æ–‡æœ¬æ–‡ä»¶ï¼Œå¤„ç†å¾ˆæ…¢

# è§£å†³æ–¹æ¡ˆï¼šæµå¼å¤„ç†
def load_large_file_in_chunks(file_path, chunk_size=1000000):
    """æµå¼è¯»å–å¤§æ–‡ä»¶"""
    with open(file_path, 'r', encoding='utf-8') as f:
        while True:
            chunk = f.read(chunk_size)
            if not chunk:
                break
            yield Document(page_content=chunk)

# ä½¿ç”¨
for doc in load_large_file_in_chunks("huge_file.txt"):
    # é€å—å¤„ç†
    process(doc)
```

#### é—®é¢˜4.2ï¼šæ‰¹é‡å¤„ç†æ…¢

```python
# é—®é¢˜ï¼šå¤„ç† 1000 ä¸ªæ–‡ä»¶å¤ªæ…¢

# è§£å†³æ–¹æ¡ˆï¼šå¹¶è¡Œå¤„ç†
from concurrent.futures import ProcessPoolExecutor
from pathlib import Path

def process_single_file(file_path):
    """å¤„ç†å•ä¸ªæ–‡ä»¶"""
    try:
        docs = DocumentLoaderFactory.load(str(file_path))
        splitter = RecursiveTextSplitter()
        chunks = splitter.split_documents(docs)
        return chunks
    except Exception as e:
        return []

def process_files_parallel(file_paths, max_workers=4):
    """å¹¶è¡Œå¤„ç†å¤šä¸ªæ–‡ä»¶"""
    with ProcessPoolExecutor(max_workers=max_workers) as executor:
        results = executor.map(process_single_file, file_paths)
    
    all_chunks = []
    for chunks in results:
        all_chunks.extend(chunks)
    
    return all_chunks

# ä½¿ç”¨
file_paths = list(Path("./docs/").glob("*.pdf"))
chunks = process_files_parallel(file_paths)
```

### 8.5 é—®é¢˜5ï¼šç¼–ç é—®é¢˜

```python
# é—®é¢˜ï¼šWindows ä¸Šçš„æ–‡æœ¬æ–‡ä»¶é»˜è®¤æ˜¯ GBK ç¼–ç 

# è§£å†³æ–¹æ¡ˆ1ï¼šè‡ªåŠ¨æ£€æµ‹ç¼–ç 
import chardet

def detect_encoding(file_path):
    with open(file_path, 'rb') as f:
        raw_data = f.read()
    result = chardet.detect(raw_data)
    return result['encoding']

# ä½¿ç”¨
encoding = detect_encoding("file.txt")
loader = TextLoader(encoding=encoding)

# è§£å†³æ–¹æ¡ˆ2ï¼šå°è¯•å¤šç§ç¼–ç 
def load_with_fallback_encoding(file_path):
    encodings = ['utf-8', 'gbk', 'gb18030', 'latin-1']
    
    for encoding in encodings:
        try:
            loader = TextLoader(encoding=encoding, autodetect_encoding=False)
            return loader.load(file_path)
        except UnicodeDecodeError:
            continue
    
    raise Exception("æ— æ³•è¯†åˆ«æ–‡ä»¶ç¼–ç ")
```

---

## 9. ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ

### 9.1 ä¸ Embedding æ¨¡å—é›†æˆ

```python
from src.core.document import DocumentLoaderFactory, RecursiveTextSplitter
from src.core.embedding import OpenAIEmbedding

# å®Œæ•´æµç¨‹ï¼šæ–‡æ¡£ â†’ åˆ‡åˆ† â†’ å‘é‡åŒ–
def document_to_vectors(file_path: str):
    # 1. åŠ è½½æ–‡æ¡£
    documents = DocumentLoaderFactory.load(file_path)
    print(f"âœ… åŠ è½½æ–‡æ¡£: {len(documents)} ä¸ª")
    
    # 2. åˆ‡åˆ†æ–‡æ¡£
    splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(documents)
    print(f"âœ… åˆ‡åˆ†æ–‡æ¡£: {len(chunks)} ä¸ªå—")
    
    # 3. æå–æ–‡æœ¬
    texts = [chunk.page_content for chunk in chunks]
    
    # 4. å‘é‡åŒ–
    embedding = OpenAIEmbedding(model_name="text-embedding-ada-002")
    vectors = embedding.embed_documents(texts)
    print(f"âœ… å‘é‡åŒ–å®Œæˆ: {len(vectors)} ä¸ªå‘é‡")
    
    return chunks, vectors

# ä½¿ç”¨
chunks, vectors = document_to_vectors("document.pdf")
```

### 9.2 ä¸ VectorStore æ¨¡å—é›†æˆ

```python
from src.core.document import DocumentLoaderFactory, RecursiveTextSplitter
from src.core.vectorstore import FAISSVectorStore

# å®Œæ•´æµç¨‹ï¼šæ–‡æ¡£ â†’ åˆ‡åˆ† â†’ å­˜å‚¨
def build_knowledge_base(file_paths: List[str], persist_dir: str = "./vectorstore"):
    # 1. åˆ›å»ºå‘é‡åº“
    vectorstore = FAISSVectorStore(persist_directory=persist_dir)
    
    # 2. å¤„ç†æ¯ä¸ªæ–‡ä»¶
    for file_path in file_paths:
        print(f"\nå¤„ç†: {file_path}")
        
        # åŠ è½½
        documents = DocumentLoaderFactory.load(file_path)
        
        # åˆ‡åˆ†
        splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(documents)
        
        # æ·»åŠ åˆ°å‘é‡åº“
        vectorstore.add_documents(chunks)
        print(f"  âœ… æ·»åŠ äº† {len(chunks)} ä¸ªæ–‡æ¡£å—")
    
    # 3. æŒä¹…åŒ–
    vectorstore.persist()
    print(f"\nâœ… çŸ¥è¯†åº“æ„å»ºå®Œæˆï¼Œå·²ä¿å­˜åˆ°: {persist_dir}")
    
    return vectorstore

# ä½¿ç”¨
file_paths = ["doc1.pdf", "doc2.txt", "doc3.md"]
vectorstore = build_knowledge_base(file_paths)

# æµ‹è¯•æ£€ç´¢
results = vectorstore.similarity_search("ä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ï¼Ÿ", k=3)
for i, doc in enumerate(results):
    print(f"\nç»“æœ {i+1}:")
    print(f"å†…å®¹: {doc.page_content[:100]}...")
    print(f"æ¥æº: {doc.metadata['source']}")
```

### 9.3 ä¸ RAG Chain æ¨¡å—é›†æˆ

```python
from src.core.document import DocumentLoaderFactory, RecursiveTextSplitter
from src.core.llm import OpenAILLM
from src.core.vectorstore import FAISSVectorStore
from src.core.chain import RAGChain

# å®Œæ•´çš„ RAG ç³»ç»Ÿ
def create_rag_system(knowledge_files: List[str]):
    """ä»æ–‡æ¡£åˆ›å»º RAG ç³»ç»Ÿ"""
    
    print("=" * 60)
    print("åˆ›å»º RAG ç³»ç»Ÿ")
    print("=" * 60)
    
    # æ­¥éª¤1: å¤„ç†æ–‡æ¡£
    print("\næ­¥éª¤ 1: åŠ è½½å’Œåˆ‡åˆ†æ–‡æ¡£...")
    all_chunks = []
    for file_path in knowledge_files:
        # åŠ è½½
        documents = DocumentLoaderFactory.load(file_path)
        # åˆ‡åˆ†
        splitter = RecursiveTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = splitter.split_documents(documents)
        all_chunks.extend(chunks)
        print(f"  âœ… {file_path}: {len(chunks)} å—")
    
    print(f"\næ€»è®¡: {len(all_chunks)} ä¸ªæ–‡æ¡£å—")
    
    # æ­¥éª¤2: æ„å»ºå‘é‡åº“
    print("\næ­¥éª¤ 2: æ„å»ºå‘é‡åº“...")
    vectorstore = FAISSVectorStore()
    vectorstore.add_documents(all_chunks)
    print("  âœ… å‘é‡åº“æ„å»ºå®Œæˆ")
    
    # æ­¥éª¤3: åˆ›å»º RAG Chain
    print("\næ­¥éª¤ 3: åˆ›å»º RAG Chain...")
    llm = OpenAILLM(model_name="gpt-3.5-turbo")
    rag_chain = RAGChain(llm=llm, vectorstore=vectorstore)
    print("  âœ… RAG Chain åˆ›å»ºå®Œæˆ")
    
    print("\n" + "=" * 60)
    print("âœ… RAG ç³»ç»Ÿå°±ç»ªï¼")
    print("=" * 60)
    
    return rag_chain

# ä½¿ç”¨
knowledge_files = [
    "knowledge/python_intro.pdf",
    "knowledge/ai_basics.txt",
    "knowledge/rag_explained.md"
]

rag_chain = create_rag_system(knowledge_files)

# æµ‹è¯•é—®ç­”
question = "ä»€ä¹ˆæ˜¯ RAGï¼Ÿ"
answer = rag_chain.query(question)
print(f"\né—®é¢˜: {question}")
print(f"ç­”æ¡ˆ: {answer}")
```

### 9.4 åˆ›å»ºå®Œæ•´çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿

```python
class DocumentProcessingPipeline:
    """æ–‡æ¡£å¤„ç†æµæ°´çº¿"""
    
    def __init__(
        self,
        loader_type: str = "auto",
        splitter_type: str = "recursive",
        chunk_size: int = 500,
        chunk_overlap: int = 50
    ):
        self.loader_type = loader_type
        self.splitter_type = splitter_type
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'files_processed': 0,
            'documents_loaded': 0,
            'chunks_created': 0,
            'errors': []
        }
    
    def process_file(self, file_path: str) -> List[Document]:
        """å¤„ç†å•ä¸ªæ–‡ä»¶"""
        try:
            # 1. åŠ è½½
            if self.loader_type == "auto":
                documents = DocumentLoaderFactory.load(file_path)
            else:
                loader = self._get_loader(file_path)
                documents = loader.load(file_path)
            
            self.stats['documents_loaded'] += len(documents)
            
            # 2. åˆ‡åˆ†
            splitter = TextSplitterFactory.get_splitter(
                splitter_type=self.splitter_type,
                chunk_size=self.chunk_size,
                chunk_overlap=self.chunk_overlap
            )
            chunks = splitter.split_documents(documents)
            
            self.stats['chunks_created'] += len(chunks)
            self.stats['files_processed'] += 1
            
            return chunks
            
        except Exception as e:
            self.stats['errors'].append({
                'file': file_path,
                'error': str(e)
            })
            return []
    
    def process_directory(self, dir_path: str, recursive: bool = True) -> List[Document]:
        """å¤„ç†æ•´ä¸ªç›®å½•"""
        dir_path = Path(dir_path)
        pattern = "**/*" if recursive else "*"
        
        all_chunks = []
        for file_path in dir_path.glob(pattern):
            if file_path.is_file():
                chunks = self.process_file(str(file_path))
                all_chunks.extend(chunks)
        
        return all_chunks
    
    def get_stats(self) -> dict:
        """è·å–å¤„ç†ç»Ÿè®¡"""
        return self.stats
    
    def print_stats(self):
        """æ‰“å°ç»Ÿè®¡ä¿¡æ¯"""
        print("\n" + "=" * 60)
        print("æ–‡æ¡£å¤„ç†ç»Ÿè®¡")
        print("=" * 60)
        print(f"å¤„ç†æ–‡ä»¶æ•°: {self.stats['files_processed']}")
        print(f"åŠ è½½æ–‡æ¡£æ•°: {self.stats['documents_loaded']}")
        print(f"ç”Ÿæˆæ–‡æ¡£å—: {self.stats['chunks_created']}")
        
        if self.stats['errors']:
            print(f"\né”™è¯¯æ•°: {len(self.stats['errors'])}")
            for error in self.stats['errors']:
                print(f"  - {error['file']}: {error['error']}")
        else:
            print("\nâœ… æ²¡æœ‰é”™è¯¯")
        print("=" * 60)

# ä½¿ç”¨
pipeline = DocumentProcessingPipeline(
    splitter_type="chinese",
    chunk_size=400,
    chunk_overlap=40
)

# å¤„ç†æ•´ä¸ªç›®å½•
chunks = pipeline.process_directory("./knowledge_base/", recursive=True)

# æŸ¥çœ‹ç»Ÿè®¡
pipeline.print_stats()

# è¾“å‡ºç¤ºä¾‹:
# ============================================================
# æ–‡æ¡£å¤„ç†ç»Ÿè®¡
# ============================================================
# å¤„ç†æ–‡ä»¶æ•°: 15
# åŠ è½½æ–‡æ¡£æ•°: 150
# ç”Ÿæˆæ–‡æ¡£å—: 450
# 
# âœ… æ²¡æœ‰é”™è¯¯
# ============================================================
```

---

## 10. å­¦ä¹ è·¯å¾„å»ºè®®

### 10.1 åˆå­¦è€…è·¯å¾„ï¼ˆ1-2 å¤©ï¼‰

**Day 1: åŸºç¡€æ¦‚å¿µ**
```
ä¸Šåˆ:
1. ç†è§£ Document å¯¹è±¡
2. å­¦ä¹ æ–‡æ¡£åŠ è½½å™¨ï¼ˆTextLoaderï¼‰
3. å®è·µï¼šåŠ è½½ 3 ä¸ªä¸åŒæ ¼å¼çš„æ–‡ä»¶

ä¸‹åˆ:
4. ç†è§£ chunk_size å’Œ chunk_overlap
5. å­¦ä¹ é€’å½’åˆ‡åˆ†å™¨
6. å®è·µï¼šç”¨ä¸åŒå‚æ•°åˆ‡åˆ†æ–‡æ¡£ï¼Œè§‚å¯Ÿæ•ˆæœ
```

**Day 2: å®æˆ˜åº”ç”¨**
```
ä¸Šåˆ:
7. å­¦ä¹ å·¥å‚æ¨¡å¼çš„ä½¿ç”¨
8. å®è·µï¼šæ‰¹é‡å¤„ç†æ–‡ä»¶ç›®å½•

ä¸‹åˆ:
9. é›†æˆåˆ° RAG ç³»ç»Ÿ
10. å®è·µï¼šå®Œæˆ examples/test_document_processing.py
```

### 10.2 è¿›é˜¶è·¯å¾„ï¼ˆ3-5 å¤©ï¼‰

**Day 1-2: æ·±å…¥ç†è§£**
- ç ”ç©¶é€’å½’åˆ‡åˆ†çš„ç®—æ³•
- å¯¹æ¯”ä¸åŒåˆ‡åˆ†ç­–ç•¥çš„æ•ˆæœ
- å­¦ä¹  Token è®¡æ•°å’Œä¼˜åŒ–

**Day 3: ç‰¹æ®Šåœºæ™¯**
- PDF æå–ä¼˜åŒ–
- Markdown ç»“æ„ä¿ç•™
- ä¸­æ–‡åˆ†è¯åˆ‡åˆ†

**Day 4: æ€§èƒ½ä¼˜åŒ–**
- å¤§æ–‡ä»¶æµå¼å¤„ç†
- å¹¶è¡Œæ‰¹é‡å¤„ç†
- å†…å­˜ä¼˜åŒ–

**Day 5: ç”Ÿäº§çº§å®è·µ**
- æ„å»ºå®Œæ•´çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿
- é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•
- ç›‘æ§å’Œç»Ÿè®¡

### 10.3 æ ¸å¿ƒçŸ¥è¯†æ¸…å•

#### âœ… å¿…é¡»æŒæ¡
- [ ] Document å¯¹è±¡çš„ç»“æ„å’Œç”¨æ³•
- [ ] TextLoader çš„ä½¿ç”¨
- [ ] RecursiveTextSplitter çš„ä½¿ç”¨
- [ ] chunk_size å’Œ chunk_overlap çš„å«ä¹‰
- [ ] DocumentLoaderFactory çš„ä½¿ç”¨
- [ ] åŸºæœ¬çš„æ–‡æ¡£å¤„ç†æµç¨‹

#### âœ… åº”è¯¥æŒæ¡
- [ ] PDFLoader å’Œ MarkdownLoader
- [ ] ChineseTextSplitter
- [ ] ä¸åŒåˆ‡åˆ†ç­–ç•¥çš„å¯¹æ¯”
- [ ] metadata çš„ä½¿ç”¨
- [ ] ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ

#### âœ… å¯ä»¥æŒæ¡ï¼ˆè¿›é˜¶ï¼‰
- [ ] Token è®¡æ•°å’Œä¼˜åŒ–
- [ ] è‡ªå®šä¹‰åˆ‡åˆ†ç­–ç•¥
- [ ] å¹¶è¡Œå¤„ç†ä¼˜åŒ–
- [ ] æµå¼å¤„ç†å¤§æ–‡ä»¶
- [ ] å¤æ‚æ–‡æ¡£ç»“æ„ä¿ç•™

### 10.4 å®è·µç»ƒä¹ 

#### ç»ƒä¹ 1ï¼šåŸºç¡€åŠ è½½å’Œåˆ‡åˆ†
```python
# ä»»åŠ¡ï¼š
# 1. åˆ›å»ºä¸€ä¸ªåŒ…å« 500 å­—çš„æ–‡æœ¬æ–‡ä»¶
# 2. åŠ è½½å¹¶åˆ‡åˆ†ï¼ˆchunk_size=200, overlap=20ï¼‰
# 3. æ‰“å°æ¯ä¸ªå—çš„å¤§å°

# ä½ çš„ä»£ç ï¼š
```

#### ç»ƒä¹ 2ï¼šå¯¹æ¯”ä¸åŒé…ç½®
```python
# ä»»åŠ¡ï¼š
# 1. ä½¿ç”¨åŒä¸€æ–‡æ¡£
# 2. å°è¯• 3 ç»„ä¸åŒçš„ (chunk_size, overlap) é…ç½®
# 3. å¯¹æ¯”åˆ‡åˆ†ç»“æœï¼ˆå—æ•°ã€å¹³å‡å¤§å°ï¼‰

# ä½ çš„ä»£ç ï¼š
```

#### ç»ƒä¹ 3ï¼šå¤„ç†çœŸå®æ–‡æ¡£
```python
# ä»»åŠ¡ï¼š
# 1. ä¸‹è½½ä¸€ä»½ PDF æŠ€æœ¯æ–‡æ¡£
# 2. åŠ è½½å¹¶åˆ‡åˆ†
# 3. å°†åˆ‡åˆ†ç»“æœä¿å­˜åˆ° JSON æ–‡ä»¶

# ä½ çš„ä»£ç ï¼š
```

#### ç»ƒä¹ 4ï¼šæ„å»ºçŸ¥è¯†åº“
```python
# ä»»åŠ¡ï¼š
# 1. æ”¶é›† 5 ä¸ªä¸åŒæ ¼å¼çš„æ–‡æ¡£
# 2. æ‰¹é‡å¤„ç†å¹¶åˆ‡åˆ†
# 3. æ·»åŠ åˆ°å‘é‡åº“
# 4. æµ‹è¯•æ£€ç´¢åŠŸèƒ½

# ä½ çš„ä»£ç ï¼š
```

---

## æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹å›é¡¾

1. **æ–‡æ¡£å¤„ç†æ˜¯ RAG çš„æ•°æ®å…¥å£**
   - è´Ÿè´£å°†åŸå§‹æ–‡æ¡£è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼
   - æ˜¯æ•´ä¸ªç³»ç»Ÿçš„ç¬¬ä¸€æ­¥

2. **Document å¯¹è±¡æ˜¯æ ¸å¿ƒæ•°æ®ç»“æ„**
   - page_content: æ–‡æœ¬å†…å®¹
   - metadata: å…ƒæ•°æ®ï¼ˆæ¥æºã€é¡µç ç­‰ï¼‰

3. **æ–‡æ¡£åŠ è½½å™¨æ”¯æŒå¤šç§æ ¼å¼**
   - TextLoader: TXT
   - PDFLoader: PDF
   - MarkdownLoader: Markdown
   - ä½¿ç”¨å·¥å‚æ¨¡å¼è‡ªåŠ¨é€‰æ‹©

4. **æ–‡æœ¬åˆ‡åˆ†çš„å…³é”®å‚æ•°**
   - chunk_size: æ§åˆ¶å—å¤§å°ï¼ˆæ¨è 300-600ï¼‰
   - chunk_overlap: ä¿æŒè¯­ä¹‰è¿è´¯ï¼ˆæ¨è 10%-20%ï¼‰

5. **é€’å½’åˆ‡åˆ†æ˜¯æœ€é€šç”¨çš„ç­–ç•¥**
   - ä¼˜å…ˆæŒ‰æ®µè½ã€å¥å­åˆ‡åˆ†
   - è‡ªåŠ¨é™çº§åˆ°å­—ç¬¦çº§
   - é€‚ç”¨äºå¤§å¤šæ•°æ–‡æœ¬ç±»å‹

6. **ä¸­æ–‡æ–‡æœ¬éœ€è¦ç‰¹æ®Šå¤„ç†**
   - ä½¿ç”¨ä¸­æ–‡åˆ‡åˆ†å™¨
   - è€ƒè™‘ PDF æå–çš„æ ¼å¼é—®é¢˜
   - æŒ‰ä¸­æ–‡æ ‡ç‚¹ç¬¦å·åˆ‡åˆ†

7. **è®¾è®¡æ¨¡å¼çš„åº”ç”¨**
   - ç­–ç•¥æ¨¡å¼: ä¸åŒçš„åŠ è½½å™¨å’Œåˆ‡åˆ†å™¨
   - å·¥å‚æ¨¡å¼: è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„å¤„ç†å™¨
   - æ¨¡æ¿æ–¹æ³•: ç»Ÿä¸€çš„å¤„ç†æµç¨‹

8. **ä¸å…¶ä»–æ¨¡å—çš„é›†æˆ**
   - ä¸‹æ¸¸æ˜¯ Embedding æ¨¡å—
   - æœ€ç»ˆè¾“å‡ºç»™ VectorStore æ¨¡å—
   - æ”¯æ’‘æ•´ä¸ª RAG ç³»ç»Ÿ

### ä¸‹ä¸€æ­¥å­¦ä¹ å»ºè®®

1. **å®è·µä¼˜å…ˆ**ï¼šè¿è¡Œ `examples/test_document_processing.py`
2. **å‚æ•°è°ƒä¼˜**ï¼šå°è¯•ä¸åŒçš„ chunk_size å’Œ overlap
3. **çœŸå®åœºæ™¯**ï¼šç”¨è‡ªå·±çš„æ–‡æ¡£æµ‹è¯•
4. **æ€§èƒ½ä¼˜åŒ–**ï¼šå¤„ç†å¤§è§„æ¨¡æ–‡æ¡£é›†åˆ
5. **æ¨¡å—é›†æˆ**ï¼šè¿æ¥åˆ°å®Œæ•´çš„ RAG ç³»ç»Ÿ

---

**ğŸ‰ æ­å–œå®Œæˆæ–‡æ¡£å¤„ç†æ¨¡å—çš„å­¦ä¹ ï¼**

ä½ ç°åœ¨æŒæ¡äº†ï¼š
- âœ… æ–‡æ¡£åŠ è½½çš„åŸç†å’Œå®è·µ
- âœ… æ–‡æœ¬åˆ‡åˆ†çš„ç­–ç•¥å’ŒæŠ€å·§
- âœ… å‚æ•°é…ç½®çš„æ–¹æ³•å’Œå»ºè®®
- âœ… ä¸å…¶ä»–æ¨¡å—çš„é›†æˆæ–¹å¼

ä¸‹ä¸€æ­¥ï¼Œä½ å¯ä»¥ï¼š
1. ç»§ç»­å®Œå–„ RAG ç³»ç»Ÿçš„å…¶ä»–æ¨¡å—
2. ä¼˜åŒ–æ–‡æ¡£å¤„ç†æµç¨‹
3. å¤„ç†æ›´å¤æ‚çš„æ–‡æ¡£ç±»å‹
4. æ„å»ºç”Ÿäº§çº§çš„æ–‡æ¡£å¤„ç†æµæ°´çº¿

**ç»§ç»­åŠ æ²¹ï¼** ğŸš€


