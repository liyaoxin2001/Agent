# 知识库管理实现指南

## 📋 已完成：2.1 和 2.2

### ✅ 2.1 KnowledgeBase 类（已完成）

实现了单个知识库的核心功能：
- `add_documents()`: 添加文档到知识库
- `search()`: 在知识库中搜索相关文档
- `delete()`: 删除整个知识库
- `get_document_count()`: 获取文档数量（辅助方法）

### ✅ 2.2 KnowledgeBaseManager 类（已完成）

实现了多知识库管理功能：
- `create_kb()`: 创建新知识库
- `get_kb()`: 获取指定知识库
- `list_kb()`: 列出所有知识库
- `delete_kb()`: 删除指定知识库
- `get_kb_info()`: 获取知识库信息（辅助方法）

---

## 🎯 下一步：2.3 文档管理功能

### 需要实现的功能

根据 TODO.md 的要求：

```
### 2.3 文档管理功能
- [ ] 实现文档上传功能
- [ ] 实现文档列表查询
- [ ] 实现文档删除功能
- [ ] 实现向量库重建功能
```

### 功能说明

#### 1. 文档上传功能

**目标**：允许用户上传文件到指定知识库

**需要实现**：
- 接收文件路径或文件对象
- 使用文档加载器（`DocumentLoader`）加载文件
- 使用文本切分器（`TextSplitter`）切分文档
- 将切分后的文档块添加到知识库

**实现位置**：可以在 `KnowledgeBase` 类中添加方法

```python
def upload_file(
    self, 
    file_path: str, 
    chunk_size: int = 500,
    chunk_overlap: int = 50
) -> int:
    """
    上传文件到知识库
    
    工作流程：
    1. 使用 DocumentLoaderFactory 加载文件
    2. 使用 TextSplitterFactory 切分文档
    3. 调用 add_documents() 添加到向量库
    
    Args:
        file_path: 文件路径（支持 .txt, .pdf, .md）
        chunk_size: 文本块大小
        chunk_overlap: 文本块重叠大小
        
    Returns:
        int: 添加的文档块数量
    """
    # TODO: 实现
    pass
```

#### 2. 文档列表查询

**目标**：查询知识库中的所有文档信息

**需要实现**：
- 获取知识库中所有文档的元数据
- 返回文档列表（包括来源、大小、添加时间等）

**挑战**：
- FAISS 向量库没有直接提供"列出所有文档"的功能
- 需要维护一个文档索引（可以使用 JSON 文件）

**实现建议**：
- 在 `KnowledgeBase` 类中维护一个文档列表
- 每次添加文档时，同时更新文档列表
- 提供 `list_documents()` 方法查询

```python
class KnowledgeBase:
    def __init__(...):
        # ...
        self.doc_index_file = self.kb_path / "documents.json"
        self.documents_index = self._load_doc_index()
    
    def _load_doc_index(self) -> List[dict]:
        """加载文档索引"""
        if self.doc_index_file.exists():
            with open(self.doc_index_file, 'r') as f:
                return json.load(f)
        return []
    
    def _save_doc_index(self):
        """保存文档索引"""
        with open(self.doc_index_file, 'w') as f:
            json.dump(self.documents_index, f, indent=2)
    
    def list_documents(self) -> List[dict]:
        """
        列出知识库中的所有文档
        
        Returns:
            List[dict]: 文档信息列表，每个文档包含：
                - source: 文件路径
                - chunk_count: 文档块数量
                - added_at: 添加时间
                - size: 文件大小
        """
        return self.documents_index
```

#### 3. 文档删除功能

**目标**：删除知识库中的指定文档

**挑战**：
- FAISS 支持按 ID 删除，但需要维护文档 ID 映射
- 删除后需要更新文档索引

**实现建议**：
```python
def delete_document(self, source: str):
    """
    删除指定来源的文档
    
    Args:
        source: 文档来源（文件路径）
        
    注意：
        - FAISS 的删除功能有限
        - 建议：删除整个向量库，重新添加其他文档
    """
    # 方案1: 如果 FAISS 支持按 ID 删除
    # - 找到该文档的所有 chunk ID
    # - 调用 vectorstore.delete(ids)
    
    # 方案2: 重建向量库（更可靠）
    # - 从文档索引中移除该文档
    # - 清空向量库
    # - 重新添加其他文档
    pass
```

#### 4. 向量库重建功能

**目标**：重新构建整个向量库

**使用场景**：
- 更换 Embedding 模型后重建
- 向量库损坏时恢复
- 优化向量库性能

**实现建议**：
```python
def rebuild_vectorstore(
    self,
    new_embedding: Optional[BaseEmbedding] = None
):
    """
    重建向量库
    
    工作流程：
    1. 从文档索引获取所有原始文档路径
    2. 清空当前向量库
    3. 重新加载、切分、添加所有文档
    
    Args:
        new_embedding: 新的 Embedding 模型（可选）
                      如果提供，将使用新模型重新向量化
    """
    # 1. 保存原始文档路径
    doc_sources = [doc['source'] for doc in self.documents_index]
    
    # 2. 清空向量库
    self.vectorstore.delete()
    
    # 3. 如果提供了新 embedding，替换
    if new_embedding:
        self.embedding = new_embedding
        self.vectorstore = FAISSVectorStore(
            persist_directory=str(self.kb_path),
            embedding=new_embedding
        )
    
    # 4. 重新加载所有文档
    for source in doc_sources:
        self.upload_file(source)
    
    print(f"重建完成，共 {len(doc_sources)} 个文档")
```

---

## 🔧 实现步骤建议

### 步骤 1: 添加文档索引维护

在 `KnowledgeBase.__init__()` 中添加文档索引：

```python
import json
from datetime import datetime

class KnowledgeBase:
    def __init__(self, ...):
        # ... 原有代码 ...
        
        # 文档索引文件
        self.doc_index_file = self.kb_path / "documents.json"
        self.documents_index = self._load_doc_index()
    
    def _load_doc_index(self) -> List[dict]:
        """加载文档索引"""
        if self.doc_index_file.exists():
            with open(self.doc_index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return []
    
    def _save_doc_index(self):
        """保存文档索引"""
        with open(self.doc_index_file, 'w', encoding='utf-8') as f:
            json.dump(self.documents_index, f, indent=2, ensure_ascii=False)
```

### 步骤 2: 修改 `add_documents()` 方法

在添加文档时同时更新索引：

```python
def add_documents(self, documents: List[Document]) -> int:
    try:
        if not documents:
            return 0
        
        # 添加到向量库
        self.vectorstore.add_documents(documents)
        self.vectorstore.persist()
        
        # 更新文档索引（按来源分组）
        source_chunks = {}
        for doc in documents:
            source = doc.metadata.get('source', 'unknown')
            if source not in source_chunks:
                source_chunks[source] = 0
            source_chunks[source] += 1
        
        # 添加到索引
        for source, chunk_count in source_chunks.items():
            # 检查是否已存在
            existing = next((d for d in self.documents_index if d['source'] == source), None)
            if existing:
                existing['chunk_count'] += chunk_count
                existing['updated_at'] = datetime.now().isoformat()
            else:
                self.documents_index.append({
                    'source': source,
                    'chunk_count': chunk_count,
                    'added_at': datetime.now().isoformat(),
                })
        
        # 保存索引
        self._save_doc_index()
        
        return len(documents)
    except Exception as e:
        raise Exception(f"添加文档到知识库 '{self.name}' 失败: {str(e)}") from e
```

### 步骤 3: 实现 `upload_file()` 方法

```python
def upload_file(
    self, 
    file_path: str, 
    chunk_size: int = 500,
    chunk_overlap: int = 50
) -> int:
    """上传文件到知识库"""
    from src.core.document import DocumentLoaderFactory, TextSplitterFactory
    
    try:
        # 1. 加载文件
        documents = DocumentLoaderFactory.load(file_path)
        
        # 2. 切分文档
        chunks = TextSplitterFactory.split(
            documents,
            splitter_type="chinese",  # 或根据语言自动选择
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )
        
        # 3. 添加到向量库
        count = self.add_documents(chunks)
        
        return count
    except Exception as e:
        raise Exception(f"上传文件 '{file_path}' 失败: {str(e)}") from e
```

### 步骤 4: 实现其他方法

- `list_documents()`: 返回文档索引
- `delete_document()`: 从索引中删除并重建
- `rebuild_vectorstore()`: 重建整个向量库

---

## 📝 实现建议

### 推荐实现顺序

1. ✅ **先实现文档索引维护**（最基础）
2. ✅ **实现 `upload_file()`**（最常用）
3. ✅ **实现 `list_documents()`**（查询功能）
4. ⏸️ **实现 `delete_document()`**（可选，FAISS 支持有限）
5. ⏸️ **实现 `rebuild_vectorstore()`**（可选，高级功能）

### 重点注意事项

1. **文档索引的一致性**
   - 添加文档时更新索引
   - 删除文档时更新索引
   - 索引损坏时能够恢复

2. **错误处理**
   - 文件不存在
   - 文件格式不支持
   - 磁盘空间不足

3. **性能优化**
   - 批量上传文件
   - 异步处理大文件
   - 进度显示

---

## 🎯 下一步行动

### 选项 1: 我帮你直接实现（推荐）✅

我可以直接帮你实现完整的 2.3 功能，包括：
- 文档索引维护
- 文件上传功能
- 文档列表查询
- 详细注释和测试代码

### 选项 2: 你自己尝试实现

根据上面的指南自己实现，然后我帮你检查和完善。

---

**建议**：直接让我帮你实现，这样你可以：
- 更快地完成功能
- 学习最佳实践
- 获得详细的注释和文档

请告诉我你的选择！💪

