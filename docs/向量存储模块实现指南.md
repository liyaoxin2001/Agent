# 向量存储模块实现指南

## 一、向量存储模块概述

### 1.1 什么是向量存储？

**向量存储（Vector Store）**是专门用于存储和管理高维向量的数据库系统。它能够高效地存储向量数据，并支持基于相似度的快速检索。

**核心功能**：
- **存储向量**：将文档的向量表示存储到数据库
- **相似度搜索**：根据查询向量找到最相似的文档向量
- **持久化**：将向量数据保存到磁盘
- **增删改查**：对向量库进行管理

### 1.2 在 RAG 中的作用

**RAG 流程中的关键环节**：

```
文档 → Embedding 向量化 → 向量存储（存储）→ 相似度检索 → 返回相关文档
                               ↑
用户问题 → Embedding 向量化 → 向量相似度搜索 ──────┘
```

**具体作用**：
1. **文档存储**：将文档的向量表示持久化保存
2. **快速检索**：根据用户查询快速找到相关文档
3. **相似度排序**：返回按相似度排序的文档列表
4. **数据管理**：支持添加、删除、更新向量数据

### 1.3 FAISS 简介

**FAISS（Facebook AI Similarity Search）**是 Facebook 开源的高性能向量相似度搜索库。

**主要特性**：
- **高效检索**：支持十亿级向量的高速搜索
- **多种索引**：支持不同的索引算法（L2距离、余弦相似度、内积等）
- **GPU 加速**：支持 GPU 加速，提升性能
- **易于使用**：Python API 简单易用

**为什么选择 FAISS？**
- 性能优秀：搜索速度快，内存效率高
- 功能丰富：支持多种相似度度量
- 成熟稳定：Facebook 出品，开源活跃
- LangChain 集成：与 LangChain 无缝集成

---

## 二、核心概念

### 2.1 向量索引

**向量索引**是将高维向量组织成可以快速搜索的数据结构。

**常见索引类型**：

1. **Flat Index（暴力搜索）**
   - 计算查询向量与所有文档向量的相似度
   - 准确但慢，适合小数据集

2. **IVF（Inverted File Index）**
   - 将向量空间分成多个区域（Voronoi 图）
   - 先搜索最近的区域，再在区域内搜索
   - 平衡速度和准确性

3. **PQ（Product Quantization）**
   - 将高维向量压缩为低维表示
   - 减少内存使用，提升搜索速度
   - 轻微损失准确性

4. **HNSW（Hierarchical Navigable Small World）**
   - 图结构索引，搜索速度快
   - 适合实时检索

**FAISS 使用的索引**：
- 默认使用 L2 距离（欧几里得距离）
- 支持余弦相似度（需要归一化向量）
- 支持多种索引算法

### 2.2 相似度搜索

**相似度搜索**是根据查询向量找到最相似的文档向量。

**工作流程**：

```
查询向量 → 向量索引 → 候选向量 → 精确计算相似度 → 排序返回
```

**相似度度量**：

1. **欧几里得距离（L2 Distance）**
   ```
   distance = sqrt(sum((a_i - b_i)^2))
   ```
   - 数值越小，越相似
   - FAISS 默认使用

2. **余弦相似度（Cosine Similarity）**
   ```
   similarity = (A·B) / (||A|| * ||B||)
   ```
   - 数值越大，越相似（-1 到 1）
   - 需要归一化向量

3. **内积（Inner Product）**
   ```
   score = sum(a_i * b_i)
   ```
   - 数值越大，越相似

### 2.3 持久化

**持久化**是将向量数据保存到磁盘，避免每次重启都要重新构建。

**FAISS 持久化**：
- `save_local()`：保存到本地文件
- `load_local()`：从本地文件加载
- 支持保存索引结构和向量数据

---

## 三、架构设计

### 3.1 接口设计

```python
class BaseVectorStore(ABC):
    def add_documents(
        self, 
        documents: List[Document], 
        embeddings: Optional[List[List[float]]] = None
    ):
        """添加文档到向量库"""
        pass
    
    def similarity_search(
        self, 
        query: str, 
        k: int = 4,
        filter: Optional[dict] = None
    ) -> List[Document]:
        """相似度搜索"""
        pass
    
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 4,
        filter: Optional[dict] = None
    ) -> List[tuple[Document, float]]:
        """带相似度分数的搜索"""
        pass
    
    def delete(self, ids: Optional[List[str]] = None):
        """删除文档"""
        pass
    
    def persist(self):
        """持久化向量库到磁盘"""
        pass
```

**方法详解**：

1. **add_documents()**
   - 添加文档和向量到向量库
   - 支持预计算向量（可选）

2. **similarity_search()**
   - 文本相似度搜索
   - 返回相关文档列表

3. **similarity_search_with_score()**
   - 带相似度分数的搜索
   - 返回 (文档, 分数) 元组

4. **delete()**
   - 删除指定文档或清空向量库

5. **persist()**
   - 保存向量库到磁盘

### 3.2 为什么需要抽象接口？

**设计模式**：策略模式

**优势**：
- **可扩展性**：可以轻松添加其他向量存储（如 Chroma、Milvus）
- **可测试性**：可以创建 Mock VectorStore
- **代码解耦**：上层代码不依赖具体实现

```python
# 可以轻松切换
class RAGChain:
    def __init__(self, vectorstore: BaseVectorStore):
        self.vectorstore = vectorstore  # 依赖抽象接口

# 使用不同的实现
faiss_store = FAISSVectorStore(...)
chroma_store = ChromaVectorStore(...)  # 如果实现的话

# 代码无需修改
rag = RAGChain(faiss_store)
rag = RAGChain(chroma_store)  # 直接替换
```

---

## 四、实现步骤详解

### 4.1 查找 LangChain 文档

**文档位置**：
- 主文档：https://python.langchain.com/docs/integrations/vectorstores/faiss
- 搜索关键词：`FAISS`, `from_documents`, `similarity_search`

**关键信息**：
1. 如何初始化 FAISS
2. 如何创建向量库（`from_documents`）
3. 如何进行相似度搜索（`similarity_search`）
4. 如何持久化（`save_local`, `load_local`）

### 4.2 实现 FAISSVectorStore 类

#### 步骤1：导入必要的模块

```python
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
from langchain.schema import Document
import os
```

#### 步骤2：初始化方法

```python
class FAISSVectorStore(BaseVectorStore):
    def __init__(self, persist_directory: Optional[str] = None):
        super().__init__(persist_directory)
        
        # 初始化 Embedding（用于向量化）
        api_key = os.getenv("OPENAI_API_KEY")
        self.embedding = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=api_key
        )
        
        # 初始化向量库
        self.vectorstore: Optional[FAISS] = None
        
        # 如果指定了目录，尝试加载现有向量库
        if persist_directory and os.path.exists(persist_directory):
            try:
                self.vectorstore = FAISS.load_local(
                    persist_directory, 
                    self.embedding,
                    allow_dangerous_deserialization=True
                )
            except:
                # 加载失败，创建新的向量库
                self.vectorstore = None
```

**关键点**：
- `persist_directory`：持久化目录
- `self.embedding`：用于向量化文本
- `self.vectorstore`：FAISS 实例，可能为 None（空库）

#### 步骤3：实现 add_documents()

```python
def add_documents(
    self, 
    documents: List[Document], 
    embeddings: Optional[List[List[float]]] = None
):
    """
    添加文档到向量库
    
    Args:
        documents: 文档列表
        embeddings: 预计算的向量（可选）
    """
    try:
        if self.vectorstore is None:
            # 创建新的向量库
            if embeddings is not None:
                # 使用预计算的向量
                self.vectorstore = FAISS.from_embeddings(
                    text_embeddings=list(zip(
                        [doc.page_content for doc in documents],
                        embeddings
                    )),
                    embedding=self.embedding
                )
            else:
                # 自动计算向量
                self.vectorstore = FAISS.from_documents(
                    documents=documents,
                    embedding=self.embedding
                )
        else:
            # 添加到现有向量库
            if embeddings is not None:
                # 使用预计算的向量
                self.vectorstore.add_embeddings(
                    list(zip(
                        [doc.page_content for doc in documents],
                        embeddings
                    ))
                )
            else:
                # 自动计算向量
                self.vectorstore.add_documents(documents)
                
    except Exception as e:
        raise Exception(f"添加文档失败: {str(e)}") from e
```

**关键点**：

1. **两种创建方式**：
   - `FAISS.from_documents()`：自动计算向量
   - `FAISS.from_embeddings()`：使用预计算向量

2. **两种添加方式**：
   - `add_documents()`：自动计算向量
   - `add_embeddings()`：使用预计算向量

3. **预计算向量**：
   - 提高效率，避免重复计算
   - 在批量处理时特别有用

#### 步骤4：实现 similarity_search()

```python
def similarity_search(
    self, 
    query: str, 
    k: int = 4,
    filter: Optional[dict] = None
) -> List[Document]:
    """
    相似度搜索
    
    Args:
        query: 查询文本
        k: 返回的文档数量
        filter: 过滤条件（FAISS 不支持）
        
    Returns:
        相关文档列表
    """
    if self.vectorstore is None:
        return []
    
    try:
        # FAISS 的相似度搜索
        docs = self.vectorstore.similarity_search(query, k=k)
        return docs
    except Exception as e:
        raise Exception(f"相似度搜索失败: {str(e)}") from e
```

**关键点**：
- `similarity_search()`：FAISS 提供的方法
- 自动将查询文本向量化
- 返回最相关的 k 个文档

#### 步骤5：实现 similarity_search_with_score()

```python
def similarity_search_with_score(
    self,
    query: str,
    k: int = 4,
    filter: Optional[dict] = None
) -> List[tuple[Document, float]]:
    """
    带相似度分数的搜索
    
    Returns:
        (文档, 相似度分数) 的元组列表
    """
    if self.vectorstore is None:
        return []
    
    try:
        # FAISS 的带分数搜索
        docs_with_scores = self.vectorstore.similarity_search_with_score(
            query, k=k
        )
        return docs_with_scores
    except Exception as e:
        raise Exception(f"带分数搜索失败: {str(e)}") from e
```

**关键点**：
- `similarity_search_with_score()`：返回 (文档, 分数)
- 分数是相似度分数，数值越小越相似（L2 距离）

#### 步骤6：实现 delete()

```python
def delete(self, ids: Optional[List[str]] = None):
    """
    删除文档
    
    Args:
        ids: 要删除的文档 ID 列表，如果为 None 则清空
    """
    try:
        if ids is None:
            # 清空向量库
            self.vectorstore = None
        else:
            # FAISS 不支持按 ID 删除，重新创建向量库
            # （生产环境中需要更复杂的实现）
            raise NotImplementedError("FAISS 不支持按 ID 删除文档")
    except Exception as e:
        raise Exception(f"删除文档失败: {str(e)}") from e
```

**关键点**：
- FAISS 本身不支持按 ID 删除文档
- 简单实现：清空整个向量库
- 生产环境需要重新构建向量库

#### 步骤7：实现 persist()

```python
def persist(self):
    """持久化向量库到磁盘"""
    if self.vectorstore is None or not self.persist_directory:
        return
    
    try:
        # 保存到本地
        self.vectorstore.save_local(self.persist_directory)
    except Exception as e:
        raise Exception(f"持久化失败: {str(e)}") from e
```

**关键点**：
- `save_local()`：FAISS 提供的方法
- 保存索引结构和向量数据
- 可以用 `load_local()` 加载

### 4.3 完整实现示例

```python
"""
向量存储基础接口定义
"""
from abc import ABC, abstractmethod
from typing import List, Optional
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings
import os
import dotenv

dotenv.load_dotenv()

class BaseVectorStore(ABC):
    """向量存储基础接口"""
    
    def __init__(self, persist_directory: Optional[str] = None):
        self.persist_directory = persist_directory
    
    @abstractmethod
    def add_documents(
        self, 
        documents: List[Document], 
        embeddings: Optional[List[List[float]]] = None
    ):
        """添加文档到向量库"""
        pass
    
    @abstractmethod
    def similarity_search(
        self, 
        query: str, 
        k: int = 4,
        filter: Optional[dict] = None
    ) -> List[Document]:
        """相似度搜索"""
        pass
    
    @abstractmethod
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 4,
        filter: Optional[dict] = None
    ) -> List[tuple[Document, float]]:
        """带相似度分数的搜索"""
        pass
    
    @abstractmethod
    def delete(self, ids: Optional[List[str]] = None):
        """删除文档"""
        pass
    
    @abstractmethod
    def persist(self):
        """持久化向量库到磁盘"""
        pass


class FAISSVectorStore(BaseVectorStore):
    """FAISS 向量存储实现"""
    
    def __init__(self, persist_directory: Optional[str] = None):
        super().__init__(persist_directory)
        
        # 初始化 Embedding
        api_key = os.getenv("OPENAI_API_KEY")
        self.embedding = OpenAIEmbeddings(
            model="text-embedding-ada-002",
            openai_api_key=api_key
        )
        
        # 初始化向量库
        self.vectorstore: Optional[FAISS] = None
        
        # 尝试加载现有向量库
        if persist_directory and os.path.exists(persist_directory):
            try:
                self.vectorstore = FAISS.load_local(
                    persist_directory, 
                    self.embedding,
                    allow_dangerous_deserialization=True
                )
            except:
                self.vectorstore = None
    
    def add_documents(
        self, 
        documents: List[Document], 
        embeddings: Optional[List[List[float]]] = None
    ):
        """添加文档到向量库"""
        try:
            if self.vectorstore is None:
                # 创建新的向量库
                if embeddings is not None:
                    # 使用预计算的向量
                    text_embeddings = list(zip(
                        [doc.page_content for doc in documents],
                        embeddings
                    ))
                    self.vectorstore = FAISS.from_embeddings(
                        text_embeddings,
                        embedding=self.embedding
                    )
                else:
                    # 自动计算向量
                    self.vectorstore = FAISS.from_documents(
                        documents=documents,
                        embedding=self.embedding
                    )
            else:
                # 添加到现有向量库
                if embeddings is not None:
                    # 使用预计算的向量
                    text_embeddings = list(zip(
                        [doc.page_content for doc in documents],
                        embeddings
                    ))
                    self.vectorstore.add_embeddings(text_embeddings)
                else:
                    # 自动计算向量
                    self.vectorstore.add_documents(documents)
                    
        except Exception as e:
            raise Exception(f"添加文档失败: {str(e)}") from e
    
    def similarity_search(
        self, 
        query: str, 
        k: int = 4,
        filter: Optional[dict] = None
    ) -> List[Document]:
        """相似度搜索"""
        if self.vectorstore is None:
            return []
        
        try:
            docs = self.vectorstore.similarity_search(query, k=k)
            return docs
        except Exception as e:
            raise Exception(f"相似度搜索失败: {str(e)}") from e
    
    def similarity_search_with_score(
        self,
        query: str,
        k: int = 4,
        filter: Optional[dict] = None
    ) -> List[tuple[Document, float]]:
        """带相似度分数的搜索"""
        if self.vectorstore is None:
            return []
        
        try:
            docs_with_scores = self.vectorstore.similarity_search_with_score(
                query, k=k
            )
            return docs_with_scores
        except Exception as e:
            raise Exception(f"带分数搜索失败: {str(e)}") from e
    
    def delete(self, ids: Optional[List[str]] = None):
        """删除文档"""
        try:
            if ids is None:
                # 清空向量库
                self.vectorstore = None
            else:
                # FAISS 不支持按 ID 删除
                raise NotImplementedError("FAISS 不支持按 ID 删除文档")
        except Exception as e:
            raise Exception(f"删除文档失败: {str(e)}") from e
    
    def persist(self):
        """持久化向量库到磁盘"""
        if self.vectorstore is None or not self.persist_directory:
            return
        
        try:
            self.vectorstore.save_local(self.persist_directory)
        except Exception as e:
            raise Exception(f"持久化失败: {str(e)}") from e
```

---

## 五、测试代码

### 5.1 基本功能测试

```python
# test_vectorstore.py
from src.core.vectorstore.base import FAISSVectorStore
from langchain.schema import Document
import tempfile
import os

def test_faiss_vectorstore():
    """测试 FAISS 向量存储"""
    
    # 创建临时目录
    with tempfile.TemporaryDirectory() as temp_dir:
        print("=" * 50)
        print("测试 FAISS 向量存储")
        print("=" * 50)
        
        # 创建向量存储实例
        vectorstore = FAISSVectorStore(persist_directory=temp_dir)
        
        # 准备测试文档
        documents = [
            Document(page_content="Python 是一种编程语言"),
            Document(page_content="Java 是一种面向对象的编程语言"),
            Document(page_content="今天天气真好，适合出去散步"),
        ]
        
        print(f"准备文档数量: {len(documents)}")
        
        # 测试添加文档
        try:
            vectorstore.add_documents(documents)
            print("✅ 添加文档成功")
        except Exception as e:
            print(f"❌ 添加文档失败: {e}")
            return
        
        # 测试相似度搜索
        query = "什么是 Python？"
        print(f"\n查询: {query}")
        
        try:
            results = vectorstore.similarity_search(query, k=2)
            print(f"✅ 相似度搜索成功，返回 {len(results)} 个结果")
            for i, doc in enumerate(results, 1):
                print(f"  {i}. {doc.page_content}")
        except Exception as e:
            print(f"❌ 相似度搜索失败: {e}")
            return
        
        # 测试带分数搜索
        try:
            results_with_scores = vectorstore.similarity_search_with_score(
                query, k=2
            )
            print(f"\n✅ 带分数搜索成功")
            for i, (doc, score) in enumerate(results_with_scores, 1):
                print(f"  {i}. 分数: {score:.4f} - {doc.page_content}")
        except Exception as e:
            print(f"❌ 带分数搜索失败: {e}")
        
        # 测试持久化
        try:
            vectorstore.persist()
            print(f"\n✅ 持久化成功，保存到: {temp_dir}")
        except Exception as e:
            print(f"❌ 持久化失败: {e}")

if __name__ == "__main__":
    test_faiss_vectorstore()
```

### 5.2 与 Embedding 集成测试

```python
from src.core.embedding.base import OpenAIEmbedding

def test_with_embedding():
    """测试与 Embedding 模块的集成"""
    
    # 创建 Embedding 和 VectorStore
    embedding = OpenAIEmbedding(model_name="text-embedding-ada-002")
    vectorstore = FAISSVectorStore()
    
    # 准备文档
    documents = [
        Document(page_content="Python 是一种高级编程语言"),
        Document(page_content="Java 是面向对象的编程语言"),
        Document(page_content="天气很好")
    ]
    
    # 预计算向量（更高效）
    print("计算文档向量...")
    doc_embeddings = embedding.embed_documents([doc.page_content for doc in documents])
    
    # 添加到向量库
    print("添加到向量库...")
    vectorstore.add_documents(documents, doc_embeddings)
    
    # 搜索测试
    query = "Python 是什么？"
    query_vector = embedding.embed_query(query)
    
    # 使用向量搜索（如果实现的话）
    # results = vectorstore.similarity_search_by_vector(query_vector, k=2)
    
    # 使用文本搜索
    results = vectorstore.similarity_search(query, k=2)
    
    print(f"查询: {query}")
    for i, doc in enumerate(results, 1):
        print(f"{i}. {doc.page_content}")
```

---

## 六、常见问题

### Q1: FAISS 安装失败？

**解决**：
```bash
# 安装 CPU 版本
pip install faiss-cpu

# 或者 GPU 版本（需要 CUDA）
pip install faiss-gpu
```

### Q2: 向量维度不匹配？

**原因**：使用了不同模型的向量
- OpenAI ada-002：1536 维
- OpenAI 3-small：1536 维
- OpenAI 3-large：3072 维

**解决**：确保所有向量都来自同一模型

### Q3: 搜索结果不准确？

**可能原因**：
1. 文档太少：相似度搜索需要足够的对比样本
2. 文档太短：短文本向量化效果差
3. 相似度阈值：可以设置分数阈值过滤结果

### Q4: 内存不足？

**解决**：
1. 使用更小的模型（7B 而不是 13B）
2. 减少文档数量
3. 使用 PQ 压缩索引（生产环境）

### Q5: 如何删除特定文档？

**FAISS 限制**：FAISS 不支持按 ID 删除文档

**解决方法**：
1. 简单：重建整个向量库
2. 复杂：维护文档 ID 到向量 ID 的映射
3. 推荐：使用支持删除的向量库（如 Chroma、Pinecone）

---

## 七、性能优化

### 7.1 批量处理

```python
# 推荐：批量添加文档
documents = [doc1, doc2, doc3, ...]
vectorstore.add_documents(documents)

# 不推荐：逐个添加（效率低）
for doc in documents:
    vectorstore.add_documents([doc])
```

### 7.2 预计算向量

```python
# 推荐：预计算向量，避免重复计算
embeddings = embedding.embed_documents(texts)
vectorstore.add_documents(documents, embeddings)

# 避免：每次都重新计算
vectorstore.add_documents(documents)  # 内部会重新计算向量
```

### 7.3 索引优化

```python
# 使用不同的索引算法（高级用法）
import faiss

# IVF + PQ 索引（平衡速度和准确性）
nlist = 100  # 聚类中心数量
m = 8        # 子向量数量
nbits = 8    # 每个子向量编码位数

quantizer = faiss.IndexFlatIP(1536)  # 内积索引
index = faiss.IndexIVFPQ(quantizer, 1536, nlist, m, nbits)
```

---

## 八、扩展方向

### 8.1 支持其他向量存储

**Chroma**：
```python
from langchain_community.vectorstores import Chroma

class ChromaVectorStore(BaseVectorStore):
    def __init__(self, persist_directory=None):
        self.vectorstore = Chroma(persist_directory=persist_directory)
    
    # 实现接口方法...
```

**Milvus**：
```python
from langchain_community.vectorstores import Milvus

class MilvusVectorStore(BaseVectorStore):
    def __init__(self, connection_args):
        self.vectorstore = Milvus(connection_args=connection_args)
    
    # 实现接口方法...
```

### 8.2 支持向量过滤

```python
def similarity_search(
    self, 
    query: str, 
    k: int = 4,
    filter: Optional[dict] = None
) -> List[Document]:
    """
    支持元数据过滤
    """
    # 实现过滤逻辑
    # filter = {"source": "book.pdf", "page": 1}
    pass
```

### 8.3 支持异步操作

```python
import asyncio

async def similarity_search_async(
    self, 
    query: str, 
    k: int = 4
) -> List[Document]:
    """
    异步相似度搜索
    """
    # 异步实现
    pass
```

---

## 九、总结

### 9.1 核心知识点

1. **向量存储**：存储和管理高维向量的数据库
2. **FAISS**：高效的向量相似度搜索库
3. **相似度搜索**：基于向量距离的检索
4. **持久化**：向量数据保存到磁盘
5. **抽象接口**：支持多种向量存储实现

### 9.2 实现要点

- `FAISS.from_documents()`：创建向量库
- `similarity_search()`：相似度搜索
- `save_local()` / `load_local()`：持久化
- `add_documents()`：添加新文档
- 错误处理和边界检查

### 9.3 应用场景

1. **文档检索**：RAG 中的文档检索
2. **问答系统**：基于文档的问答
3. **推荐系统**：基于向量的内容推荐
4. **语义搜索**：超越关键词的智能搜索

### 9.4 学习价值

- 理解向量数据库的工作原理
- 掌握 FAISS 的使用方法
- 学习相似度搜索算法
- 理解 RAG 系统的核心组件

---

## 十、参考资源

- [LangChain FAISS 文档](https://python.langchain.com/docs/integrations/vectorstores/faiss)
- [FAISS 官方文档](https://github.com/facebookresearch/faiss)
- [向量数据库对比](https://vectordb.com/)
- [相似度搜索算法](https://en.wikipedia.org/wiki/Nearest_neighbor_search)

